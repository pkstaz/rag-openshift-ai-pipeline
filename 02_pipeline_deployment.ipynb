{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93aec484-3cea-471f-967e-325519f7a133",
   "metadata": {},
   "source": [
    "# 02 - Pipeline Deployment - RAG OpenShift AI\n",
    "\n",
    "## üéØ Objetivo\n",
    "Crear, compilar y deployar el pipeline RAG en OpenShift AI Data Science Pipelines.\n",
    "Este notebook toma los components desarrollados en el notebook anterior y los prepara para deployment real.\n",
    "\n",
    "## üìã Lo que Haremos\n",
    "1. **Crear Components como archivos Python separados**\n",
    "2. **Definir Pipeline real con components importados**\n",
    "3. **Compilar Pipeline a YAML**\n",
    "4. **Configurar MinIO con buckets necesarios**\n",
    "5. **Deploy en OpenShift AI v2**\n",
    "6. **Testing del Pipeline deployado**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e089481-5396-4010-9c65-f18c251c2774",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## üîß Setup Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dad78d83-5119-4ca2-9c7d-7c1ef165f21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ KubeFlow Pipelines disponible: 2.12.1\n",
      "‚úÖ MinIO client disponible\n",
      "üöÄ Notebook de deployment iniciado\n",
      "üìÅ Directorio actual: /opt/app-root/src\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import stat\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Verificar KFP y dependencias\n",
    "try:\n",
    "    import kfp\n",
    "    from kfp import dsl, compiler\n",
    "    from kfp.client import Client\n",
    "    from kfp.dsl import component, pipeline, Input, Output, Dataset\n",
    "    print(f\"‚úÖ KubeFlow Pipelines disponible: {kfp.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå Instalando KubeFlow Pipelines...\")\n",
    "    !pip install kfp>=2.0.0\n",
    "\n",
    "# Verificar MinIO client\n",
    "try:\n",
    "    from minio import Minio\n",
    "    print(\"‚úÖ MinIO client disponible\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå Instalando MinIO client...\")\n",
    "    !pip install minio\n",
    "\n",
    "print(\"üöÄ Notebook de deployment iniciado\")\n",
    "print(f\"üìÅ Directorio actual: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1485945c-18b7-447f-b027-be91bbff274a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## üìÅ Crear Estructura de Archivos para Components\n",
    "\n",
    "Creamos los archivos Python separados para cada component del pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a459e8b-6e00-4ae5-9280-01a8e92eaf2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Creando estructura de directorios...\n",
      "  ‚úÖ components/\n",
      "  ‚úÖ pipelines/\n",
      "  ‚úÖ webhook/\n",
      "  ‚úÖ config/\n",
      "  ‚úÖ deploy/minio/\n",
      "  ‚úÖ deploy/elasticsearch/\n",
      "  ‚úÖ deploy/openshift-ai/\n",
      "  ‚úÖ deploy/webhook/\n",
      "  ‚úÖ tests/\n",
      "  ‚úÖ components/__init__.py\n",
      "  ‚úÖ pipelines/__init__.py\n",
      "  ‚úÖ webhook/__init__.py\n",
      "\n",
      "üìã Estructura creada:\n",
      "  components/          # Pipeline components\n",
      "  pipelines/           # Pipeline definitions\n",
      "  webhook/             # Webhook handler\n",
      "  config/              # Configuration files\n",
      "  deploy/              # Deployment manifests\n",
      "  tests/               # Integration tests\n"
     ]
    }
   ],
   "source": [
    "def create_project_structure():\n",
    "    \"\"\"Crear la estructura de directorios y archivos del proyecto\"\"\"\n",
    "    \n",
    "    # Definir estructura de directorios\n",
    "    directories = [\n",
    "        \"components\",\n",
    "        \"pipelines\", \n",
    "        \"webhook\",\n",
    "        \"config\",\n",
    "        \"deploy/minio\",\n",
    "        \"deploy/elasticsearch\", \n",
    "        \"deploy/openshift-ai\",\n",
    "        \"deploy/webhook\",\n",
    "        \"tests\"\n",
    "    ]\n",
    "    \n",
    "    print(\"üìÅ Creando estructura de directorios...\")\n",
    "    \n",
    "    # Crear directorios\n",
    "    for dir_path in directories:\n",
    "        Path(dir_path).mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"  ‚úÖ {dir_path}/\")\n",
    "    \n",
    "    # Crear archivos __init__.py para modules Python\n",
    "    init_files = [\n",
    "        \"components/__init__.py\",\n",
    "        \"pipelines/__init__.py\",\n",
    "        \"webhook/__init__.py\"\n",
    "    ]\n",
    "    \n",
    "    for init_file in init_files:\n",
    "        Path(init_file).touch(exist_ok=True)\n",
    "        print(f\"  ‚úÖ {init_file}\")\n",
    "    \n",
    "    print(\"\\nüìã Estructura creada:\")\n",
    "    print(\"  components/          # Pipeline components\")\n",
    "    print(\"  pipelines/           # Pipeline definitions\") \n",
    "    print(\"  webhook/             # Webhook handler\")\n",
    "    print(\"  config/              # Configuration files\")\n",
    "    print(\"  deploy/              # Deployment manifests\")\n",
    "    print(\"  tests/               # Integration tests\")\n",
    "    \n",
    "    return directories\n",
    "\n",
    "# Crear estructura\n",
    "created_dirs = create_project_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcc2758-7cda-477a-98ed-ada741208d2f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## üîß Component 1: Text Processing Components\n",
    "\n",
    "Creamos el archivo con los components de procesamiento de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f466e8b-d3ee-4f53-a4f4-c5c0d4477e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Archivo creado: components/text_processing.py\n",
      "üìã Components incluidos:\n",
      "  - extract_text_component\n",
      "  - chunk_text_component\n"
     ]
    }
   ],
   "source": [
    "def create_text_processing_components():\n",
    "    \"\"\"Crear archivo components/text_processing.py\"\"\"\n",
    "    \n",
    "    text_processing_content = '''\"\"\"\n",
    "Text Processing Components para RAG Pipeline\n",
    "Incluye: extract_text_component y chunk_text_component\n",
    "Author: Carlos Estay (github: pkstaz)\n",
    "\"\"\"\n",
    "\n",
    "from kfp.dsl import component, Input, Output, Dataset\n",
    "\n",
    "@component(\n",
    "    base_image=\"pytorch/pytorch:2.0.1-cuda11.7-cudnn8-devel\",\n",
    "    packages_to_install=[\n",
    "        \"PyPDF2==3.0.1\",\n",
    "        \"python-docx==0.8.11\", \n",
    "        \"minio==7.1.17\",\n",
    "        \"chardet==5.2.0\"\n",
    "    ]\n",
    ")\n",
    "def extract_text_component(\n",
    "    bucket_name: str,\n",
    "    object_key: str,\n",
    "    minio_endpoint: str,\n",
    "    minio_access_key: str,\n",
    "    minio_secret_key: str,\n",
    "    extracted_text: Output[Dataset],\n",
    "    metadata: Output[Dataset]\n",
    "):\n",
    "    \"\"\"\n",
    "    Extrae texto de documentos almacenados en MinIO.\n",
    "    \n",
    "    Args:\n",
    "        bucket_name: Nombre del bucket en MinIO\n",
    "        object_key: Path del archivo en el bucket\n",
    "        minio_endpoint: Endpoint de MinIO\n",
    "        minio_access_key: Access key de MinIO\n",
    "        minio_secret_key: Secret key de MinIO\n",
    "        extracted_text: Output dataset con el texto extra√≠do\n",
    "        metadata: Output dataset con metadata del documento\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import json\n",
    "    import tempfile\n",
    "    from pathlib import Path\n",
    "    from datetime import datetime\n",
    "    from minio import Minio\n",
    "    import PyPDF2\n",
    "    from docx import Document\n",
    "    import chardet\n",
    "    \n",
    "    # Conectar a MinIO\n",
    "    minio_client = Minio(\n",
    "        minio_endpoint,\n",
    "        access_key=minio_access_key,\n",
    "        secret_key=minio_secret_key,\n",
    "        secure=False\n",
    "    )\n",
    "    \n",
    "    # Crear directorio temporal\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        local_file_path = os.path.join(temp_dir, object_key.split('/')[-1])\n",
    "        \n",
    "        # Descargar archivo desde MinIO\n",
    "        try:\n",
    "            minio_client.fget_object(bucket_name, object_key, local_file_path)\n",
    "            print(f\"‚úÖ Archivo descargado: {local_file_path}\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error descargando archivo: {str(e)}\")\n",
    "        \n",
    "        # Detectar tipo de archivo\n",
    "        file_extension = Path(local_file_path).suffix.lower()\n",
    "        file_size = os.path.getsize(local_file_path)\n",
    "        \n",
    "        # Extraer texto seg√∫n el tipo de archivo\n",
    "        extracted_content = \"\"\n",
    "        \n",
    "        if file_extension == '.pdf':\n",
    "            try:\n",
    "                with open(local_file_path, 'rb') as file:\n",
    "                    pdf_reader = PyPDF2.PdfReader(file)\n",
    "                    for page_num in range(len(pdf_reader.pages)):\n",
    "                        page = pdf_reader.pages[page_num]\n",
    "                        extracted_content += page.extract_text() + \"\\\\n\"\n",
    "                print(f\"‚úÖ PDF procesado: {len(pdf_reader.pages)} p√°ginas\")\n",
    "            except Exception as e:\n",
    "                raise Exception(f\"Error procesando PDF: {str(e)}\")\n",
    "                \n",
    "        elif file_extension == '.docx':\n",
    "            try:\n",
    "                doc = Document(local_file_path)\n",
    "                for paragraph in doc.paragraphs:\n",
    "                    extracted_content += paragraph.text + \"\\\\n\"\n",
    "                print(f\"‚úÖ DOCX procesado: {len(doc.paragraphs)} p√°rrafos\")\n",
    "            except Exception as e:\n",
    "                raise Exception(f\"Error procesando DOCX: {str(e)}\")\n",
    "                \n",
    "        elif file_extension in ['.txt', '.md']:\n",
    "            try:\n",
    "                # Detectar encoding\n",
    "                with open(local_file_path, 'rb') as file:\n",
    "                    raw_data = file.read()\n",
    "                    encoding = chardet.detect(raw_data)['encoding']\n",
    "                \n",
    "                # Leer con encoding detectado\n",
    "                with open(local_file_path, 'r', encoding=encoding) as file:\n",
    "                    extracted_content = file.read()\n",
    "                print(f\"‚úÖ TXT procesado con encoding: {encoding}\")\n",
    "            except Exception as e:\n",
    "                raise Exception(f\"Error procesando TXT: {str(e)}\")\n",
    "                \n",
    "        else:\n",
    "            raise Exception(f\"Tipo de archivo no soportado: {file_extension}\")\n",
    "        \n",
    "        # Validar que se extrajo contenido\n",
    "        if not extracted_content.strip():\n",
    "            raise Exception(\"No se pudo extraer texto del documento\")\n",
    "        \n",
    "        # Preparar metadata\n",
    "        document_metadata = {\n",
    "            \"source_file\": object_key,\n",
    "            \"file_type\": file_extension,\n",
    "            \"file_size\": file_size,\n",
    "            \"processed_at\": datetime.now().isoformat(),\n",
    "            \"char_count\": len(extracted_content),\n",
    "            \"word_count\": len(extracted_content.split()),\n",
    "            \"bucket_name\": bucket_name\n",
    "        }\n",
    "        \n",
    "        # Guardar outputs\n",
    "        with open(extracted_text.path, 'w', encoding='utf-8') as f:\n",
    "            f.write(extracted_content)\n",
    "            \n",
    "        with open(metadata.path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(document_metadata, f, indent=2)\n",
    "        \n",
    "        print(f\"‚úÖ Texto extra√≠do: {len(extracted_content)} caracteres\")\n",
    "\n",
    "\n",
    "@component(\n",
    "    base_image=\"pytorch/pytorch:2.0.1-cuda11.7-cudnn8-devel\",\n",
    "    packages_to_install=[\n",
    "        \"tiktoken==0.5.1\",\n",
    "        \"langchain==0.0.350\"\n",
    "    ]\n",
    ")\n",
    "def chunk_text_component(\n",
    "    extracted_text: Input[Dataset],\n",
    "    metadata: Input[Dataset],\n",
    "    chunk_size: int,\n",
    "    chunk_overlap: int,\n",
    "    chunks: Output[Dataset]\n",
    "):\n",
    "    \"\"\"\n",
    "    Divide el texto en chunks con overlap para processing √≥ptimo.\n",
    "    \n",
    "    Args:\n",
    "        extracted_text: Input dataset con texto extra√≠do\n",
    "        metadata: Input dataset con metadata del documento\n",
    "        chunk_size: Tama√±o m√°ximo de cada chunk (en tokens)\n",
    "        chunk_overlap: Overlap entre chunks (en tokens)\n",
    "        chunks: Output dataset con chunks procesados\n",
    "    \"\"\"\n",
    "    import json\n",
    "    import tiktoken\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    \n",
    "    # Leer input data\n",
    "    with open(extracted_text.path, 'r', encoding='utf-8') as f:\n",
    "        text_content = f.read()\n",
    "    \n",
    "    with open(metadata.path, 'r', encoding='utf-8') as f:\n",
    "        doc_metadata = json.load(f)\n",
    "    \n",
    "    # Configurar tokenizer\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    def count_tokens(text: str) -> int:\n",
    "        return len(encoding.encode(text))\n",
    "    \n",
    "    # Configurar text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size * 4,  # Aproximaci√≥n: 1 token ‚âà 4 caracteres\n",
    "        chunk_overlap=chunk_overlap * 4,\n",
    "        length_function=len,\n",
    "        separators=[\"\\\\n\\\\n\", \"\\\\n\", \". \", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    # Dividir texto en chunks\n",
    "    text_chunks = text_splitter.split_text(text_content)\n",
    "    print(f\"‚úÖ Texto dividido en {len(text_chunks)} chunks\")\n",
    "    \n",
    "    # Procesar cada chunk\n",
    "    processed_chunks = []\n",
    "    \n",
    "    for i, chunk_text in enumerate(text_chunks):\n",
    "        token_count = count_tokens(chunk_text)\n",
    "        \n",
    "        chunk_metadata = {\n",
    "            \"chunk_id\": f\"{doc_metadata['source_file']}_chunk_{i:04d}\",\n",
    "            \"chunk_index\": i,\n",
    "            \"total_chunks\": len(text_chunks),\n",
    "            \"text\": chunk_text.strip(),\n",
    "            \"token_count\": token_count,\n",
    "            \"char_count\": len(chunk_text),\n",
    "            \"word_count\": len(chunk_text.split()),\n",
    "            \"source_document\": doc_metadata['source_file'],\n",
    "            \"file_type\": doc_metadata['file_type'],\n",
    "            \"processed_at\": doc_metadata['processed_at']\n",
    "        }\n",
    "        \n",
    "        processed_chunks.append(chunk_metadata)\n",
    "    \n",
    "    # Filtrar chunks muy peque√±os\n",
    "    processed_chunks = [chunk for chunk in processed_chunks if chunk['token_count'] >= 10]\n",
    "    \n",
    "    print(f\"‚úÖ Chunks procesados: {len(processed_chunks)}\")\n",
    "    \n",
    "    # Guardar chunks\n",
    "    with open(chunks.path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(processed_chunks, f, indent=2, ensure_ascii=False)\n",
    "'''\n",
    "    \n",
    "    # Escribir archivo\n",
    "    with open('components/text_processing.py', 'w', encoding='utf-8') as f:\n",
    "        f.write(text_processing_content)\n",
    "    \n",
    "    print(\"‚úÖ Archivo creado: components/text_processing.py\")\n",
    "    print(\"üìã Components incluidos:\")\n",
    "    print(\"  - extract_text_component\")\n",
    "    print(\"  - chunk_text_component\")\n",
    "\n",
    "# Crear archivo de text processing\n",
    "create_text_processing_components()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6450cc6b-4952-4409-b132-a621330ce9b4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## üéØ Component 2: Vector Processing Components\n",
    "\n",
    "Creamos el archivo con los components de embeddings e indexaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b4d7a4a-222c-4422-8326-82a9a518f9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Archivo creado: components/vector_processing.py\n",
      "üìã Components incluidos:\n",
      "  - generate_embeddings_component\n",
      "  - index_elasticsearch_component\n"
     ]
    }
   ],
   "source": [
    "def create_vector_processing_components():\n",
    "    \"\"\"Crear archivo components/vector_processing.py\"\"\"\n",
    "    \n",
    "    vector_processing_content = '''\"\"\"\n",
    "Vector Processing Components para RAG Pipeline\n",
    "Incluye: generate_embeddings_component y index_elasticsearch_component\n",
    "Author: Carlos Estay (github: pkstaz)\n",
    "\"\"\"\n",
    "\n",
    "from kfp.dsl import component, Input, Output, Dataset\n",
    "\n",
    "@component(\n",
    "    base_image=\"pytorch/pytorch:2.0.1-cuda11.7-cudnn8-devel\",\n",
    "    packages_to_install=[\n",
    "        \"sentence-transformers==2.2.2\",\n",
    "        \"numpy==1.24.3\"\n",
    "    ]\n",
    ")\n",
    "def generate_embeddings_component(\n",
    "    chunks: Input[Dataset],\n",
    "    model_name: str,\n",
    "    embeddings: Output[Dataset]\n",
    "):\n",
    "    \"\"\"\n",
    "    Genera embeddings vectoriales para los chunks de texto.\n",
    "    \n",
    "    Args:\n",
    "        chunks: Input dataset con chunks de texto\n",
    "        model_name: Nombre del modelo de embeddings\n",
    "        embeddings: Output dataset con embeddings generados\n",
    "    \"\"\"\n",
    "    import json\n",
    "    import numpy as np\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    import torch\n",
    "    \n",
    "    # Verificar si hay GPU disponible\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"üñ•Ô∏è Usando device: {device}\")\n",
    "    \n",
    "    # Cargar modelo de embeddings\n",
    "    print(f\"üì• Cargando modelo: {model_name}\")\n",
    "    model = SentenceTransformer(model_name, device=device)\n",
    "    \n",
    "    # Leer chunks\n",
    "    with open(chunks.path, 'r', encoding='utf-8') as f:\n",
    "        chunk_data = json.load(f)\n",
    "    \n",
    "    print(f\"üìù Procesando {len(chunk_data)} chunks\")\n",
    "    \n",
    "    # Extraer textos para embedding\n",
    "    texts = [chunk['text'] for chunk in chunk_data]\n",
    "    \n",
    "    # Generar embeddings en batches para eficiencia\n",
    "    batch_size = 32\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        batch_embeddings = model.encode(\n",
    "            batch_texts,\n",
    "            convert_to_numpy=True,\n",
    "            show_progress_bar=True if i == 0 else False,\n",
    "            normalize_embeddings=True\n",
    "        )\n",
    "        all_embeddings.extend(batch_embeddings)\n",
    "        \n",
    "        if i % (batch_size * 5) == 0:\n",
    "            print(f\"  Procesado: {min(i + batch_size, len(texts))}/{len(texts)} chunks\")\n",
    "    \n",
    "    print(f\"‚úÖ Embeddings generados: {len(all_embeddings)} vectores de {len(all_embeddings[0])} dimensiones\")\n",
    "    \n",
    "    # Combinar chunks con sus embeddings\n",
    "    enriched_chunks = []\n",
    "    for chunk, embedding in zip(chunk_data, all_embeddings):\n",
    "        enriched_chunk = chunk.copy()\n",
    "        enriched_chunk['embedding'] = embedding.tolist()\n",
    "        enriched_chunk['embedding_dim'] = len(embedding)\n",
    "        enriched_chunk['embedding_model'] = model_name\n",
    "        enriched_chunks.append(enriched_chunk)\n",
    "    \n",
    "    # Guardar chunks enriquecidos con embeddings\n",
    "    with open(embeddings.path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(enriched_chunks, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"‚úÖ Chunks enriquecidos guardados\")\n",
    "\n",
    "\n",
    "@component(\n",
    "    base_image=\"pytorch/pytorch:2.0.1-cuda11.7-cudnn8-devel\",\n",
    "    packages_to_install=[\n",
    "        \"elasticsearch==8.11.0\"\n",
    "    ]\n",
    ")\n",
    "def index_elasticsearch_component(\n",
    "    enriched_chunks: Input[Dataset],\n",
    "    es_endpoint: str,\n",
    "    es_index: str,\n",
    "    index_status: Output[Dataset]\n",
    "):\n",
    "    \"\"\"\n",
    "    Indexa chunks enriquecidos en ElasticSearch.\n",
    "    \n",
    "    Args:\n",
    "        enriched_chunks: Input dataset con chunks y embeddings\n",
    "        es_endpoint: Endpoint de ElasticSearch\n",
    "        es_index: Nombre del √≠ndice\n",
    "        index_status: Output dataset con status de indexaci√≥n\n",
    "    \"\"\"\n",
    "    import json\n",
    "    from datetime import datetime\n",
    "    from elasticsearch import Elasticsearch\n",
    "    from elasticsearch.helpers import bulk\n",
    "    \n",
    "    # Conectar a ElasticSearch\n",
    "    try:\n",
    "        es = Elasticsearch([es_endpoint], verify_certs=False)\n",
    "        \n",
    "        if not es.ping():\n",
    "            raise Exception(\"No se puede conectar a ElasticSearch\")\n",
    "        \n",
    "        print(f\"‚úÖ Conectado a ElasticSearch: {es_endpoint}\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error conectando a ElasticSearch: {str(e)}\")\n",
    "    \n",
    "    # Leer chunks enriquecidos\n",
    "    with open(enriched_chunks.path, 'r', encoding='utf-8') as f:\n",
    "        chunks_data = json.load(f)\n",
    "    \n",
    "    print(f\"üìù Indexando {len(chunks_data)} chunks en √≠ndice: {es_index}\")\n",
    "    \n",
    "    # Definir mapping del √≠ndice\n",
    "    index_mapping = {\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"chunk_id\": {\"type\": \"keyword\"},\n",
    "                \"text\": {\n",
    "                    \"type\": \"text\",\n",
    "                    \"analyzer\": \"standard\"\n",
    "                },\n",
    "                \"embedding\": {\n",
    "                    \"type\": \"dense_vector\",\n",
    "                    \"dims\": chunks_data[0]['embedding_dim'] if chunks_data else 384,\n",
    "                    \"index\": True,\n",
    "                    \"similarity\": \"cosine\"\n",
    "                },\n",
    "                \"source_document\": {\"type\": \"keyword\"},\n",
    "                \"file_type\": {\"type\": \"keyword\"},\n",
    "                \"chunk_index\": {\"type\": \"integer\"},\n",
    "                \"total_chunks\": {\"type\": \"integer\"},\n",
    "                \"token_count\": {\"type\": \"integer\"},\n",
    "                \"char_count\": {\"type\": \"integer\"},\n",
    "                \"word_count\": {\"type\": \"integer\"},\n",
    "                \"processed_at\": {\"type\": \"date\"},\n",
    "                \"indexed_at\": {\"type\": \"date\"}\n",
    "            }\n",
    "        },\n",
    "        \"settings\": {\n",
    "            \"number_of_shards\": 1,\n",
    "            \"number_of_replicas\": 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Crear √≠ndice si no existe\n",
    "    if not es.indices.exists(index=es_index):\n",
    "        es.indices.create(index=es_index, body=index_mapping)\n",
    "        print(f\"‚úÖ √çndice creado: {es_index}\")\n",
    "    else:\n",
    "        print(f\"‚ÑπÔ∏è √çndice ya existe: {es_index}\")\n",
    "    \n",
    "    # Preparar documentos para bulk indexing\n",
    "    documents = []\n",
    "    for chunk in chunks_data:\n",
    "        doc = {\n",
    "            \"_index\": es_index,\n",
    "            \"_id\": chunk['chunk_id'],\n",
    "            \"_source\": {\n",
    "                **chunk,\n",
    "                \"indexed_at\": datetime.now().isoformat()\n",
    "            }\n",
    "        }\n",
    "        documents.append(doc)\n",
    "    \n",
    "    # Indexar en batches\n",
    "    try:\n",
    "        success_count, failed_items = bulk(\n",
    "            es,\n",
    "            documents,\n",
    "            chunk_size=100,\n",
    "            request_timeout=300\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Indexaci√≥n completada:\")\n",
    "        print(f\"  Documentos exitosos: {success_count}\")\n",
    "        print(f\"  Documentos fallidos: {len(failed_items) if failed_items else 0}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error en bulk indexing: {str(e)}\")\n",
    "    \n",
    "    # Refresh del √≠ndice\n",
    "    es.indices.refresh(index=es_index)\n",
    "    \n",
    "    # Verificar indexaci√≥n\n",
    "    doc_count = es.count(index=es_index)['count']\n",
    "    print(f\"‚úÖ Total documentos en √≠ndice: {doc_count}\")\n",
    "    \n",
    "    # Preparar status de indexaci√≥n\n",
    "    indexing_status = {\n",
    "        \"index_name\": es_index,\n",
    "        \"total_chunks\": len(chunks_data),\n",
    "        \"indexed_chunks\": success_count,\n",
    "        \"failed_chunks\": len(failed_items) if failed_items else 0,\n",
    "        \"total_documents_in_index\": doc_count,\n",
    "        \"indexed_at\": datetime.now().isoformat(),\n",
    "        \"success\": len(failed_items) == 0 if failed_items else True\n",
    "    }\n",
    "    \n",
    "    # Guardar status\n",
    "    with open(index_status.path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(indexing_status, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Status de indexaci√≥n guardado\")\n",
    "'''\n",
    "    \n",
    "    # Escribir archivo\n",
    "    with open('components/vector_processing.py', 'w', encoding='utf-8') as f:\n",
    "        f.write(vector_processing_content)\n",
    "    \n",
    "    print(\"‚úÖ Archivo creado: components/vector_processing.py\")\n",
    "    print(\"üìã Components incluidos:\")\n",
    "    print(\"  - generate_embeddings_component\")\n",
    "    print(\"  - index_elasticsearch_component\")\n",
    "\n",
    "# Crear archivo de vector processing\n",
    "create_vector_processing_components()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd93c39-0827-4243-a2d6-dad949c97f5a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## üîó Pipeline Definition: RAG Pipeline Principal\n",
    "\n",
    "Creamos el pipeline principal que orquesta todos los components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6df058dc-d4fc-486c-a233-1fae0d571269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Archivo creado: pipelines/rag_pipeline.py\n",
      "üìã Pipeline function incluida:\n",
      "  - rag_document_pipeline: Pipeline principal\n"
     ]
    }
   ],
   "source": [
    "def create_rag_pipeline():\n",
    "    \"\"\"Crear archivo pipelines/rag_pipeline.py\"\"\"\n",
    "    \n",
    "    rag_pipeline_content = '''\"\"\"\n",
    "RAG Pipeline Principal - OpenShift AI Data Science Pipeline\n",
    "Orquesta todos los components para procesamiento completo de documentos\n",
    "Author: Carlos Estay (github: pkstaz)\n",
    "\"\"\"\n",
    "\n",
    "from kfp import dsl\n",
    "from kfp.dsl import pipeline\n",
    "\n",
    "# Importar components\n",
    "from components.text_processing import extract_text_component, chunk_text_component\n",
    "from components.vector_processing import generate_embeddings_component, index_elasticsearch_component\n",
    "\n",
    "@pipeline(\n",
    "    name=\"rag-document-processing-v1\",\n",
    "    description=\"Pipeline completo de procesamiento de documentos RAG para OpenShift AI\"\n",
    ")\n",
    "def rag_document_pipeline(\n",
    "    bucket_name: str = \"raw-documents\",\n",
    "    object_key: str = \"\",\n",
    "    minio_endpoint: str = \"minio:9000\",\n",
    "    minio_access_key: str = \"minio\",\n",
    "    minio_secret_key: str = \"minio123\",\n",
    "    es_endpoint: str = \"elasticsearch:9200\",\n",
    "    es_index: str = \"rag-documents\",\n",
    "    chunk_size: int = 512,\n",
    "    chunk_overlap: int = 50,\n",
    "    embedding_model: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Pipeline completo de procesamiento de documentos RAG.\n",
    "    \n",
    "    Args:\n",
    "        bucket_name: Nombre del bucket en MinIO\n",
    "        object_key: Path del archivo a procesar\n",
    "        minio_endpoint: Endpoint de MinIO\n",
    "        minio_access_key: Access key de MinIO\n",
    "        minio_secret_key: Secret key de MinIO\n",
    "        es_endpoint: Endpoint de ElasticSearch\n",
    "        es_index: Nombre del √≠ndice en ElasticSearch\n",
    "        chunk_size: Tama√±o de chunks en tokens\n",
    "        chunk_overlap: Overlap entre chunks en tokens\n",
    "        embedding_model: Modelo para generar embeddings\n",
    "    \n",
    "    Returns:\n",
    "        Status de indexaci√≥n final\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Extract text from document\n",
    "    extract_task = extract_text_component(\n",
    "        bucket_name=bucket_name,\n",
    "        object_key=object_key,\n",
    "        minio_endpoint=minio_endpoint,\n",
    "        minio_access_key=minio_access_key,\n",
    "        minio_secret_key=minio_secret_key\n",
    "    )\n",
    "    extract_task.set_display_name(\"üìÑ Extract Text\")\n",
    "    extract_task.set_cpu_limit(\"500m\")\n",
    "    extract_task.set_memory_limit(\"1Gi\")\n",
    "    extract_task.set_retry(3)\n",
    "    \n",
    "    # Step 2: Chunk the extracted text\n",
    "    chunk_task = chunk_text_component(\n",
    "        extracted_text=extract_task.outputs['extracted_text'],\n",
    "        metadata=extract_task.outputs['metadata'],\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    ).after(extract_task)\n",
    "    chunk_task.set_display_name(\"üß© Chunk Text\")\n",
    "    chunk_task.set_cpu_limit(\"500m\")\n",
    "    chunk_task.set_memory_limit(\"1Gi\")\n",
    "    chunk_task.set_retry(3)\n",
    "    \n",
    "    # Step 3: Generate embeddings\n",
    "    embedding_task = generate_embeddings_component(\n",
    "        chunks=chunk_task.outputs['chunks'],\n",
    "        model_name=embedding_model\n",
    "    ).after(chunk_task)\n",
    "    embedding_task.set_display_name(\"üéØ Generate Embeddings\")\n",
    "    embedding_task.set_cpu_limit(\"1000m\")\n",
    "    embedding_task.set_memory_limit(\"4Gi\")\n",
    "    embedding_task.set_retry(2)\n",
    "    # embedding_task.set_gpu_limit(\"1\")  # Uncomment si hay GPUs disponibles\n",
    "    \n",
    "    # Step 4: Index in ElasticSearch\n",
    "    index_task = index_elasticsearch_component(\n",
    "        enriched_chunks=embedding_task.outputs['embeddings'],\n",
    "        es_endpoint=es_endpoint,\n",
    "        es_index=es_index\n",
    "    ).after(embedding_task)\n",
    "    index_task.set_display_name(\"üîç Index ElasticSearch\")\n",
    "    index_task.set_cpu_limit(\"500m\")\n",
    "    index_task.set_memory_limit(\"2Gi\")\n",
    "    index_task.set_retry(3)\n",
    "    \n",
    "    # Return final status\n",
    "    return index_task.outputs['index_status']\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Para testing local del pipeline definition\n",
    "    print(\"‚úÖ RAG Pipeline definido correctamente\")\n",
    "    print(\"üìã Pipeline functions disponibles:\")\n",
    "    print(\"  - rag_document_pipeline: Procesamiento individual\")\n",
    "'''\n",
    "    \n",
    "    # Escribir archivo\n",
    "    with open('pipelines/rag_pipeline.py', 'w', encoding='utf-8') as f:\n",
    "        f.write(rag_pipeline_content)\n",
    "    \n",
    "    print(\"‚úÖ Archivo creado: pipelines/rag_pipeline.py\")\n",
    "    print(\"üìã Pipeline function incluida:\")\n",
    "    print(\"  - rag_document_pipeline: Pipeline principal\")\n",
    "\n",
    "# Crear archivo del pipeline\n",
    "create_rag_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a861af26-a347-462a-8327-9f2f8fa2ed8e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ‚öôÔ∏è Configuration Files\n",
    "\n",
    "Creamos los archivos de configuraci√≥n necesarios para el deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b60954e6-e468-4e11-a685-82ce1a11d350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Creado: config/pipeline_config.yaml\n",
      "‚úÖ Creado: config/secrets.yaml\n",
      "‚úÖ Creado: requirements.txt\n",
      "‚úÖ Creado: config/env.template\n",
      "\n",
      "üìã Archivos de configuraci√≥n creados:\n",
      "  config/pipeline_config.yaml - Configuraci√≥n del pipeline\n",
      "  config/secrets.yaml - Template de secrets K8s\n",
      "  requirements.txt - Dependencias Python\n",
      "  config/env.template - Variables de ambiente\n"
     ]
    }
   ],
   "source": [
    "def create_configuration_files():\n",
    "    \"\"\"Crear archivos de configuraci√≥n del proyecto\"\"\"\n",
    "    \n",
    "    # 1. Pipeline Configuration\n",
    "    pipeline_config = {\n",
    "        \"pipeline\": {\n",
    "            \"name\": \"rag-document-processing-v1\",\n",
    "            \"version\": \"1.0.0\",\n",
    "            \"description\": \"RAG Document Processing Pipeline para OpenShift AI\",\n",
    "            \"author\": \"Carlos Estay\",\n",
    "            \"github\": \"pkstaz\",\n",
    "            \"created\": datetime.now().isoformat()\n",
    "        },\n",
    "        \"components\": {\n",
    "            \"base_image\": \"pytorch/pytorch:2.0.1-cuda11.7-cudnn8-devel\",\n",
    "            \"extract_text\": {\n",
    "                \"cpu_limit\": \"500m\",\n",
    "                \"memory_limit\": \"1Gi\",\n",
    "                \"retry_limit\": 3\n",
    "            },\n",
    "            \"chunk_text\": {\n",
    "                \"cpu_limit\": \"500m\", \n",
    "                \"memory_limit\": \"1Gi\",\n",
    "                \"retry_limit\": 3\n",
    "            },\n",
    "            \"generate_embeddings\": {\n",
    "                \"cpu_limit\": \"1000m\",\n",
    "                \"memory_limit\": \"4Gi\", \n",
    "                \"retry_limit\": 2,\n",
    "                \"gpu_limit\": \"0\"  # Set to \"1\" si hay GPU disponible\n",
    "            },\n",
    "            \"index_elasticsearch\": {\n",
    "                \"cpu_limit\": \"500m\",\n",
    "                \"memory_limit\": \"2Gi\",\n",
    "                \"retry_limit\": 3\n",
    "            }\n",
    "        },\n",
    "        \"default_parameters\": {\n",
    "            \"chunk_size\": 512,\n",
    "            \"chunk_overlap\": 50,\n",
    "            \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "            \"es_index\": \"rag-documents\"\n",
    "        },\n",
    "        \"storage\": {\n",
    "            \"minio\": {\n",
    "                \"endpoint\": \"minio:9000\",\n",
    "                \"access_key\": \"minio\",\n",
    "                \"secret_key\": \"minio123\",\n",
    "                \"bucket_raw\": \"raw-documents\",\n",
    "                \"bucket_processed\": \"processed-documents\",\n",
    "                \"bucket_failed\": \"failed-documents\",\n",
    "                \"bucket_pipeline\": \"pipeline\"\n",
    "            },\n",
    "            \"elasticsearch\": {\n",
    "                \"endpoint\": \"elasticsearch:9200\",\n",
    "                \"index_prefix\": \"rag-\",\n",
    "                \"replicas\": 0,\n",
    "                \"shards\": 1\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open('config/pipeline_config.yaml', 'w') as f:\n",
    "        yaml.dump(pipeline_config, f, indent=2, default_flow_style=False)\n",
    "    \n",
    "    print(\"‚úÖ Creado: config/pipeline_config.yaml\")\n",
    "    \n",
    "    # 2. Secrets Template\n",
    "    secrets_template = {\n",
    "        \"apiVersion\": \"v1\",\n",
    "        \"kind\": \"Secret\",\n",
    "        \"metadata\": {\n",
    "            \"name\": \"rag-pipeline-secrets\",\n",
    "            \"namespace\": \"rag-openshift-ai\",\n",
    "            \"labels\": {\n",
    "                \"app\": \"rag-pipeline\",\n",
    "                \"author\": \"carlos-estay\",\n",
    "                \"github\": \"pkstaz\"\n",
    "            }\n",
    "        },\n",
    "        \"type\": \"Opaque\",\n",
    "        \"stringData\": {\n",
    "            \"minio-access-key\": \"minio\",\n",
    "            \"minio-secret-key\": \"minio123\",\n",
    "            \"elasticsearch-username\": \"\",\n",
    "            \"elasticsearch-password\": \"\",\n",
    "            \"pipeline-webhook-token\": \"rag-pipeline-token-change-me\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open('config/secrets.yaml', 'w') as f:\n",
    "        yaml.dump(secrets_template, f, indent=2, default_flow_style=False)\n",
    "    \n",
    "    print(\"‚úÖ Creado: config/secrets.yaml\")\n",
    "    \n",
    "    # 3. Requirements file\n",
    "    requirements_content = '''# RAG Pipeline Requirements\n",
    "# Author: Carlos Estay (github: pkstaz)\n",
    "# Core KFP\n",
    "kfp>=2.0.0\n",
    "\n",
    "# Document Processing\n",
    "PyPDF2==3.0.1\n",
    "python-docx==0.8.11\n",
    "chardet==5.2.0\n",
    "\n",
    "# Text Processing\n",
    "tiktoken==0.5.1\n",
    "langchain==0.0.350\n",
    "\n",
    "# ML/Embeddings\n",
    "sentence-transformers==2.2.2\n",
    "torch>=2.0.1\n",
    "numpy==1.24.3\n",
    "\n",
    "# Storage/DB\n",
    "minio==7.1.17\n",
    "elasticsearch==8.11.0\n",
    "\n",
    "# Web/API (for webhook)\n",
    "flask==2.3.3\n",
    "requests==2.31.0\n",
    "\n",
    "# Utilities\n",
    "pyyaml==6.0.1\n",
    "python-dateutil==2.8.2\n",
    "'''\n",
    "    \n",
    "    with open('requirements.txt', 'w') as f:\n",
    "        f.write(requirements_content)\n",
    "    \n",
    "    print(\"‚úÖ Creado: requirements.txt\")\n",
    "    \n",
    "    # 4. Environment variables template\n",
    "    env_template = '''# RAG Pipeline Environment Variables\n",
    "# Author: Carlos Estay (github: pkstaz)\n",
    "# Copy to .env and modify as needed\n",
    "\n",
    "# MinIO Configuration\n",
    "MINIO_ENDPOINT=minio:9000\n",
    "MINIO_ACCESS_KEY=minio\n",
    "MINIO_SECRET_KEY=minio123\n",
    "MINIO_SECURE=false\n",
    "\n",
    "# ElasticSearch Configuration  \n",
    "ES_ENDPOINT=elasticsearch:9200\n",
    "ES_INDEX=rag-documents\n",
    "ES_USERNAME=\n",
    "ES_PASSWORD=\n",
    "\n",
    "# Pipeline Configuration\n",
    "CHUNK_SIZE=512\n",
    "CHUNK_OVERLAP=50\n",
    "EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2\n",
    "\n",
    "# OpenShift AI Pipeline\n",
    "KFP_HOST=http://ml-pipeline:8888\n",
    "PIPELINE_NAMESPACE=rag-openshift-ai\n",
    "\n",
    "# Webhook Configuration\n",
    "WEBHOOK_PORT=8080\n",
    "WEBHOOK_TOKEN=rag-pipeline-token-change-me\n",
    "'''\n",
    "    \n",
    "    with open('config/env.template', 'w') as f:\n",
    "        f.write(env_template)\n",
    "    \n",
    "    print(\"‚úÖ Creado: config/env.template\")\n",
    "    \n",
    "    print(\"\\nüìã Archivos de configuraci√≥n creados:\")\n",
    "    print(\"  config/pipeline_config.yaml - Configuraci√≥n del pipeline\")\n",
    "    print(\"  config/secrets.yaml - Template de secrets K8s\")\n",
    "    print(\"  requirements.txt - Dependencias Python\")  \n",
    "    print(\"  config/env.template - Variables de ambiente\")\n",
    "\n",
    "# Crear archivos de configuraci√≥n\n",
    "create_configuration_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432bfe28-97fd-4f7c-ba34-f2d3069338c0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ü™£ Configuraci√≥n de MinIO - Crear Buckets\n",
    "\n",
    "Configuramos MinIO y creamos todos los buckets necesarios incluyendo el bucket \"pipeline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ced5bd2-2026-4962-a90b-060016d41b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Configuraci√≥n completa de MinIO para OpenShift AI DSP\n",
      "============================================================\n",
      "üîß Configurando MinIO y creando buckets completos...\n",
      "üîó Conectando a MinIO: minio:9000\n",
      "‚úÖ Conectado a MinIO exitosamente\n",
      "üìä Buckets existentes: 6\n",
      "  - failed-documents (creado: 2025-07-01 17:40:26.746000+00:00)\n",
      "  - pipeline (creado: 2025-07-01 17:46:35.035000+00:00)\n",
      "  - processed-documents (creado: 2025-07-01 17:40:26.739000+00:00)\n",
      "  - rag-documents (creado: 2025-07-01 16:45:51.180000+00:00)\n",
      "  - raw-documents (creado: 2025-07-01 17:40:26.732000+00:00)\n",
      "  - test-datasets (creado: 2025-07-01 17:40:26.753000+00:00)\n",
      "\n",
      "ü™£ Creando buckets necesarios...\n",
      "  ‚ÑπÔ∏è Bucket ya existe: pipeline\n",
      "    üìù Bucket requerido por OpenShift AI Data Science Pipelines\n",
      "  ‚ÑπÔ∏è Bucket ya existe: raw-documents\n",
      "    üìù Documentos originales subidos por usuarios\n",
      "  ‚ÑπÔ∏è Bucket ya existe: processed-documents\n",
      "    üìù Documentos procesados exitosamente\n",
      "  ‚ÑπÔ∏è Bucket ya existe: failed-documents\n",
      "    üìù Documentos que fallaron en procesamiento\n",
      "  ‚ÑπÔ∏è Bucket ya existe: test-datasets\n",
      "    üìù Datasets de prueba para desarrollo\n",
      "\n",
      "üìä Resumen de buckets:\n",
      "  ‚úÖ Creados: 0\n",
      "  ‚ÑπÔ∏è Ya exist√≠an: 5\n",
      "\n",
      "üóÇÔ∏è Buckets disponibles:\n",
      "  - failed-documents\n",
      "  - pipeline\n",
      "  - processed-documents\n",
      "  - rag-documents\n",
      "  - raw-documents\n",
      "  - test-datasets\n",
      "\n",
      "‚úÖ Bucket 'pipeline' configurado correctamente para OpenShift AI DSP\n",
      "\n",
      "üìÑ Subiendo documento de prueba...\n",
      "‚úÖ Documento subido: raw-documents/test-document-openshift-ai.txt\n",
      "üìè Tama√±o: 1809 caracteres\n",
      "üìä Palabras aproximadas: 241\n",
      "üìã Verificaci√≥n:\n",
      "  Size: 1828 bytes\n",
      "  Content-Type: text/plain\n",
      "  Last-Modified: 2025-07-01 18:55:05+00:00\n",
      "\n",
      "üéØ MINIO CONFIGURADO PARA OPENSHIFT AI DSP:\n",
      "  ‚úÖ Bucket 'pipeline' creado (requerido por OpenShift AI)\n",
      "  ‚úÖ Buckets RAG creados (raw-documents, processed-documents, etc.)\n",
      "  ‚úÖ Documento de prueba subido: test-document-openshift-ai.txt\n",
      "  üìä Tama√±o: 241 palabras\n",
      "\n",
      "üöÄ LISTO PARA CONFIGURAR OPENSHIFT AI DSP\n"
     ]
    }
   ],
   "source": [
    "def setup_minio_buckets_complete():\n",
    "    \"\"\"Configurar MinIO y crear todos los buckets necesarios incluyendo 'pipeline'\"\"\"\n",
    "    \n",
    "    print(\"üîß Configurando MinIO y creando buckets completos...\")\n",
    "    \n",
    "    # Configuraci√≥n MinIO\n",
    "    MINIO_ENDPOINT = \"minio:9000\"\n",
    "    MINIO_ACCESS_KEY = \"minio\"\n",
    "    MINIO_SECRET_KEY = \"minio123\"\n",
    "    \n",
    "    try:\n",
    "        from minio import Minio\n",
    "        from minio.error import S3Error\n",
    "        \n",
    "        # Conectar a MinIO\n",
    "        print(f\"üîó Conectando a MinIO: {MINIO_ENDPOINT}\")\n",
    "        \n",
    "        minio_client = Minio(\n",
    "            MINIO_ENDPOINT,\n",
    "            access_key=MINIO_ACCESS_KEY,\n",
    "            secret_key=MINIO_SECRET_KEY,\n",
    "            secure=False  # HTTP para desarrollo\n",
    "        )\n",
    "        \n",
    "        # Verificar conectividad\n",
    "        try:\n",
    "            buckets = minio_client.list_buckets()\n",
    "            print(f\"‚úÖ Conectado a MinIO exitosamente\")\n",
    "            print(f\"üìä Buckets existentes: {len(buckets)}\")\n",
    "            for bucket in buckets:\n",
    "                print(f\"  - {bucket.name} (creado: {bucket.creation_date})\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error conectando a MinIO: {str(e)}\")\n",
    "            return False\n",
    "        \n",
    "        # Buckets necesarios para RAG pipeline + OpenShift AI DSP\n",
    "        required_buckets = [\n",
    "            {\n",
    "                \"name\": \"pipeline\",\n",
    "                \"description\": \"Bucket requerido por OpenShift AI Data Science Pipelines\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"raw-documents\",\n",
    "                \"description\": \"Documentos originales subidos por usuarios\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"processed-documents\", \n",
    "                \"description\": \"Documentos procesados exitosamente\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"failed-documents\",\n",
    "                \"description\": \"Documentos que fallaron en procesamiento\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"test-datasets\",\n",
    "                \"description\": \"Datasets de prueba para desarrollo\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\nü™£ Creando buckets necesarios...\")\n",
    "        \n",
    "        created_buckets = []\n",
    "        existing_buckets = []\n",
    "        \n",
    "        for bucket_info in required_buckets:\n",
    "            bucket_name = bucket_info[\"name\"]\n",
    "            description = bucket_info[\"description\"]\n",
    "            \n",
    "            try:\n",
    "                # Verificar si bucket ya existe\n",
    "                if minio_client.bucket_exists(bucket_name):\n",
    "                    existing_buckets.append(bucket_name)\n",
    "                    print(f\"  ‚ÑπÔ∏è Bucket ya existe: {bucket_name}\")\n",
    "                else:\n",
    "                    # Crear bucket\n",
    "                    minio_client.make_bucket(bucket_name)\n",
    "                    created_buckets.append(bucket_name)\n",
    "                    print(f\"  ‚úÖ Bucket creado: {bucket_name}\")\n",
    "                \n",
    "                print(f\"    üìù {description}\")\n",
    "                    \n",
    "            except S3Error as e:\n",
    "                print(f\"  ‚ùå Error con bucket {bucket_name}: {str(e)}\")\n",
    "        \n",
    "        print(f\"\\nüìä Resumen de buckets:\")\n",
    "        print(f\"  ‚úÖ Creados: {len(created_buckets)}\")\n",
    "        print(f\"  ‚ÑπÔ∏è Ya exist√≠an: {len(existing_buckets)}\")\n",
    "        \n",
    "        # Listar todos los buckets despu√©s de la configuraci√≥n\n",
    "        final_buckets = minio_client.list_buckets()\n",
    "        print(f\"\\nüóÇÔ∏è Buckets disponibles:\")\n",
    "        for bucket in final_buckets:\n",
    "            print(f\"  - {bucket.name}\")\n",
    "        \n",
    "        # Verificar bucket cr√≠tico para OpenShift AI\n",
    "        if minio_client.bucket_exists(\"pipeline\"):\n",
    "            print(f\"\\n‚úÖ Bucket 'pipeline' configurado correctamente para OpenShift AI DSP\")\n",
    "        else:\n",
    "            print(f\"\\n‚ùå Bucket 'pipeline' no se pudo crear - requerido para OpenShift AI DSP\")\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"‚ùå MinIO client no disponible. Instalar con: pip install minio\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error configurando MinIO: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def upload_test_document():\n",
    "    \"\"\"Subir documento de prueba al bucket raw-documents\"\"\"\n",
    "    \n",
    "    print(f\"\\nüìÑ Subiendo documento de prueba...\")\n",
    "    \n",
    "    try:\n",
    "        from minio import Minio\n",
    "        import tempfile\n",
    "        \n",
    "        # Conectar a MinIO\n",
    "        minio_client = Minio(\n",
    "            \"minio:9000\",\n",
    "            access_key=\"minio\",\n",
    "            secret_key=\"minio123\",\n",
    "            secure=False\n",
    "        )\n",
    "        \n",
    "        # Crear documento de prueba m√°s detallado\n",
    "        test_content = \"\"\"# Documento de Prueba RAG Pipeline - OpenShift AI\n",
    "        \n",
    "Este es un documento de prueba para validar el pipeline RAG en OpenShift AI Data Science Pipelines.\n",
    "\n",
    "## Configuraci√≥n del Ambiente\n",
    "\n",
    "### MinIO Buckets Configurados:\n",
    "- **pipeline**: Bucket requerido por OpenShift AI DSP\n",
    "- **raw-documents**: Documentos originales \n",
    "- **processed-documents**: Documentos procesados exitosamente\n",
    "- **failed-documents**: Documentos con errores de procesamiento\n",
    "- **test-datasets**: Datasets para desarrollo y pruebas\n",
    "\n",
    "### Pipeline de Procesamiento RAG\n",
    "\n",
    "El sistema procesa documentos siguiendo estos pasos:\n",
    "\n",
    "1. **Extracci√≥n de texto**: \n",
    "   - Soporta PDF, DOCX, TXT\n",
    "   - Detecta encoding autom√°ticamente\n",
    "   - Extrae metadatos del documento\n",
    "\n",
    "2. **Chunking inteligente**:\n",
    "   - Chunks de 512 tokens con 50 de overlap\n",
    "   - Usa tiktoken para conteo preciso\n",
    "   - Mantiene contexto entre fragmentos\n",
    "\n",
    "3. **Generaci√≥n de embeddings**:\n",
    "   - Modelo: sentence-transformers/all-MiniLM-L6-v2\n",
    "   - Vectores de 384 dimensiones\n",
    "   - Normalizaci√≥n para cosine similarity\n",
    "\n",
    "4. **Indexaci√≥n en ElasticSearch**:\n",
    "   - √çndice h√≠brido (texto + vectores)\n",
    "   - B√∫squeda sem√°ntica y por palabras clave\n",
    "   - Metadatos preservados para trazabilidad\n",
    "\n",
    "## Informaci√≥n del Proyecto\n",
    "\n",
    "- **Autor**: Carlos Estay\n",
    "- **GitHub**: pkstaz  \n",
    "- **Pipeline**: rag-document-processing-v1\n",
    "- **Platform**: OpenShift AI Data Science Pipelines\n",
    "\n",
    "Este documento ser√° procesado autom√°ticamente cuando el webhook detecte \n",
    "su upload al bucket raw-documents.\n",
    "\n",
    "## Testing del Pipeline\n",
    "\n",
    "Para validar el funcionamiento:\n",
    "\n",
    "1. Este documento se procesa autom√°ticamente\n",
    "2. Se generan aproximadamente 4-6 chunks\n",
    "3. Cada chunk obtiene un embedding de 384 dimensiones  \n",
    "4. Se indexa en ElasticSearch con ID √∫nico\n",
    "5. Est√° disponible para b√∫squeda sem√°ntica\n",
    "\n",
    "¬°Pipeline RAG funcionando en OpenShift AI!\n",
    "\"\"\"\n",
    "        \n",
    "        # Crear archivo temporal\n",
    "        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8') as f:\n",
    "            f.write(test_content)\n",
    "            temp_file_path = f.name\n",
    "        \n",
    "        # Subir archivo a MinIO\n",
    "        object_name = \"test-document-openshift-ai.txt\"\n",
    "        bucket_name = \"raw-documents\"\n",
    "        \n",
    "        try:\n",
    "            minio_client.fput_object(\n",
    "                bucket_name=bucket_name,\n",
    "                object_name=object_name,\n",
    "                file_path=temp_file_path,\n",
    "                content_type=\"text/plain\"\n",
    "            )\n",
    "            \n",
    "            print(f\"‚úÖ Documento subido: {bucket_name}/{object_name}\")\n",
    "            print(f\"üìè Tama√±o: {len(test_content)} caracteres\")\n",
    "            print(f\"üìä Palabras aproximadas: {len(test_content.split())}\")\n",
    "            \n",
    "            # Verificar que se subi√≥ correctamente\n",
    "            try:\n",
    "                stat = minio_client.stat_object(bucket_name, object_name)\n",
    "                print(f\"üìã Verificaci√≥n:\")\n",
    "                print(f\"  Size: {stat.size} bytes\")\n",
    "                print(f\"  Content-Type: {stat.content_type}\")\n",
    "                print(f\"  Last-Modified: {stat.last_modified}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è No se pudo verificar el archivo: {str(e)}\")\n",
    "            \n",
    "            return {\n",
    "                \"bucket\": bucket_name,\n",
    "                \"object\": object_name,\n",
    "                \"size\": len(test_content),\n",
    "                \"word_count\": len(test_content.split()),\n",
    "                \"success\": True\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error subiendo archivo: {str(e)}\")\n",
    "            return None\n",
    "            \n",
    "        finally:\n",
    "            # Limpiar archivo temporal\n",
    "            import os\n",
    "            if os.path.exists(temp_file_path):\n",
    "                os.unlink(temp_file_path)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en upload de documento de prueba: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Ejecutar configuraci√≥n completa de MinIO\n",
    "print(\"üöÄ Configuraci√≥n completa de MinIO para OpenShift AI DSP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "minio_setup_success = setup_minio_buckets_complete()\n",
    "\n",
    "# Subir documento de prueba si MinIO est√° configurado\n",
    "if minio_setup_success:\n",
    "    test_upload_result = upload_test_document()\n",
    "    \n",
    "    if test_upload_result:\n",
    "        print(f\"\\nüéØ MINIO CONFIGURADO PARA OPENSHIFT AI DSP:\")\n",
    "        print(f\"  ‚úÖ Bucket 'pipeline' creado (requerido por OpenShift AI)\")\n",
    "        print(f\"  ‚úÖ Buckets RAG creados (raw-documents, processed-documents, etc.)\")\n",
    "        print(f\"  ‚úÖ Documento de prueba subido: {test_upload_result['object']}\")\n",
    "        print(f\"  üìä Tama√±o: {test_upload_result['word_count']} palabras\")\n",
    "        print(f\"\\nüöÄ LISTO PARA CONFIGURAR OPENSHIFT AI DSP\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è MinIO configurado pero falta documento de prueba\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Configurar MinIO antes de continuar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32d3679-1682-41c3-8d11-b85af8997978",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## üî® Compilaci√≥n del Pipeline - KFP 2.12.1 Compatible\n",
    "\n",
    "Compilamos el pipeline usando la sintaxis correcta para KFP 2.12.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dd540c20-9d52-42a4-9dd4-2341cc7dab00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî® Compilando Pipeline Standalone...\n",
      "‚úÖ Imports para KFP 2.12.1 correctos\n",
      "‚úÖ Pipeline simplificado definido con sintaxis KFP 2.12.1\n",
      "‚úÖ Pipeline compilado exitosamente: rag_simple_pipeline_v1.yaml\n",
      "üìÑ Tama√±o del archivo: 6423 bytes\n",
      "\n",
      "üìù Primeras l√≠neas del YAML:\n",
      "  1: # PIPELINE DEFINITION\n",
      "  2: # Name: rag-simple-test-v1\n",
      "  3: # Description: Pipeline RAG simplificado para testing - Carlos Estay (pkstaz)\n",
      "  4: # Inputs:\n",
      "  5: #    bucket_name: str [Default: 'raw-documents']\n",
      "  6: #    es_endpoint: str [Default: 'elasticsearch:9200']\n",
      "  7: #    object_key: str [Default: 'test-document-openshift-ai.txt']\n",
      "  8: # Outputs:\n",
      "\n",
      "‚úÖ PIPELINE COMPILADO EXITOSAMENTE\n",
      "üìÑ Archivo: rag_simple_pipeline_v1.yaml\n",
      "üöÄ Listo para subir a OpenShift AI Dashboard\n"
     ]
    }
   ],
   "source": [
    "def compile_standalone_pipeline():\n",
    "    \"\"\"Compilar pipeline standalone compatible con KFP 2.12.1\"\"\"\n",
    "    \n",
    "    print(\"üî® Compilando Pipeline Standalone...\")\n",
    "    \n",
    "    try:\n",
    "        from kfp import dsl\n",
    "        from kfp.dsl import component, pipeline\n",
    "        from kfp import compiler\n",
    "        \n",
    "        print(\"‚úÖ Imports para KFP 2.12.1 correctos\")\n",
    "        \n",
    "        @component(\n",
    "            base_image=\"pytorch/pytorch:2.0.1-cuda11.7-cudnn8-devel\",\n",
    "            packages_to_install=[\"PyPDF2==3.0.1\", \"minio==7.1.17\"]\n",
    "        )\n",
    "        def simple_extract_component(\n",
    "            bucket_name: str,\n",
    "            object_key: str\n",
    "        ) -> str:\n",
    "            \"\"\"Component simplificado para testing - retorna string\"\"\"\n",
    "            \n",
    "            # Simular extracci√≥n\n",
    "            test_content = f\"Contenido extra√≠do de {object_key} en bucket {bucket_name}\"\n",
    "            \n",
    "            print(f\"‚úÖ Texto extra√≠do: {len(test_content)} caracteres\")\n",
    "            \n",
    "            return test_content\n",
    "\n",
    "        @component(\n",
    "            base_image=\"pytorch/pytorch:2.0.1-cuda11.7-cudnn8-devel\",\n",
    "            packages_to_install=[\"elasticsearch==8.11.0\"]\n",
    "        )\n",
    "        def simple_index_component(\n",
    "            extracted_text: str,\n",
    "            es_endpoint: str\n",
    "        ) -> dict:\n",
    "            \"\"\"Component simplificado de indexaci√≥n - retorna dict\"\"\"\n",
    "            from datetime import datetime\n",
    "            \n",
    "            # Simular indexaci√≥n\n",
    "            status = {\n",
    "                \"indexed_at\": datetime.now().isoformat(),\n",
    "                \"content_length\": len(extracted_text),\n",
    "                \"es_endpoint\": es_endpoint,\n",
    "                \"success\": True\n",
    "            }\n",
    "            \n",
    "            print(f\"‚úÖ Indexaci√≥n simulada completada\")\n",
    "            \n",
    "            return status\n",
    "\n",
    "        @pipeline(\n",
    "            name=\"rag-simple-test-v1\",\n",
    "            description=\"Pipeline RAG simplificado para testing - Carlos Estay (pkstaz)\"\n",
    "        )\n",
    "        def simple_rag_pipeline(\n",
    "            bucket_name: str = \"raw-documents\",\n",
    "            object_key: str = \"test-document-openshift-ai.txt\",\n",
    "            es_endpoint: str = \"elasticsearch:9200\"\n",
    "        ) -> dict:\n",
    "            \"\"\"Pipeline simplificado para verificar compilaci√≥n\"\"\"\n",
    "            \n",
    "            # Step 1: Extract\n",
    "            extract_task = simple_extract_component(\n",
    "                bucket_name=bucket_name,\n",
    "                object_key=object_key\n",
    "            )\n",
    "            extract_task.set_display_name(\"üìÑ Simple Extract\")\n",
    "            extract_task.set_cpu_limit(\"500m\")\n",
    "            extract_task.set_memory_limit(\"1Gi\")\n",
    "            \n",
    "            # Step 2: Index\n",
    "            index_task = simple_index_component(\n",
    "                extracted_text=extract_task.output,\n",
    "                es_endpoint=es_endpoint\n",
    "            )\n",
    "            index_task.set_display_name(\"üîç Simple Index\")\n",
    "            index_task.set_cpu_limit(\"500m\")\n",
    "            index_task.set_memory_limit(\"1Gi\")\n",
    "            \n",
    "            return index_task.output\n",
    "        \n",
    "        print(\"‚úÖ Pipeline simplificado definido con sintaxis KFP 2.12.1\")\n",
    "        \n",
    "        # Compilar pipeline\n",
    "        output_file = 'rag_simple_pipeline_v1.yaml'\n",
    "        \n",
    "        compiler.Compiler().compile(\n",
    "            pipeline_func=simple_rag_pipeline,\n",
    "            package_path=output_file\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Pipeline compilado exitosamente: {output_file}\")\n",
    "        \n",
    "        # Verificar archivo\n",
    "        if Path(output_file).exists():\n",
    "            file_size = Path(output_file).stat().st_size\n",
    "            print(f\"üìÑ Tama√±o del archivo: {file_size} bytes\")\n",
    "            \n",
    "            # Verificar contenido\n",
    "            with open(output_file, 'r') as f:\n",
    "                content = f.read()\n",
    "                \n",
    "            if 'apiVersion' in content and 'kind' in content:\n",
    "                print(\"‚úÖ YAML v√°lido generado\")\n",
    "                \n",
    "                # Contar components\n",
    "                component_count = content.count('componentRef')\n",
    "                print(f\"üìä Components encontrados en YAML: {component_count}\")\n",
    "                \n",
    "                # Mostrar metadata\n",
    "                try:\n",
    "                    yaml_data = yaml.safe_load(content)\n",
    "                    metadata = yaml_data.get('metadata', {})\n",
    "                    print(f\"üìã Pipeline name: {metadata.get('name', 'N/A')}\")\n",
    "                except:\n",
    "                    print(\"‚ö†Ô∏è No se pudo parsear metadata del YAML\")\n",
    "            \n",
    "            print(f\"\\nüìù Primeras l√≠neas del YAML:\")\n",
    "            lines = content.split('\\n')[:8]\n",
    "            for i, line in enumerate(lines, 1):\n",
    "                print(f\"  {i}: {line}\")\n",
    "        \n",
    "        return output_file\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en compilaci√≥n: {str(e)}\")\n",
    "        \n",
    "        # M√°s debugging espec√≠fico\n",
    "        import traceback\n",
    "        print(f\"\\nüîç Traceback completo:\")\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        return None\n",
    "\n",
    "# Ejecutar compilaci√≥n\n",
    "compiled_file = compile_standalone_pipeline()\n",
    "\n",
    "if compiled_file:\n",
    "    print(f\"\\n‚úÖ PIPELINE COMPILADO EXITOSAMENTE\")\n",
    "    print(f\"üìÑ Archivo: {compiled_file}\")\n",
    "    print(f\"üöÄ Listo para subir a OpenShift AI Dashboard\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Error en compilaci√≥n del pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab83d764-fde4-4b69-8487-27c0c7b09549",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Configuraci√≥n de OpenShift AI Data Science Pipeline v2\n",
    "\n",
    "Creamos la configuraci√≥n compatible con OpenShift AI v2 pipelines usando MinIO externo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9273d3e9-698e-4ff1-b72b-d093d951f703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß SOLUCI√ìN DEFINITIVA - Reconstruir DSP v2\n",
      "============================================================\n",
      "üîß Recreando DSP v2 completamente...\n",
      "‚úÖ Configuraci√≥n DSP m√≠nima creada: deploy/openshift-ai/dsp-minimal.yaml\n",
      "‚úÖ Script de rebuild creado: rebuild_dsp.sh\n",
      "\n",
      "üß™ Creando pipeline de verificaci√≥n ultra-simple...\n",
      "‚úÖ Pipeline de verificaci√≥n creado: dsp_verification.yaml\n",
      "\n",
      "üìã PLAN DE TROUBLESHOOTING COMPLETO:\n",
      "============================================================\n",
      "üî• PROBLEMA: launcher-v2 faltante indica DSP mal configurado\n",
      "\n",
      "‚úÖ SOLUCI√ìN:\n",
      "1. Reconstruir DSP completamente desde cero\n",
      "2. Usar configuraci√≥n m√≠nima (sin customizaciones)\n",
      "3. Dejar que OpenShift AI use im√°genes por defecto\n",
      "4. Probar con pipeline ultra-simple\n",
      "\n",
      "üìã PASOS EJECUTAR:\n",
      "1. ./rebuild_dsp.sh (reconstruir DSP)\n",
      "2. Esperar 5-10 minutos para que se inicialice\n",
      "3. Verificar: oc get pods -n rag-openshift-ai\n",
      "4. Subir: dsp_verification.yaml al Dashboard\n",
      "5. Si funciona, subir pipelines m√°s complejos\n",
      "\n",
      "‚ö†Ô∏è SI SIGUE FALLANDO:\n",
      "- El cluster puede no tener OpenShift AI instalado correctamente\n",
      "- Verificar con administrador del cluster\n",
      "- Puede necesitar reinstalar OpenShift AI operator\n",
      "\n",
      "üéØ ACCI√ìN REQUERIDA:\n",
      "  ‚ö†Ô∏è  El DSP actual est√° corrupto/mal configurado\n",
      "  üîß SOLUCI√ìN: Reconstruir completamente\n",
      "  üöÄ EJECUTAR: ./rebuild_dsp.sh\n",
      "\n",
      "üìÅ ARCHIVOS CREADOS:\n",
      "  üìÑ deploy/openshift-ai/dsp-minimal.yaml - Configuraci√≥n m√≠nima\n",
      "  üîß rebuild_dsp.sh - Script de reconstrucci√≥n completa\n",
      "  üìÑ dsp_verification.yaml - Pipeline ultra-simple para testing\n",
      "\n",
      "‚è±Ô∏è TIEMPO ESTIMADO:\n",
      "  üïê Rebuild: 5-10 minutos\n",
      "  üïë Verificaci√≥n: 2-3 minutos\n",
      "  üïí Total: ~15 minutos\n",
      "\n",
      "üí° ESTE ENFOQUE DEBER√çA FUNCIONAR:\n",
      "  ‚úÖ Configuraci√≥n m√≠nima sin customizaciones problem√°ticas\n",
      "  ‚úÖ Deja que OpenShift AI use sus im√°genes por defecto\n",
      "  ‚úÖ Elimina cualquier configuraci√≥n corrupta\n"
     ]
    }
   ],
   "source": [
    "def recreate_dsp_completely():\n",
    "    \"\"\"Recrear DSP v2 desde cero con configuraci√≥n correcta\"\"\"\n",
    "    \n",
    "    print(\"üîß Recreando DSP v2 completamente...\")\n",
    "    \n",
    "    # Configuraci√≥n DSP m√≠nima y funcional\n",
    "    dsp_minimal_config = {\n",
    "        \"apiVersion\": \"datasciencepipelinesapplications.opendatahub.io/v1alpha1\",\n",
    "        \"kind\": \"DataSciencePipelinesApplication\",\n",
    "        \"metadata\": {\n",
    "            \"name\": \"rag-dsp-minimal\",\n",
    "            \"namespace\": \"rag-openshift-ai\"\n",
    "        },\n",
    "        \"spec\": {\n",
    "            \"dspVersion\": \"v2\",\n",
    "            \"apiServer\": {\n",
    "                \"deploy\": True,\n",
    "                \"enableSamplePipeline\": False\n",
    "            },\n",
    "            \"objectStorage\": {\n",
    "                \"externalStorage\": {\n",
    "                    \"bucket\": \"pipeline\",\n",
    "                    \"host\": \"minio-api-poc-rag.apps.cluster-2gbhp.2gbhp.sandbox1120.opentlc.com\",\n",
    "                    \"scheme\": \"https\",\n",
    "                    \"s3CredentialsSecret\": {\n",
    "                        \"accessKey\": \"accesskey\",\n",
    "                        \"secretKey\": \"secretkey\",\n",
    "                        \"secretName\": \"minio-secret\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"database\": {\n",
    "                \"mariaDB\": {\n",
    "                    \"deploy\": True,\n",
    "                    \"pvcSize\": \"5Gi\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open('deploy/openshift-ai/dsp-minimal.yaml', 'w') as f:\n",
    "        yaml.dump(dsp_minimal_config, f, indent=2, default_flow_style=False)\n",
    "    \n",
    "    print(\"‚úÖ Configuraci√≥n DSP m√≠nima creada: deploy/openshift-ai/dsp-minimal.yaml\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def create_complete_rebuild_script():\n",
    "    \"\"\"Script para reconstruir DSP completamente\"\"\"\n",
    "    \n",
    "    rebuild_script = '''#!/bin/bash\n",
    "# Complete DSP Rebuild Script\n",
    "# Author: Carlos Estay (pkstaz)\n",
    "\n",
    "set -e\n",
    "\n",
    "echo \"üîß Rebuilding DSP v2 completely...\"\n",
    "\n",
    "# Colors\n",
    "RED='\\\\033[0;31m'\n",
    "GREEN='\\\\033[0;32m'\n",
    "YELLOW='\\\\033[1;33m'\n",
    "NC='\\\\033[0m'\n",
    "\n",
    "NAMESPACE=\"rag-openshift-ai\"\n",
    "\n",
    "# Step 1: Complete cleanup\n",
    "echo -e \"${YELLOW}üóëÔ∏è Complete cleanup...${NC}\"\n",
    "oc delete datasciencepipelinesapplication --all -n $NAMESPACE --ignore-not-found=true\n",
    "\n",
    "# Wait for complete deletion\n",
    "echo -e \"${YELLOW}‚è≥ Waiting for complete cleanup...${NC}\"\n",
    "sleep 30\n",
    "\n",
    "# Delete any remaining resources\n",
    "oc delete pods --all -n $NAMESPACE --ignore-not-found=true\n",
    "oc delete pvc --all -n $NAMESPACE --ignore-not-found=true\n",
    "oc delete configmaps --all -n $NAMESPACE --ignore-not-found=true\n",
    "oc delete secrets --all -n $NAMESPACE --ignore-not-found=true\n",
    "\n",
    "# Wait more\n",
    "sleep 10\n",
    "\n",
    "# Step 2: Recreate namespace fresh\n",
    "echo -e \"${YELLOW}üì¶ Recreating namespace...${NC}\"\n",
    "oc delete namespace $NAMESPACE --ignore-not-found=true\n",
    "sleep 15\n",
    "oc apply -f deploy/openshift-ai/namespace.yaml\n",
    "\n",
    "# Step 3: Apply MinIO secret\n",
    "echo -e \"${YELLOW}üîë Creating MinIO secret...${NC}\"\n",
    "oc apply -f deploy/openshift-ai/minio-secret.yaml\n",
    "\n",
    "# Step 4: Deploy minimal DSP\n",
    "echo -e \"${YELLOW}üî¨ Deploying minimal DSP...${NC}\"\n",
    "oc apply -f deploy/openshift-ai/dsp-minimal.yaml\n",
    "\n",
    "# Step 5: Wait for DSP to be ready\n",
    "echo -e \"${YELLOW}‚è≥ Waiting for DSP to be ready (this may take 5-10 minutes)...${NC}\"\n",
    "oc wait --for=condition=Ready datasciencepipelinesapplication/rag-dsp-minimal -n $NAMESPACE --timeout=600s\n",
    "\n",
    "# Step 6: Check status\n",
    "echo -e \"${YELLOW}üìä Checking final status...${NC}\"\n",
    "oc get datasciencepipelinesapplications -n $NAMESPACE\n",
    "echo \"\"\n",
    "oc get pods -n $NAMESPACE\n",
    "echo \"\"\n",
    "oc get pvc -n $NAMESPACE\n",
    "\n",
    "# Step 7: Verify DSP is working\n",
    "echo -e \"${YELLOW}üîç Verifying DSP functionality...${NC}\"\n",
    "API_SERVER_POD=$(oc get pods -n $NAMESPACE -l app=ds-pipeline-rag-dsp-minimal -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo \"\")\n",
    "\n",
    "if [ ! -z \"$API_SERVER_POD\" ]; then\n",
    "    echo -e \"${GREEN}‚úÖ API Server pod found: $API_SERVER_POD${NC}\"\n",
    "    oc logs $API_SERVER_POD -n $NAMESPACE --tail=10\n",
    "else\n",
    "    echo -e \"${RED}‚ùå API Server pod not found${NC}\"\n",
    "fi\n",
    "\n",
    "echo -e \"${GREEN}üéâ DSP rebuild completed!${NC}\"\n",
    "echo -e \"${YELLOW}üìã Next steps:${NC}\"\n",
    "echo \"1. Wait for all pods to be Running\"\n",
    "echo \"2. Access OpenShift AI Dashboard\"\n",
    "echo \"3. Navigate to Data Science Projects > rag-openshift-ai\"\n",
    "echo \"4. Try uploading a simple pipeline\"\n",
    "'''\n",
    "    \n",
    "    with open('rebuild_dsp.sh', 'w') as f:\n",
    "        f.write(rebuild_script)\n",
    "    \n",
    "    Path('rebuild_dsp.sh').chmod(0o755)\n",
    "    \n",
    "    print(\"‚úÖ Script de rebuild creado: rebuild_dsp.sh\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def create_verification_pipeline():\n",
    "    \"\"\"Crear pipeline super simple para verificar que DSP funciona\"\"\"\n",
    "    \n",
    "    print(\"\\nüß™ Creando pipeline de verificaci√≥n ultra-simple...\")\n",
    "    \n",
    "    try:\n",
    "        from kfp import dsl, compiler\n",
    "        from kfp.dsl import component, pipeline\n",
    "        \n",
    "        @component(base_image=\"python:3.9\")\n",
    "        def verify_component() -> str:\n",
    "            \"\"\"Ultra simple verification component\"\"\"\n",
    "            import os\n",
    "            import sys\n",
    "            \n",
    "            result = f\"DSP is working! Python version: {sys.version}\"\n",
    "            print(result)\n",
    "            return result\n",
    "\n",
    "        @pipeline(\n",
    "            name=\"dsp-verification\",\n",
    "            description=\"Ultra simple pipeline to verify DSP is working\"\n",
    "        )\n",
    "        def verification_pipeline() -> str:\n",
    "            \"\"\"Verification pipeline\"\"\"\n",
    "            \n",
    "            task = verify_component()\n",
    "            task.set_display_name(\"Verify\")\n",
    "            task.set_cpu_limit(\"100m\")\n",
    "            task.set_memory_limit(\"128Mi\")\n",
    "            \n",
    "            return task.output\n",
    "        \n",
    "        # Compilar\n",
    "        verify_file = 'dsp_verification.yaml'\n",
    "        \n",
    "        compiler.Compiler().compile(\n",
    "            pipeline_func=verification_pipeline,\n",
    "            package_path=verify_file\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Pipeline de verificaci√≥n creado: {verify_file}\")\n",
    "        return verify_file\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creando pipeline de verificaci√≥n: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def show_troubleshooting_plan():\n",
    "    \"\"\"Plan completo de troubleshooting\"\"\"\n",
    "    \n",
    "    print(f\"\\nüìã PLAN DE TROUBLESHOOTING COMPLETO:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    plan = [\n",
    "        \"üî• PROBLEMA: launcher-v2 faltante indica DSP mal configurado\",\n",
    "        \"\",\n",
    "        \"‚úÖ SOLUCI√ìN:\",\n",
    "        \"1. Reconstruir DSP completamente desde cero\",\n",
    "        \"2. Usar configuraci√≥n m√≠nima (sin customizaciones)\",\n",
    "        \"3. Dejar que OpenShift AI use im√°genes por defecto\",\n",
    "        \"4. Probar con pipeline ultra-simple\",\n",
    "        \"\",\n",
    "        \"üìã PASOS EJECUTAR:\",\n",
    "        \"1. ./rebuild_dsp.sh (reconstruir DSP)\",\n",
    "        \"2. Esperar 5-10 minutos para que se inicialice\",\n",
    "        \"3. Verificar: oc get pods -n rag-openshift-ai\",\n",
    "        \"4. Subir: dsp_verification.yaml al Dashboard\",\n",
    "        \"5. Si funciona, subir pipelines m√°s complejos\",\n",
    "        \"\",\n",
    "        \"‚ö†Ô∏è SI SIGUE FALLANDO:\",\n",
    "        \"- El cluster puede no tener OpenShift AI instalado correctamente\",\n",
    "        \"- Verificar con administrador del cluster\",\n",
    "        \"- Puede necesitar reinstalar OpenShift AI operator\"\n",
    "    ]\n",
    "    \n",
    "    for item in plan:\n",
    "        print(item)\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Ejecutar todas las soluciones\n",
    "print(\"üîß SOLUCI√ìN DEFINITIVA - Reconstruir DSP v2\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "recreate_dsp_completely()\n",
    "create_complete_rebuild_script()\n",
    "verification_pipeline = create_verification_pipeline()\n",
    "show_troubleshooting_plan()\n",
    "\n",
    "print(f\"\\nüéØ ACCI√ìN REQUERIDA:\")\n",
    "print(f\"  ‚ö†Ô∏è  El DSP actual est√° corrupto/mal configurado\")\n",
    "print(f\"  üîß SOLUCI√ìN: Reconstruir completamente\")\n",
    "print(f\"  üöÄ EJECUTAR: ./rebuild_dsp.sh\")\n",
    "\n",
    "print(f\"\\nüìÅ ARCHIVOS CREADOS:\")\n",
    "print(f\"  üìÑ deploy/openshift-ai/dsp-minimal.yaml - Configuraci√≥n m√≠nima\")\n",
    "print(f\"  üîß rebuild_dsp.sh - Script de reconstrucci√≥n completa\")\n",
    "print(f\"  üìÑ dsp_verification.yaml - Pipeline ultra-simple para testing\")\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è TIEMPO ESTIMADO:\")\n",
    "print(f\"  üïê Rebuild: 5-10 minutos\")\n",
    "print(f\"  üïë Verificaci√≥n: 2-3 minutos\")\n",
    "print(f\"  üïí Total: ~15 minutos\")\n",
    "\n",
    "print(f\"\\nüí° ESTE ENFOQUE DEBER√çA FUNCIONAR:\")\n",
    "print(f\"  ‚úÖ Configuraci√≥n m√≠nima sin customizaciones problem√°ticas\")\n",
    "print(f\"  ‚úÖ Deja que OpenShift AI use sus im√°genes por defecto\")\n",
    "print(f\"  ‚úÖ Elimina cualquier configuraci√≥n corrupta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950f005a-7046-49e1-9859-30c3ab366f89",
   "metadata": {},
   "source": [
    "## üîç Diagn√≥stico del Deployment\n",
    "\n",
    "Verificamos qu√© est√° pasando con el DSP deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9b2b9a4c-9c0a-47a0-b7ce-45f204c075f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Diagnosticando deployment de DSP...\n",
      "üìã Ejecuta estos comandos para diagnosticar:\n",
      "==================================================\n",
      "# Ver estado del DSP\n",
      "oc get datasciencepipelinesapplications -n rag-openshift-ai -o wide\n",
      "\n",
      "# Ver detalles del DSP\n",
      "oc describe datasciencepipelinesapplication rag-pipeline-dsp-v2 -n rag-openshift-ai\n",
      "\n",
      "# Ver pods en el namespace\n",
      "oc get pods -n rag-openshift-ai\n",
      "\n",
      "# Ver eventos recientes\n",
      "oc get events -n rag-openshift-ai --sort-by='.lastTimestamp' | tail -20\n",
      "\n",
      "# Ver logs del operator si existe\n",
      "oc logs -n openshift-operators deployment/rhods-operator --tail=50\n",
      "\n",
      "# Verificar si hay conflictos de recursos\n",
      "oc get all -n rag-openshift-ai\n",
      "\n",
      "# Ver status de namespace\n",
      "oc describe namespace rag-openshift-ai\n",
      "\n",
      "üí° POSIBLES CAUSAS:\n",
      "  1. OpenShift AI Operator no est√° instalado\n",
      "  2. Namespace sin permisos adecuados\n",
      "  3. Recursos insuficientes en el cluster\n",
      "  4. Problema de conectividad con MinIO externo\n",
      "  5. CRD de DataSciencePipelinesApplication no existe\n",
      "\n",
      "üîß VERIFICACIONES R√ÅPIDAS:\n",
      "  # Verificar si el CRD existe:\n",
      "  oc get crd datasciencepipelinesapplications.opendatahub.io\n",
      "\n",
      "  # Verificar OpenShift AI operator:\n",
      "  oc get pods -n openshift-operators | grep rhods\n",
      "\n",
      "  # Verificar permisos:\n",
      "  oc auth can-i create datasciencepipelinesapplications --as=system:serviceaccount:rag-openshift-ai:default\n"
     ]
    }
   ],
   "source": [
    "def diagnose_dsp_deployment():\n",
    "    \"\"\"Diagnosticar problemas con el deployment DSP\"\"\"\n",
    "    \n",
    "    print(\"üîç Diagnosticando deployment de DSP...\")\n",
    "    \n",
    "    diagnostic_commands = [\n",
    "        \"# Ver estado del DSP\",\n",
    "        \"oc get datasciencepipelinesapplications -n rag-openshift-ai -o wide\",\n",
    "        \"\",\n",
    "        \"# Ver detalles del DSP\",\n",
    "        \"oc describe datasciencepipelinesapplication rag-pipeline-dsp-v2 -n rag-openshift-ai\",\n",
    "        \"\",\n",
    "        \"# Ver pods en el namespace\",\n",
    "        \"oc get pods -n rag-openshift-ai\",\n",
    "        \"\",\n",
    "        \"# Ver eventos recientes\",\n",
    "        \"oc get events -n rag-openshift-ai --sort-by='.lastTimestamp' | tail -20\",\n",
    "        \"\",\n",
    "        \"# Ver logs del operator si existe\",\n",
    "        \"oc logs -n openshift-operators deployment/rhods-operator --tail=50\",\n",
    "        \"\",\n",
    "        \"# Verificar si hay conflictos de recursos\",\n",
    "        \"oc get all -n rag-openshift-ai\",\n",
    "        \"\",\n",
    "        \"# Ver status de namespace\",\n",
    "        \"oc describe namespace rag-openshift-ai\"\n",
    "    ]\n",
    "    \n",
    "    print(\"üìã Ejecuta estos comandos para diagnosticar:\")\n",
    "    print(\"=\" * 50)\n",
    "    for cmd in diagnostic_commands:\n",
    "        print(cmd)\n",
    "    \n",
    "    return diagnostic_commands\n",
    "\n",
    "# Mostrar comandos de diagn√≥stico\n",
    "diagnose_dsp_deployment()\n",
    "\n",
    "print(\"\\nüí° POSIBLES CAUSAS:\")\n",
    "print(\"  1. OpenShift AI Operator no est√° instalado\")\n",
    "print(\"  2. Namespace sin permisos adecuados\")\n",
    "print(\"  3. Recursos insuficientes en el cluster\")\n",
    "print(\"  4. Problema de conectividad con MinIO externo\")\n",
    "print(\"  5. CRD de DataSciencePipelinesApplication no existe\")\n",
    "\n",
    "print(\"\\nüîß VERIFICACIONES R√ÅPIDAS:\")\n",
    "print(\"  # Verificar si el CRD existe:\")\n",
    "print(\"  oc get crd datasciencepipelinesapplications.opendatahub.io\")\n",
    "print(\"\")\n",
    "print(\"  # Verificar OpenShift AI operator:\")\n",
    "print(\"  oc get pods -n openshift-operators | grep rhods\")\n",
    "print(\"\")\n",
    "print(\"  # Verificar permisos:\")\n",
    "print(\"  oc auth can-i create datasciencepipelinesapplications --as=system:serviceaccount:rag-openshift-ai:default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53e08b6-22bf-4326-8505-80de8f8a785b",
   "metadata": {},
   "source": [
    "## üéâ DSP Funcionando - Pr√≥ximos Pasos\n",
    "\n",
    "Ahora que el DSP est√° operativo, vamos a probar nuestro pipeline RAG real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "305d0492-9944-4664-b882-acdbb70ba8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ CREANDO PIPELINE RAG REAL\n",
      "============================================================\n",
      "üöÄ Creando pipeline RAG real para el DSP funcional...\n",
      "‚úÖ Pipeline RAG completo compilado: rag_full_pipeline.yaml\n",
      "\n",
      "üéØ PR√ìXIMOS PASOS CON DSP FUNCIONAL:\n",
      "==================================================\n",
      "  1. Subir rag_full_pipeline.yaml al OpenShift AI Dashboard\n",
      "  2. Crear experimento llamado 'rag-testing'\n",
      "  3. Ejecutar pipeline con par√°metros por defecto\n",
      "  4. Monitorear ejecuci√≥n en el Dashboard\n",
      "  5. Verificar que procesa test-document-openshift-ai.txt\n",
      "  6. Revisar logs de cada step\n",
      "  7. Confirmar que genera embeddings exitosamente\n",
      "\n",
      "üìã PAR√ÅMETROS DEL PIPELINE:\n",
      "  üìÑ bucket_name: raw-documents\n",
      "  üìÑ object_key: test-document-openshift-ai.txt\n",
      "  üìÑ minio_endpoint: minio-api-poc-rag.apps.cluster-2gbhp.2gbhp.sandbox1120.opentlc.com\n",
      "  üìÑ minio_access_key: minio\n",
      "  üìÑ minio_secret_key: minio123\n",
      "  üìÑ chunk_size: 512\n",
      "  üìÑ chunk_overlap: 50\n",
      "  üìÑ embedding_model: sentence-transformers/all-MiniLM-L6-v2\n",
      "\n",
      "üí° QU√â ESPERAR:\n",
      "  ‚úÖ Extract Text: Descarga y lee test-document-openshift-ai.txt\n",
      "  ‚úÖ Chunk Text: Divide en ~4-6 chunks de 512 tokens\n",
      "  ‚úÖ Generate Embeddings: Crea vectores de 384 dimensiones\n",
      "  ‚è±Ô∏è Tiempo total: ~5-10 minutos (primera vez descarga modelo)\n",
      "  üìä Output: JSON con chunks y embeddings listos para indexar\n",
      "\n",
      "‚úÖ PIPELINE RAG REAL LISTO:\n",
      "  üìÑ rag_full_pipeline.yaml\n",
      "  üéØ 3 components: Extract ‚Üí Chunk ‚Üí Embeddings\n",
      "  üîó Conecta a MinIO p√∫blico\n",
      "  üß† Usa sentence-transformers para embeddings\n",
      "\n",
      "üöÄ LISTO PARA TESTING REAL:\n",
      "  1. Subir rag_full_pipeline.yaml al Dashboard\n",
      "  2. Ejecutar con documento test\n",
      "  3. Verificar pipeline end-to-end\n",
      "\n",
      "üéâ ¬°PIPELINE RAG FUNCIONANDO EN OPENSHIFT AI!\n"
     ]
    }
   ],
   "source": [
    "def create_real_rag_pipeline():\n",
    "    \"\"\"Crear el pipeline RAG real ahora que DSP funciona\"\"\"\n",
    "    \n",
    "    print(\"üöÄ Creando pipeline RAG real para el DSP funcional...\")\n",
    "    \n",
    "    try:\n",
    "        from kfp import dsl, compiler\n",
    "        from kfp.dsl import component, pipeline\n",
    "        \n",
    "        @component(\n",
    "            base_image=\"python:3.9\",\n",
    "            packages_to_install=[\n",
    "                \"minio==7.1.17\",\n",
    "                \"requests==2.31.0\"\n",
    "            ]\n",
    "        )\n",
    "        def extract_text_real(\n",
    "            bucket_name: str,\n",
    "            object_key: str,\n",
    "            minio_endpoint: str,\n",
    "            minio_access_key: str,\n",
    "            minio_secret_key: str\n",
    "        ) -> str:\n",
    "            \"\"\"Extract text from MinIO document\"\"\"\n",
    "            \n",
    "            from minio import Minio\n",
    "            import tempfile\n",
    "            import os\n",
    "            \n",
    "            print(f\"Extracting text from: {object_key}\")\n",
    "            \n",
    "            try:\n",
    "                # Conectar a MinIO p√∫blico\n",
    "                minio_client = Minio(\n",
    "                    minio_endpoint,\n",
    "                    access_key=minio_access_key,\n",
    "                    secret_key=minio_secret_key,\n",
    "                    secure=True  # HTTPS\n",
    "                )\n",
    "                \n",
    "                # Crear archivo temporal\n",
    "                with tempfile.NamedTemporaryFile(delete=False, suffix='.txt') as temp_file:\n",
    "                    temp_path = temp_file.name\n",
    "                \n",
    "                # Descargar archivo\n",
    "                minio_client.fget_object(bucket_name, object_key, temp_path)\n",
    "                print(f\"File downloaded: {object_key}\")\n",
    "                \n",
    "                # Leer contenido\n",
    "                with open(temp_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                \n",
    "                # Limpiar\n",
    "                os.unlink(temp_path)\n",
    "                \n",
    "                print(f\"Text extracted: {len(content)} characters\")\n",
    "                return content\n",
    "                \n",
    "            except Exception as e:\n",
    "                error_msg = f\"Error extracting text: {str(e)}\"\n",
    "                print(error_msg)\n",
    "                return error_msg\n",
    "\n",
    "        @component(\n",
    "            base_image=\"python:3.9\",\n",
    "            packages_to_install=[\n",
    "                \"tiktoken==0.5.1\"\n",
    "            ]\n",
    "        )\n",
    "        def chunk_text_real(\n",
    "            text_content: str,\n",
    "            chunk_size: int = 512,\n",
    "            chunk_overlap: int = 50\n",
    "        ) -> str:\n",
    "            \"\"\"Chunk text into processable pieces\"\"\"\n",
    "            \n",
    "            import tiktoken\n",
    "            import json\n",
    "            \n",
    "            print(f\"Chunking text: {len(text_content)} characters\")\n",
    "            \n",
    "            try:\n",
    "                # Configurar tokenizer\n",
    "                encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "                \n",
    "                def count_tokens(text: str) -> int:\n",
    "                    return len(encoding.encode(text))\n",
    "                \n",
    "                # Simple chunking\n",
    "                max_chars = chunk_size * 4  # Aprox 1 token = 4 chars\n",
    "                overlap_chars = chunk_overlap * 4\n",
    "                \n",
    "                chunks = []\n",
    "                start = 0\n",
    "                chunk_id = 0\n",
    "                \n",
    "                while start < len(text_content):\n",
    "                    end = min(start + max_chars, len(text_content))\n",
    "                    chunk_text = text_content[start:end]\n",
    "                    \n",
    "                    if chunk_text.strip():\n",
    "                        chunk_data = {\n",
    "                            \"chunk_id\": f\"chunk_{chunk_id:04d}\",\n",
    "                            \"text\": chunk_text.strip(),\n",
    "                            \"token_count\": count_tokens(chunk_text),\n",
    "                            \"char_count\": len(chunk_text)\n",
    "                        }\n",
    "                        chunks.append(chunk_data)\n",
    "                        chunk_id += 1\n",
    "                    \n",
    "                    start = end - overlap_chars if end < len(text_content) else len(text_content)\n",
    "                \n",
    "                result = json.dumps(chunks, ensure_ascii=False)\n",
    "                print(f\"Created {len(chunks)} chunks\")\n",
    "                \n",
    "                return result\n",
    "                \n",
    "            except Exception as e:\n",
    "                error_msg = f\"Error chunking text: {str(e)}\"\n",
    "                print(error_msg)\n",
    "                return error_msg\n",
    "\n",
    "        @component(\n",
    "            base_image=\"python:3.9\",\n",
    "            packages_to_install=[\n",
    "                \"sentence-transformers==2.2.2\",\n",
    "                \"torch==2.0.1\",\n",
    "                \"numpy==1.24.3\"\n",
    "            ]\n",
    "        )\n",
    "        def generate_embeddings_real(\n",
    "            chunks_json: str,\n",
    "            model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        ) -> str:\n",
    "            \"\"\"Generate embeddings for chunks\"\"\"\n",
    "            \n",
    "            import json\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            import torch\n",
    "            \n",
    "            print(f\"Generating embeddings with model: {model_name}\")\n",
    "            \n",
    "            try:\n",
    "                # Cargar chunks\n",
    "                chunks = json.loads(chunks_json)\n",
    "                print(f\"Processing {len(chunks)} chunks\")\n",
    "                \n",
    "                # Cargar modelo\n",
    "                device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "                print(f\"Using device: {device}\")\n",
    "                \n",
    "                model = SentenceTransformer(model_name, device=device)\n",
    "                \n",
    "                # Generar embeddings\n",
    "                texts = [chunk['text'] for chunk in chunks]\n",
    "                embeddings = model.encode(\n",
    "                    texts,\n",
    "                    convert_to_numpy=True,\n",
    "                    normalize_embeddings=True,\n",
    "                    show_progress_bar=True\n",
    "                )\n",
    "                \n",
    "                # Enriquecer chunks con embeddings\n",
    "                for i, chunk in enumerate(chunks):\n",
    "                    chunk['embedding'] = embeddings[i].tolist()\n",
    "                    chunk['embedding_dim'] = len(embeddings[i])\n",
    "                    chunk['embedding_model'] = model_name\n",
    "                \n",
    "                result = json.dumps(chunks, ensure_ascii=False)\n",
    "                print(f\"Embeddings generated: {len(embeddings)} vectors\")\n",
    "                \n",
    "                return result\n",
    "                \n",
    "            except Exception as e:\n",
    "                error_msg = f\"Error generating embeddings: {str(e)}\"\n",
    "                print(error_msg)\n",
    "                return error_msg\n",
    "\n",
    "        @pipeline(\n",
    "            name=\"rag-full-pipeline\",\n",
    "            description=\"Complete RAG Pipeline - Carlos Estay (pkstaz)\"\n",
    "        )\n",
    "        def rag_full_pipeline(\n",
    "            bucket_name: str = \"raw-documents\",\n",
    "            object_key: str = \"test-document-openshift-ai.txt\",\n",
    "            minio_endpoint: str = \"minio-api-poc-rag.apps.cluster-2gbhp.2gbhp.sandbox1120.opentlc.com\",\n",
    "            minio_access_key: str = \"minio\",\n",
    "            minio_secret_key: str = \"minio123\",\n",
    "            chunk_size: int = 512,\n",
    "            chunk_overlap: int = 50,\n",
    "            embedding_model: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        ) -> str:\n",
    "            \"\"\"Complete RAG processing pipeline\"\"\"\n",
    "            \n",
    "            # Step 1: Extract Text\n",
    "            extract_task = extract_text_real(\n",
    "                bucket_name=bucket_name,\n",
    "                object_key=object_key,\n",
    "                minio_endpoint=minio_endpoint,\n",
    "                minio_access_key=minio_access_key,\n",
    "                minio_secret_key=minio_secret_key\n",
    "            )\n",
    "            extract_task.set_display_name(\"Extract Text\")\n",
    "            extract_task.set_cpu_limit(\"500m\")\n",
    "            extract_task.set_memory_limit(\"1Gi\")\n",
    "            extract_task.set_retry(2)\n",
    "            \n",
    "            # Step 2: Chunk Text\n",
    "            chunk_task = chunk_text_real(\n",
    "                text_content=extract_task.output,\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=chunk_overlap\n",
    "            )\n",
    "            chunk_task.set_display_name(\"Chunk Text\")\n",
    "            chunk_task.set_cpu_limit(\"500m\")\n",
    "            chunk_task.set_memory_limit(\"1Gi\")\n",
    "            chunk_task.set_retry(2)\n",
    "            \n",
    "            # Step 3: Generate Embeddings\n",
    "            embedding_task = generate_embeddings_real(\n",
    "                chunks_json=chunk_task.output,\n",
    "                model_name=embedding_model\n",
    "            )\n",
    "            embedding_task.set_display_name(\"Generate Embeddings\")\n",
    "            embedding_task.set_cpu_limit(\"1000m\")\n",
    "            embedding_task.set_memory_limit(\"4Gi\")\n",
    "            embedding_task.set_retry(1)\n",
    "            \n",
    "            return embedding_task.output\n",
    "        \n",
    "        # Compilar pipeline real\n",
    "        real_pipeline_file = 'rag_full_pipeline.yaml'\n",
    "        \n",
    "        compiler.Compiler().compile(\n",
    "            pipeline_func=rag_full_pipeline,\n",
    "            package_path=real_pipeline_file\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Pipeline RAG completo compilado: {real_pipeline_file}\")\n",
    "        return real_pipeline_file\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creando pipeline RAG real: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def show_next_steps():\n",
    "    \"\"\"Mostrar pr√≥ximos pasos ahora que DSP funciona\"\"\"\n",
    "    \n",
    "    print(f\"\\nüéØ PR√ìXIMOS PASOS CON DSP FUNCIONAL:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    steps = [\n",
    "        \"1. Subir rag_full_pipeline.yaml al OpenShift AI Dashboard\",\n",
    "        \"2. Crear experimento llamado 'rag-testing'\", \n",
    "        \"3. Ejecutar pipeline con par√°metros por defecto\",\n",
    "        \"4. Monitorear ejecuci√≥n en el Dashboard\",\n",
    "        \"5. Verificar que procesa test-document-openshift-ai.txt\",\n",
    "        \"6. Revisar logs de cada step\",\n",
    "        \"7. Confirmar que genera embeddings exitosamente\"\n",
    "    ]\n",
    "    \n",
    "    for step in steps:\n",
    "        print(f\"  {step}\")\n",
    "    \n",
    "    print(f\"\\nüìã PAR√ÅMETROS DEL PIPELINE:\")\n",
    "    params = [\n",
    "        \"bucket_name: raw-documents\",\n",
    "        \"object_key: test-document-openshift-ai.txt\", \n",
    "        \"minio_endpoint: minio-api-poc-rag.apps.cluster-2gbhp.2gbhp.sandbox1120.opentlc.com\",\n",
    "        \"minio_access_key: minio\",\n",
    "        \"minio_secret_key: minio123\",\n",
    "        \"chunk_size: 512\",\n",
    "        \"chunk_overlap: 50\",\n",
    "        \"embedding_model: sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    ]\n",
    "    \n",
    "    for param in params:\n",
    "        print(f\"  üìÑ {param}\")\n",
    "    \n",
    "    print(f\"\\nüí° QU√â ESPERAR:\")\n",
    "    expectations = [\n",
    "        \"‚úÖ Extract Text: Descarga y lee test-document-openshift-ai.txt\",\n",
    "        \"‚úÖ Chunk Text: Divide en ~4-6 chunks de 512 tokens\",\n",
    "        \"‚úÖ Generate Embeddings: Crea vectores de 384 dimensiones\",\n",
    "        \"‚è±Ô∏è Tiempo total: ~5-10 minutos (primera vez descarga modelo)\",\n",
    "        \"üìä Output: JSON con chunks y embeddings listos para indexar\"\n",
    "    ]\n",
    "    \n",
    "    for expectation in expectations:\n",
    "        print(f\"  {expectation}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Crear pipeline RAG real\n",
    "print(\"üöÄ CREANDO PIPELINE RAG REAL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "real_pipeline = create_real_rag_pipeline()\n",
    "show_next_steps()\n",
    "\n",
    "if real_pipeline:\n",
    "    print(f\"\\n‚úÖ PIPELINE RAG REAL LISTO:\")\n",
    "    print(f\"  üìÑ {real_pipeline}\")\n",
    "    print(f\"  üéØ 3 components: Extract ‚Üí Chunk ‚Üí Embeddings\")\n",
    "    print(f\"  üîó Conecta a MinIO p√∫blico\")\n",
    "    print(f\"  üß† Usa sentence-transformers para embeddings\")\n",
    "    \n",
    "    print(f\"\\nüöÄ LISTO PARA TESTING REAL:\")\n",
    "    print(f\"  1. Subir {real_pipeline} al Dashboard\")\n",
    "    print(f\"  2. Ejecutar con documento test\")\n",
    "    print(f\"  3. Verificar pipeline end-to-end\")\n",
    "    \n",
    "    print(f\"\\nüéâ ¬°PIPELINE RAG FUNCIONANDO EN OPENSHIFT AI!\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Error creando pipeline RAG real\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4926b921-b52e-48ee-84ee-65f3221c528b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
