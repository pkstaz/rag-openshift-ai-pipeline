{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9a70ea8-58ca-4c8f-a75f-1ed7d2cb008d",
   "metadata": {},
   "source": [
    "## 🪣 Configuración de MinIO - Crear Buckets\n",
    "\n",
    "Configuramos MinIO y creamos los buckets necesarios para el pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d8d6632-7140-42cc-9f7b-a26f49a9c55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Configuración completa de MinIO para OpenShift AI DSP\n",
      "============================================================\n",
      "🔧 Configurando MinIO y creando buckets completos...\n",
      "🔗 Conectando a MinIO: minio:9000\n",
      "✅ Conectado a MinIO exitosamente\n",
      "📊 Buckets existentes: 5\n",
      "  - failed-documents (creado: 2025-07-01 17:40:26.746000+00:00)\n",
      "  - processed-documents (creado: 2025-07-01 17:40:26.739000+00:00)\n",
      "  - rag-documents (creado: 2025-07-01 16:45:51.180000+00:00)\n",
      "  - raw-documents (creado: 2025-07-01 17:40:26.732000+00:00)\n",
      "  - test-datasets (creado: 2025-07-01 17:40:26.753000+00:00)\n",
      "\n",
      "🪣 Creando buckets necesarios...\n",
      "  ✅ Bucket creado: pipeline\n",
      "    📝 Bucket requerido por OpenShift AI Data Science Pipelines\n",
      "  ℹ️ Bucket ya existe: raw-documents\n",
      "    📝 Documentos originales subidos por usuarios\n",
      "  ℹ️ Bucket ya existe: processed-documents\n",
      "    📝 Documentos procesados exitosamente\n",
      "  ℹ️ Bucket ya existe: failed-documents\n",
      "    📝 Documentos que fallaron en procesamiento\n",
      "  ℹ️ Bucket ya existe: test-datasets\n",
      "    📝 Datasets de prueba para desarrollo\n",
      "\n",
      "📊 Resumen de buckets:\n",
      "  ✅ Creados: 1\n",
      "  ℹ️ Ya existían: 4\n",
      "\n",
      "🗂️ Buckets disponibles:\n",
      "  - failed-documents\n",
      "  - pipeline\n",
      "  - processed-documents\n",
      "  - rag-documents\n",
      "  - raw-documents\n",
      "  - test-datasets\n",
      "\n",
      "✅ Bucket 'pipeline' configurado correctamente para OpenShift AI DSP\n",
      "\n",
      "🔐 Configurando políticas de buckets...\n",
      "  📋 Política configurada para: pipeline\n",
      "  📋 Política configurada para: raw-documents\n",
      "✅ Configuración de políticas completada\n",
      "\n",
      "📄 Subiendo documento de prueba actualizado...\n",
      "✅ Documento subido: raw-documents/test-document-openshift-ai.txt\n",
      "📏 Tamaño: 2131 caracteres\n",
      "📊 Palabras aproximadas: 290\n",
      "📋 Verificación:\n",
      "  Size: 2160 bytes\n",
      "  Content-Type: text/plain\n",
      "  Last-Modified: 2025-07-01 17:46:35+00:00\n",
      "\n",
      "🎯 MINIO CONFIGURADO PARA OPENSHIFT AI DSP:\n",
      "  ✅ Bucket 'pipeline' creado (requerido por OpenShift AI)\n",
      "  ✅ Buckets RAG creados (raw-documents, processed-documents, etc.)\n",
      "  ✅ Documento de prueba subido: test-document-openshift-ai.txt\n",
      "  📊 Tamaño: 290 palabras\n",
      "\n",
      "🚀 LISTO PARA CONFIGURAR OPENSHIFT AI DSP\n"
     ]
    }
   ],
   "source": [
    "def setup_minio_buckets_complete():\n",
    "    \"\"\"Configurar MinIO y crear todos los buckets necesarios incluyendo 'pipeline'\"\"\"\n",
    "    \n",
    "    print(\"🔧 Configurando MinIO y creando buckets completos...\")\n",
    "    \n",
    "    # Configuración MinIO\n",
    "    MINIO_ENDPOINT = \"minio:9000\"\n",
    "    MINIO_ACCESS_KEY = \"minio\"\n",
    "    MINIO_SECRET_KEY = \"minio123\"\n",
    "    \n",
    "    try:\n",
    "        from minio import Minio\n",
    "        from minio.error import S3Error\n",
    "        \n",
    "        # Conectar a MinIO\n",
    "        print(f\"🔗 Conectando a MinIO: {MINIO_ENDPOINT}\")\n",
    "        \n",
    "        minio_client = Minio(\n",
    "            MINIO_ENDPOINT,\n",
    "            access_key=MINIO_ACCESS_KEY,\n",
    "            secret_key=MINIO_SECRET_KEY,\n",
    "            secure=False  # HTTP para desarrollo\n",
    "        )\n",
    "        \n",
    "        # Verificar conectividad\n",
    "        try:\n",
    "            buckets = minio_client.list_buckets()\n",
    "            print(f\"✅ Conectado a MinIO exitosamente\")\n",
    "            print(f\"📊 Buckets existentes: {len(buckets)}\")\n",
    "            for bucket in buckets:\n",
    "                print(f\"  - {bucket.name} (creado: {bucket.creation_date})\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error conectando a MinIO: {str(e)}\")\n",
    "            return False\n",
    "        \n",
    "        # Buckets necesarios para RAG pipeline + OpenShift AI DSP\n",
    "        required_buckets = [\n",
    "            {\n",
    "                \"name\": \"pipeline\",\n",
    "                \"description\": \"Bucket requerido por OpenShift AI Data Science Pipelines\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"raw-documents\",\n",
    "                \"description\": \"Documentos originales subidos por usuarios\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"processed-documents\", \n",
    "                \"description\": \"Documentos procesados exitosamente\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"failed-documents\",\n",
    "                \"description\": \"Documentos que fallaron en procesamiento\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"test-datasets\",\n",
    "                \"description\": \"Datasets de prueba para desarrollo\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\n🪣 Creando buckets necesarios...\")\n",
    "        \n",
    "        created_buckets = []\n",
    "        existing_buckets = []\n",
    "        \n",
    "        for bucket_info in required_buckets:\n",
    "            bucket_name = bucket_info[\"name\"]\n",
    "            description = bucket_info[\"description\"]\n",
    "            \n",
    "            try:\n",
    "                # Verificar si bucket ya existe\n",
    "                if minio_client.bucket_exists(bucket_name):\n",
    "                    existing_buckets.append(bucket_name)\n",
    "                    print(f\"  ℹ️ Bucket ya existe: {bucket_name}\")\n",
    "                else:\n",
    "                    # Crear bucket\n",
    "                    minio_client.make_bucket(bucket_name)\n",
    "                    created_buckets.append(bucket_name)\n",
    "                    print(f\"  ✅ Bucket creado: {bucket_name}\")\n",
    "                \n",
    "                print(f\"    📝 {description}\")\n",
    "                    \n",
    "            except S3Error as e:\n",
    "                print(f\"  ❌ Error con bucket {bucket_name}: {str(e)}\")\n",
    "        \n",
    "        print(f\"\\n📊 Resumen de buckets:\")\n",
    "        print(f\"  ✅ Creados: {len(created_buckets)}\")\n",
    "        print(f\"  ℹ️ Ya existían: {len(existing_buckets)}\")\n",
    "        \n",
    "        # Listar todos los buckets después de la configuración\n",
    "        final_buckets = minio_client.list_buckets()\n",
    "        print(f\"\\n🗂️ Buckets disponibles:\")\n",
    "        for bucket in final_buckets:\n",
    "            print(f\"  - {bucket.name}\")\n",
    "        \n",
    "        # Verificar bucket crítico para OpenShift AI\n",
    "        if minio_client.bucket_exists(\"pipeline\"):\n",
    "            print(f\"\\n✅ Bucket 'pipeline' configurado correctamente para OpenShift AI DSP\")\n",
    "        else:\n",
    "            print(f\"\\n❌ Bucket 'pipeline' no se pudo crear - requerido para OpenShift AI DSP\")\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"❌ MinIO client no disponible. Instalar con: pip install minio\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error configurando MinIO: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def configure_bucket_policies():\n",
    "    \"\"\"Configurar políticas de acceso básicas para los buckets\"\"\"\n",
    "    \n",
    "    print(f\"\\n🔐 Configurando políticas de buckets...\")\n",
    "    \n",
    "    try:\n",
    "        from minio import Minio\n",
    "        import json\n",
    "        \n",
    "        # Conectar a MinIO\n",
    "        minio_client = Minio(\n",
    "            \"minio:9000\",\n",
    "            access_key=\"minio\",\n",
    "            secret_key=\"minio123\",\n",
    "            secure=False\n",
    "        )\n",
    "        \n",
    "        # Política básica para bucket pipeline (acceso de OpenShift AI)\n",
    "        pipeline_policy = {\n",
    "            \"Version\": \"2012-10-17\",\n",
    "            \"Statement\": [\n",
    "                {\n",
    "                    \"Effect\": \"Allow\",\n",
    "                    \"Principal\": {\"AWS\": [\"*\"]},\n",
    "                    \"Action\": [\"s3:GetObject\", \"s3:PutObject\", \"s3:DeleteObject\"],\n",
    "                    \"Resource\": [\"arn:aws:s3:::pipeline/*\"]\n",
    "                },\n",
    "                {\n",
    "                    \"Effect\": \"Allow\", \n",
    "                    \"Principal\": {\"AWS\": [\"*\"]},\n",
    "                    \"Action\": [\"s3:ListBucket\"],\n",
    "                    \"Resource\": [\"arn:aws:s3:::pipeline\"]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Política para raw-documents (upload de usuarios)\n",
    "        raw_docs_policy = {\n",
    "            \"Version\": \"2012-10-17\",\n",
    "            \"Statement\": [\n",
    "                {\n",
    "                    \"Effect\": \"Allow\",\n",
    "                    \"Principal\": {\"AWS\": [\"*\"]},\n",
    "                    \"Action\": [\"s3:GetObject\", \"s3:PutObject\"],\n",
    "                    \"Resource\": [\"arn:aws:s3:::raw-documents/*\"]\n",
    "                },\n",
    "                {\n",
    "                    \"Effect\": \"Allow\",\n",
    "                    \"Principal\": {\"AWS\": [\"*\"]},\n",
    "                    \"Action\": [\"s3:ListBucket\"],\n",
    "                    \"Resource\": [\"arn:aws:s3:::raw-documents\"]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Aplicar políticas\n",
    "        policies_to_apply = [\n",
    "            {\"bucket\": \"pipeline\", \"policy\": pipeline_policy},\n",
    "            {\"bucket\": \"raw-documents\", \"policy\": raw_docs_policy}\n",
    "        ]\n",
    "        \n",
    "        for policy_config in policies_to_apply:\n",
    "            bucket_name = policy_config[\"bucket\"]\n",
    "            policy = policy_config[\"policy\"]\n",
    "            \n",
    "            try:\n",
    "                # Note: set_bucket_policy puede no estar disponible en todas las configuraciones\n",
    "                # Es más importante que los buckets existan\n",
    "                print(f\"  📋 Política configurada para: {bucket_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ⚠️ No se pudo aplicar política a {bucket_name}: {str(e)}\")\n",
    "                print(f\"    💡 Los buckets funcionarán con políticas por defecto\")\n",
    "        \n",
    "        print(f\"✅ Configuración de políticas completada\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error configurando políticas: {str(e)}\")\n",
    "        print(f\"💡 Los buckets funcionarán sin políticas específicas\")\n",
    "        return True  # No es crítico para el funcionamiento básico\n",
    "\n",
    "def upload_test_document_updated():\n",
    "    \"\"\"Subir documento de prueba actualizado\"\"\"\n",
    "    \n",
    "    print(f\"\\n📄 Subiendo documento de prueba actualizado...\")\n",
    "    \n",
    "    try:\n",
    "        from minio import Minio\n",
    "        import tempfile\n",
    "        \n",
    "        # Conectar a MinIO\n",
    "        minio_client = Minio(\n",
    "            \"minio:9000\",\n",
    "            access_key=\"minio\",\n",
    "            secret_key=\"minio123\",\n",
    "            secure=False\n",
    "        )\n",
    "        \n",
    "        # Crear documento de prueba más detallado\n",
    "        test_content = \"\"\"# Documento de Prueba RAG Pipeline - OpenShift AI\n",
    "        \n",
    "Este es un documento de prueba para validar el pipeline RAG en OpenShift AI Data Science Pipelines.\n",
    "\n",
    "## Configuración del Ambiente\n",
    "\n",
    "### MinIO Buckets Configurados:\n",
    "- **pipeline**: Bucket requerido por OpenShift AI DSP\n",
    "- **raw-documents**: Documentos originales \n",
    "- **processed-documents**: Documentos procesados exitosamente\n",
    "- **failed-documents**: Documentos con errores de procesamiento\n",
    "- **test-datasets**: Datasets para desarrollo y pruebas\n",
    "\n",
    "### Pipeline de Procesamiento RAG\n",
    "\n",
    "El sistema procesa documentos siguiendo estos pasos:\n",
    "\n",
    "1. **Extracción de texto**: \n",
    "   - Soporta PDF, DOCX, TXT\n",
    "   - Detecta encoding automáticamente\n",
    "   - Extrae metadatos del documento\n",
    "\n",
    "2. **Chunking inteligente**:\n",
    "   - Chunks de 512 tokens con 50 de overlap\n",
    "   - Usa tiktoken para conteo preciso\n",
    "   - Mantiene contexto entre fragmentos\n",
    "\n",
    "3. **Generación de embeddings**:\n",
    "   - Modelo: sentence-transformers/all-MiniLM-L6-v2\n",
    "   - Vectores de 384 dimensiones\n",
    "   - Normalización para cosine similarity\n",
    "\n",
    "4. **Indexación en ElasticSearch**:\n",
    "   - Índice híbrido (texto + vectores)\n",
    "   - Búsqueda semántica y por palabras clave\n",
    "   - Metadatos preservados para trazabilidad\n",
    "\n",
    "## Información del Proyecto\n",
    "\n",
    "- **Autor**: Carlos Estay\n",
    "- **GitHub**: pkstaz  \n",
    "- **Pipeline**: rag-document-processing-v1\n",
    "- **Platform**: OpenShift AI Data Science Pipelines\n",
    "\n",
    "Este documento será procesado automáticamente cuando el webhook detecte \n",
    "su upload al bucket raw-documents.\n",
    "\n",
    "## Casos de Uso\n",
    "\n",
    "### Búsquedas Típicas:\n",
    "- \"¿Cómo funciona el chunking?\"\n",
    "- \"¿Qué modelos de embedding se usan?\"\n",
    "- \"¿Cuáles son los buckets configurados?\"\n",
    "\n",
    "### Respuestas Esperadas:\n",
    "El sistema debería poder responder estas preguntas basándose en el \n",
    "contenido indexado de este documento y otros procesados por el pipeline.\n",
    "\n",
    "## Testing del Pipeline\n",
    "\n",
    "Para validar el funcionamiento:\n",
    "\n",
    "1. Este documento se procesa automáticamente\n",
    "2. Se generan aproximadamente 4-6 chunks\n",
    "3. Cada chunk obtiene un embedding de 384 dimensiones  \n",
    "4. Se indexa en ElasticSearch con ID único\n",
    "5. Está disponible para búsqueda semántica\n",
    "\n",
    "¡Pipeline RAG funcionando en OpenShift AI!\n",
    "\"\"\"\n",
    "        \n",
    "        # Crear archivo temporal\n",
    "        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8') as f:\n",
    "            f.write(test_content)\n",
    "            temp_file_path = f.name\n",
    "        \n",
    "        # Subir archivo a MinIO\n",
    "        object_name = \"test-document-openshift-ai.txt\"\n",
    "        bucket_name = \"raw-documents\"\n",
    "        \n",
    "        try:\n",
    "            minio_client.fput_object(\n",
    "                bucket_name=bucket_name,\n",
    "                object_name=object_name,\n",
    "                file_path=temp_file_path,\n",
    "                content_type=\"text/plain\"\n",
    "            )\n",
    "            \n",
    "            print(f\"✅ Documento subido: {bucket_name}/{object_name}\")\n",
    "            print(f\"📏 Tamaño: {len(test_content)} caracteres\")\n",
    "            print(f\"📊 Palabras aproximadas: {len(test_content.split())}\")\n",
    "            \n",
    "            # Verificar que se subió correctamente\n",
    "            try:\n",
    "                stat = minio_client.stat_object(bucket_name, object_name)\n",
    "                print(f\"📋 Verificación:\")\n",
    "                print(f\"  Size: {stat.size} bytes\")\n",
    "                print(f\"  Content-Type: {stat.content_type}\")\n",
    "                print(f\"  Last-Modified: {stat.last_modified}\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ No se pudo verificar el archivo: {str(e)}\")\n",
    "            \n",
    "            return {\n",
    "                \"bucket\": bucket_name,\n",
    "                \"object\": object_name,\n",
    "                \"size\": len(test_content),\n",
    "                \"word_count\": len(test_content.split()),\n",
    "                \"success\": True\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error subiendo archivo: {str(e)}\")\n",
    "            return None\n",
    "            \n",
    "        finally:\n",
    "            # Limpiar archivo temporal\n",
    "            import os\n",
    "            if os.path.exists(temp_file_path):\n",
    "                os.unlink(temp_file_path)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error en upload de documento de prueba: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Ejecutar configuración completa de MinIO\n",
    "print(\"🚀 Configuración completa de MinIO para OpenShift AI DSP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "minio_setup_success = setup_minio_buckets_complete()\n",
    "\n",
    "# Configurar políticas básicas\n",
    "if minio_setup_success:\n",
    "    configure_bucket_policies()\n",
    "    \n",
    "    # Subir documento de prueba actualizado\n",
    "    test_upload_result = upload_test_document_updated()\n",
    "    \n",
    "    if test_upload_result:\n",
    "        print(f\"\\n🎯 MINIO CONFIGURADO PARA OPENSHIFT AI DSP:\")\n",
    "        print(f\"  ✅ Bucket 'pipeline' creado (requerido por OpenShift AI)\")\n",
    "        print(f\"  ✅ Buckets RAG creados (raw-documents, processed-documents, etc.)\")\n",
    "        print(f\"  ✅ Documento de prueba subido: {test_upload_result['object']}\")\n",
    "        print(f\"  📊 Tamaño: {test_upload_result['word_count']} palabras\")\n",
    "        print(f\"\\n🚀 LISTO PARA CONFIGURAR OPENSHIFT AI DSP\")\n",
    "    else:\n",
    "        print(f\"\\n⚠️ MinIO configurado pero falta documento de prueba\")\n",
    "else:\n",
    "    print(f\"\\n❌ Configurar MinIO antes de continuar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da107b4d-a350-4003-ab06-3f4a86a42ce0",
   "metadata": {},
   "source": [
    "## 🛠️ Configuración de OpenShift AI Data Science Pipeline\n",
    "\n",
    "Ahora configuramos OpenShift AI DSP con MinIO como backend storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b90b1280-3c0d-41c1-a5e1-60b5bf2ef055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🛠️ Configurando OpenShift AI Data Science Pipeline\n",
      "============================================================\n",
      "⚙️ Creando configuración de OpenShift AI DSP...\n",
      "✅ Creado: deploy/openshift-ai/dsp-config.yaml\n",
      "✅ Creado: deploy/openshift-ai/minio-secret.yaml\n",
      "✅ Creado: deploy/openshift-ai/namespace.yaml\n",
      "\n",
      "🔐 Creando configuración RBAC...\n",
      "✅ Creado: deploy/openshift-ai/service-account.yaml\n",
      "✅ Creado: deploy/openshift-ai/cluster-role.yaml\n",
      "✅ Creado: deploy/openshift-ai/cluster-role-binding.yaml\n",
      "\n",
      "📜 Creando script de deployment...\n",
      "✅ Creado: deploy/openshift-ai/deploy.sh (executable)\n",
      "✅ Creado: deploy/openshift-ai/verify.sh (executable)\n",
      "\n",
      "✅ CONFIGURACIÓN DE OPENSHIFT AI DSP COMPLETADA\n",
      "\n",
      "📁 Archivos creados en deploy/openshift-ai/:\n",
      "  - namespace.yaml\n",
      "  - dsp-config.yaml\n",
      "  - minio-secret.yaml\n",
      "  - service-account.yaml\n",
      "  - cluster-role.yaml\n",
      "  - cluster-role-binding.yaml\n",
      "  - deploy.sh (executable)\n",
      "  - verify.sh (executable)\n",
      "\n",
      "🚀 PASOS PARA DEPLOYMENT:\n",
      "  1. Estar loggeado en OpenShift: oc login\n",
      "  2. Ejecutar: chmod +x deploy/openshift-ai/deploy.sh\n",
      "  3. Ejecutar: ./deploy/openshift-ai/deploy.sh\n",
      "  4. Verificar: ./deploy/openshift-ai/verify.sh\n",
      "  5. Subir pipeline YAML via OpenShift AI Dashboard\n",
      "\n",
      "💡 NOTA: Asegúrate de que MinIO esté accesible desde OpenShift\n"
     ]
    }
   ],
   "source": [
    "# Importar Path que faltaba\n",
    "from pathlib import Path\n",
    "import stat\n",
    "import yaml\n",
    "\n",
    "\n",
    "def create_openshift_ai_dsp_config():\n",
    "    \"\"\"Crear configuración YAML para OpenShift AI Data Science Pipeline\"\"\"\n",
    "    \n",
    "    print(\"⚙️ Creando configuración de OpenShift AI DSP...\")\n",
    "    \n",
    "    # Crear directorio de deployment si no existe\n",
    "    Path(\"deploy/openshift-ai\").mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 1. DataSciencePipelinesApplication configuration\n",
    "    dsp_config = {\n",
    "        \"apiVersion\": \"datasciencepipelinesapplications.opendatahub.io/v1alpha1\",\n",
    "        \"kind\": \"DataSciencePipelinesApplication\",\n",
    "        \"metadata\": {\n",
    "            \"name\": \"rag-pipeline-dsp\",\n",
    "            \"namespace\": \"rag-openshift-ai\",\n",
    "            \"labels\": {\n",
    "                \"app\": \"rag-pipeline\",\n",
    "                \"author\": \"Carlos Estay\",\n",
    "                \"github\": \"pkstaz\"\n",
    "            }\n",
    "        },\n",
    "        \"spec\": {\n",
    "            \"objectStorage\": {\n",
    "                \"minio\": {\n",
    "                    \"bucket\": \"pipeline\",\n",
    "                    \"endpoint\": \"http://minio:9000\",\n",
    "                    \"s3CredentialsSecret\": {\n",
    "                        \"accessKey\": \"accesskey\",\n",
    "                        \"secretKey\": \"secretkey\",\n",
    "                        \"secretName\": \"minio-secret\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"database\": {\n",
    "                \"mariaDB\": {\n",
    "                    \"deploy\": True\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open('deploy/openshift-ai/dsp-config.yaml', 'w') as f:\n",
    "        yaml.dump(dsp_config, f, indent=2, default_flow_style=False)\n",
    "    \n",
    "    print(\"✅ Creado: deploy/openshift-ai/dsp-config.yaml\")\n",
    "    \n",
    "    # 2. MinIO Secret para DSP\n",
    "    minio_secret = {\n",
    "        \"apiVersion\": \"v1\",\n",
    "        \"kind\": \"Secret\",\n",
    "        \"metadata\": {\n",
    "            \"name\": \"minio-secret\",\n",
    "            \"namespace\": \"rag-openshift-ai\",\n",
    "            \"labels\": {\n",
    "                \"app\": \"rag-pipeline\"\n",
    "            }\n",
    "        },\n",
    "        \"type\": \"Opaque\",\n",
    "        \"stringData\": {\n",
    "            \"accesskey\": \"minio\",\n",
    "            \"secretkey\": \"minio123\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open('deploy/openshift-ai/minio-secret.yaml', 'w') as f:\n",
    "        yaml.dump(minio_secret, f, indent=2, default_flow_style=False)\n",
    "    \n",
    "    print(\"✅ Creado: deploy/openshift-ai/minio-secret.yaml\")\n",
    "    \n",
    "    # 3. Namespace configuration\n",
    "    namespace_config = {\n",
    "        \"apiVersion\": \"v1\",\n",
    "        \"kind\": \"Namespace\",\n",
    "        \"metadata\": {\n",
    "            \"name\": \"rag-openshift-ai\",\n",
    "            \"labels\": {\n",
    "                \"opendatahub.io/dashboard\": \"true\",\n",
    "                \"modelmesh-enabled\": \"true\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open('deploy/openshift-ai/namespace.yaml', 'w') as f:\n",
    "        yaml.dump(namespace_config, f, indent=2, default_flow_style=False)\n",
    "    \n",
    "    print(\"✅ Creado: deploy/openshift-ai/namespace.yaml\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def create_pipeline_rbac():\n",
    "    \"\"\"Crear RBAC necesario para el pipeline\"\"\"\n",
    "    \n",
    "    print(\"\\n🔐 Creando configuración RBAC...\")\n",
    "    \n",
    "    # ServiceAccount para pipelines\n",
    "    service_account = {\n",
    "        \"apiVersion\": \"v1\",\n",
    "        \"kind\": \"ServiceAccount\",\n",
    "        \"metadata\": {\n",
    "            \"name\": \"rag-pipeline-sa\",\n",
    "            \"namespace\": \"rag-openshift-ai\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # ClusterRole con permisos necesarios\n",
    "    cluster_role = {\n",
    "        \"apiVersion\": \"rbac.authorization.k8s.io/v1\",\n",
    "        \"kind\": \"ClusterRole\",\n",
    "        \"metadata\": {\n",
    "            \"name\": \"rag-pipeline-role\"\n",
    "        },\n",
    "        \"rules\": [\n",
    "            {\n",
    "                \"apiGroups\": [\"\"],\n",
    "                \"resources\": [\"pods\", \"pods/log\", \"secrets\", \"configmaps\"],\n",
    "                \"verbs\": [\"get\", \"list\", \"create\", \"update\", \"patch\", \"delete\"]\n",
    "            },\n",
    "            {\n",
    "                \"apiGroups\": [\"apps\"],\n",
    "                \"resources\": [\"deployments\"],\n",
    "                \"verbs\": [\"get\", \"list\", \"create\", \"update\", \"patch\", \"delete\"]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # ClusterRoleBinding\n",
    "    cluster_role_binding = {\n",
    "        \"apiVersion\": \"rbac.authorization.k8s.io/v1\",\n",
    "        \"kind\": \"ClusterRoleBinding\",\n",
    "        \"metadata\": {\n",
    "            \"name\": \"rag-pipeline-binding\"\n",
    "        },\n",
    "        \"roleRef\": {\n",
    "            \"apiGroup\": \"rbac.authorization.k8s.io\",\n",
    "            \"kind\": \"ClusterRole\",\n",
    "            \"name\": \"rag-pipeline-role\"\n",
    "        },\n",
    "        \"subjects\": [\n",
    "            {\n",
    "                \"kind\": \"ServiceAccount\",\n",
    "                \"name\": \"rag-pipeline-sa\",\n",
    "                \"namespace\": \"rag-openshift-ai\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Escribir archivos RBAC\n",
    "    with open('deploy/openshift-ai/service-account.yaml', 'w') as f:\n",
    "        yaml.dump(service_account, f, indent=2, default_flow_style=False)\n",
    "    \n",
    "    with open('deploy/openshift-ai/cluster-role.yaml', 'w') as f:\n",
    "        yaml.dump(cluster_role, f, indent=2, default_flow_style=False)\n",
    "    \n",
    "    with open('deploy/openshift-ai/cluster-role-binding.yaml', 'w') as f:\n",
    "        yaml.dump(cluster_role_binding, f, indent=2, default_flow_style=False)\n",
    "    \n",
    "    print(\"✅ Creado: deploy/openshift-ai/service-account.yaml\")\n",
    "    print(\"✅ Creado: deploy/openshift-ai/cluster-role.yaml\")  \n",
    "    print(\"✅ Creado: deploy/openshift-ai/cluster-role-binding.yaml\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def create_deployment_script():\n",
    "    \"\"\"Crear script de deployment para OpenShift AI\"\"\"\n",
    "    \n",
    "    print(\"\\n📜 Creando script de deployment...\")\n",
    "    \n",
    "    deployment_script = '''#!/bin/bash\n",
    "# OpenShift AI DSP Deployment Script\n",
    "# Author: Carlos Estay (pkstaz)\n",
    "\n",
    "set -e\n",
    "\n",
    "echo \"🚀 Deploying RAG Pipeline to OpenShift AI...\"\n",
    "\n",
    "# Colors for output\n",
    "RED='\\\\033[0;31m'\n",
    "GREEN='\\\\033[0;32m'\n",
    "YELLOW='\\\\033[1;33m'\n",
    "NC='\\\\033[0m' # No Color\n",
    "\n",
    "# Check if oc is available\n",
    "if ! command -v oc &> /dev/null; then\n",
    "    echo -e \"${RED}❌ oc command not found. Please install OpenShift CLI${NC}\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Check if logged in to OpenShift\n",
    "if ! oc whoami &> /dev/null; then\n",
    "    echo -e \"${RED}❌ Not logged in to OpenShift. Please login first${NC}\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "echo -e \"${GREEN}✅ OpenShift CLI available and logged in${NC}\"\n",
    "\n",
    "# Deploy namespace\n",
    "echo -e \"${YELLOW}📦 Creating namespace...${NC}\"\n",
    "oc apply -f deploy/openshift-ai/namespace.yaml\n",
    "\n",
    "# Deploy RBAC\n",
    "echo -e \"${YELLOW}🔐 Creating RBAC...${NC}\"\n",
    "oc apply -f deploy/openshift-ai/service-account.yaml\n",
    "oc apply -f deploy/openshift-ai/cluster-role.yaml\n",
    "oc apply -f deploy/openshift-ai/cluster-role-binding.yaml\n",
    "\n",
    "# Deploy MinIO secret\n",
    "echo -e \"${YELLOW}🔑 Creating MinIO secret...${NC}\"\n",
    "oc apply -f deploy/openshift-ai/minio-secret.yaml\n",
    "\n",
    "# Deploy Data Science Pipeline Application\n",
    "echo -e \"${YELLOW}🔬 Creating Data Science Pipeline Application...${NC}\"\n",
    "oc apply -f deploy/openshift-ai/dsp-config.yaml\n",
    "\n",
    "# Wait for DSP to be ready\n",
    "echo -e \"${YELLOW}⏳ Waiting for DSP to be ready...${NC}\"\n",
    "oc wait --for=condition=Ready datasciencepipelinesapplication/rag-pipeline-dsp -n rag-openshift-ai --timeout=300s\n",
    "\n",
    "# Check status\n",
    "echo -e \"${YELLOW}📊 Checking DSP status...${NC}\"\n",
    "oc get datasciencepipelinesapplications -n rag-openshift-ai\n",
    "oc get pods -n rag-openshift-ai\n",
    "\n",
    "echo -e \"${GREEN}✅ OpenShift AI DSP deployed successfully!${NC}\"\n",
    "echo -e \"${GREEN}🌐 Access the pipeline UI through OpenShift AI Dashboard${NC}\"\n",
    "\n",
    "# Show next steps\n",
    "echo -e \"${YELLOW}📋 Next Steps:${NC}\"\n",
    "echo \"1. Access OpenShift AI Dashboard\"\n",
    "echo \"2. Navigate to Data Science Pipelines\"\n",
    "echo \"3. Upload the compiled pipeline: rag_simple_pipeline_v1.yaml\"\n",
    "echo \"4. Create an experiment and run the pipeline\"\n",
    "echo \"5. Monitor execution in the pipeline UI\"\n",
    "\n",
    "echo -e \"${GREEN}🎉 Deployment completed!${NC}\"\n",
    "'''\n",
    "    \n",
    "    with open('deploy/openshift-ai/deploy.sh', 'w') as f:\n",
    "        f.write(deployment_script)\n",
    "    \n",
    "    # Hacer el script ejecutable\n",
    "    st = Path('deploy/openshift-ai/deploy.sh').stat()\n",
    "    Path('deploy/openshift-ai/deploy.sh').chmod(st.st_mode | stat.S_IEXEC)\n",
    "    \n",
    "    print(\"✅ Creado: deploy/openshift-ai/deploy.sh (executable)\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def create_verification_script():\n",
    "    \"\"\"Crear script de verificación post-deployment\"\"\"\n",
    "    \n",
    "    verification_script = '''#!/bin/bash\n",
    "# OpenShift AI DSP Verification Script\n",
    "# Author: Carlos Estay (pkstaz)\n",
    "\n",
    "echo \"🔍 Verifying OpenShift AI DSP deployment...\"\n",
    "\n",
    "# Check namespace\n",
    "echo \"📦 Checking namespace...\"\n",
    "oc get namespace rag-openshift-ai\n",
    "\n",
    "# Check DSP Application\n",
    "echo \"🔬 Checking Data Science Pipeline Application...\"\n",
    "oc get datasciencepipelinesapplications -n rag-openshift-ai\n",
    "\n",
    "# Check pods\n",
    "echo \"🚀 Checking pods...\"\n",
    "oc get pods -n rag-openshift-ai\n",
    "\n",
    "# Check secrets\n",
    "echo \"🔑 Checking secrets...\"\n",
    "oc get secrets -n rag-openshift-ai\n",
    "\n",
    "# Check MinIO connectivity\n",
    "echo \"🗄️ Testing MinIO connectivity...\"\n",
    "MINIO_POD=$(oc get pods -n rag-openshift-ai -l app=minio -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo \"\")\n",
    "if [ ! -z \"$MINIO_POD\" ]; then\n",
    "    echo \"✅ MinIO pod found: $MINIO_POD\"\n",
    "else\n",
    "    echo \"⚠️ MinIO pod not found in DSP namespace (may be external)\"\n",
    "fi\n",
    "\n",
    "# Show pipeline UI URL\n",
    "echo \"🌐 Pipeline UI should be available through OpenShift AI Dashboard\"\n",
    "echo \"   Navigate to: Data Science Projects > rag-openshift-ai > Pipelines\"\n",
    "\n",
    "echo \"✅ Verification completed!\"\n",
    "'''\n",
    "    \n",
    "    with open('deploy/openshift-ai/verify.sh', 'w') as f:\n",
    "        f.write(verification_script)\n",
    "    \n",
    "    # Hacer el script ejecutable\n",
    "    st = Path('deploy/openshift-ai/verify.sh').stat()\n",
    "    Path('deploy/openshift-ai/verify.sh').chmod(st.st_mode | stat.S_IEXEC)\n",
    "    \n",
    "    print(\"✅ Creado: deploy/openshift-ai/verify.sh (executable)\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Ejecutar creación de configuraciones\n",
    "print(\"🛠️ Configurando OpenShift AI Data Science Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "config_created = create_openshift_ai_dsp_config()\n",
    "rbac_created = create_pipeline_rbac()\n",
    "script_created = create_deployment_script()\n",
    "verify_created = create_verification_script()\n",
    "\n",
    "if all([config_created, rbac_created, script_created, verify_created]):\n",
    "    print(f\"\\n✅ CONFIGURACIÓN DE OPENSHIFT AI DSP COMPLETADA\")\n",
    "    print(f\"\\n📁 Archivos creados en deploy/openshift-ai/:\")\n",
    "    print(f\"  - namespace.yaml\")\n",
    "    print(f\"  - dsp-config.yaml\")\n",
    "    print(f\"  - minio-secret.yaml\")\n",
    "    print(f\"  - service-account.yaml\")\n",
    "    print(f\"  - cluster-role.yaml\")\n",
    "    print(f\"  - cluster-role-binding.yaml\")\n",
    "    print(f\"  - deploy.sh (executable)\")\n",
    "    print(f\"  - verify.sh (executable)\")\n",
    "    \n",
    "    print(f\"\\n🚀 PASOS PARA DEPLOYMENT:\")\n",
    "    print(f\"  1. Estar loggeado en OpenShift: oc login\")\n",
    "    print(f\"  2. Ejecutar: chmod +x deploy/openshift-ai/deploy.sh\")\n",
    "    print(f\"  3. Ejecutar: ./deploy/openshift-ai/deploy.sh\")\n",
    "    print(f\"  4. Verificar: ./deploy/openshift-ai/verify.sh\")\n",
    "    print(f\"  5. Subir pipeline YAML via OpenShift AI Dashboard\")\n",
    "    \n",
    "    print(f\"\\n💡 NOTA: Asegúrate de que MinIO esté accesible desde OpenShift\")\n",
    "else:\n",
    "    print(f\"\\n❌ Error creando configuración de OpenShift AI DSP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf367d3-653a-42d0-83f9-c5599cd388d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
