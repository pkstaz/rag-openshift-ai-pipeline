{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93aec484-3cea-471f-967e-325519f7a133",
   "metadata": {},
   "source": [
    "# 00 - Initial Configuration\n",
    "\n",
    "## 🎯 Objetivo\n",
    "Crear, compilar y deployar el pipeline RAG en OpenShift AI Data Science Pipelines.\n",
    "Este notebook toma los components desarrollados en el notebook anterior y los prepara para deployment real.\n",
    "\n",
    "## 📋 Lo que Haremos\n",
    "1. **Configurar MinIO con buckets necesarios**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1800f3a",
   "metadata": {},
   "source": [
    "## 🔧 Setup Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee277a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ KubeFlow Pipelines disponible: 2.12.1\n",
      "✅ MinIO client disponible\n",
      "🚀 Notebook de deployment iniciado\n",
      "📁 Directorio actual: /opt/app-root/src\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Verificar KFP y dependencias\n",
    "try:\n",
    "    import kfp\n",
    "    from kfp import dsl, compiler\n",
    "    from kfp.client import Client\n",
    "    from kfp.dsl import component, pipeline, Input, Output, Dataset\n",
    "    print(f\"✅ KubeFlow Pipelines disponible: {kfp.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"❌ Instalando KubeFlow Pipelines...\")\n",
    "    !pip install kfp>=2.0.0\n",
    "\n",
    "# Verificar MinIO client\n",
    "try:\n",
    "    from minio import Minio\n",
    "    print(\"✅ MinIO client disponible\")\n",
    "except ImportError:\n",
    "    print(\"❌ Instalando MinIO client...\")\n",
    "    !pip install minio\n",
    "\n",
    "print(\"🚀 Notebook de deployment iniciado\")\n",
    "print(f\"📁 Directorio actual: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a861af26-a347-462a-8327-9f2f8fa2ed8e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ⚙️ Configuration Files\n",
    "\n",
    "Creamos los archivos de configuración necesarios para el deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b60954e6-e468-4e11-a685-82ce1a11d350",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datetime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 177\u001b[39m\n\u001b[32m    174\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m  config/env.template - Variables de ambiente\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    176\u001b[39m \u001b[38;5;66;03m# Crear archivos de configuración\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m \u001b[43mcreate_configuration_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mcreate_configuration_files\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Crear archivos de configuración del proyecto\"\"\"\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 1. Pipeline Configuration\u001b[39;00m\n\u001b[32m      5\u001b[39m pipeline_config = {\n\u001b[32m      6\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpipeline\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m      7\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mrag-document-processing-v1\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      8\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mversion\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m1.0.0\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdescription\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mRAG Document Processing Pipeline para OpenShift AI\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     10\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mauthor\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mCarlos Estay\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     11\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mgithub\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mpkstaz\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcreated\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mdatetime\u001b[49m.now().isoformat()\n\u001b[32m     13\u001b[39m     },\n\u001b[32m     14\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcomponents\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m     15\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mbase_image\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mpytorch/pytorch:2.0.1-cuda11.7-cudnn8-devel\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     16\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mextract_text\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m     17\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mcpu_limit\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m500m\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     18\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmemory_limit\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m1Gi\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     19\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mretry_limit\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m3\u001b[39m\n\u001b[32m     20\u001b[39m         },\n\u001b[32m     21\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mchunk_text\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m     22\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mcpu_limit\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m500m\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m     23\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmemory_limit\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m1Gi\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     24\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mretry_limit\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m3\u001b[39m\n\u001b[32m     25\u001b[39m         },\n\u001b[32m     26\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mgenerate_embeddings\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m     27\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mcpu_limit\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m1000m\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     28\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmemory_limit\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m4Gi\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m     29\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mretry_limit\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m2\u001b[39m,\n\u001b[32m     30\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mgpu_limit\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m0\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# Set to \"1\" si hay GPU disponible\u001b[39;00m\n\u001b[32m     31\u001b[39m         },\n\u001b[32m     32\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mindex_elasticsearch\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m     33\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mcpu_limit\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m500m\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     34\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmemory_limit\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m2Gi\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     35\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mretry_limit\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m3\u001b[39m\n\u001b[32m     36\u001b[39m         }\n\u001b[32m     37\u001b[39m     },\n\u001b[32m     38\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdefault_parameters\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m     39\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mchunk_size\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m512\u001b[39m,\n\u001b[32m     40\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mchunk_overlap\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m50\u001b[39m,\n\u001b[32m     41\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33membedding_model\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msentence-transformers/all-MiniLM-L6-v2\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     42\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mes_index\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mrag-documents\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     43\u001b[39m     },\n\u001b[32m     44\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mstorage\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m     45\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mminio\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m     46\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mendpoint\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mminio:9000\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     47\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33maccess_key\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mminio\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     48\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33msecret_key\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mminio123\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     49\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mbucket_raw\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mraw-documents\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     50\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mbucket_processed\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mprocessed-documents\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     51\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mbucket_failed\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mfailed-documents\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     52\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mbucket_pipeline\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mpipeline\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     53\u001b[39m         },\n\u001b[32m     54\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33melasticsearch\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m     55\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mendpoint\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33melasticsearch:9200\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     56\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mindex_prefix\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mrag-\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     57\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mreplicas\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0\u001b[39m,\n\u001b[32m     58\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mshards\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m1\u001b[39m\n\u001b[32m     59\u001b[39m         }\n\u001b[32m     60\u001b[39m     }\n\u001b[32m     61\u001b[39m }\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mconfig/pipeline_config.yaml\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     64\u001b[39m     yaml.dump(pipeline_config, f, indent=\u001b[32m2\u001b[39m, default_flow_style=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'datetime' is not defined"
     ]
    }
   ],
   "source": [
    "def create_configuration_files():\n",
    "    \"\"\"Crear archivos de configuración del proyecto\"\"\"\n",
    "    \n",
    "    # 1. Pipeline Configuration\n",
    "    pipeline_config = {\n",
    "        \"pipeline\": {\n",
    "            \"name\": \"rag-document-processing-v1\",\n",
    "            \"version\": \"1.0.0\",\n",
    "            \"description\": \"RAG Document Processing Pipeline para OpenShift AI\",\n",
    "            \"author\": \"Carlos Estay\",\n",
    "            \"github\": \"pkstaz\",\n",
    "            \"created\": datetime.now().isoformat()\n",
    "        },\n",
    "        \"components\": {\n",
    "            \"base_image\": \"pytorch/pytorch:2.0.1-cuda11.7-cudnn8-devel\",\n",
    "            \"extract_text\": {\n",
    "                \"cpu_limit\": \"500m\",\n",
    "                \"memory_limit\": \"1Gi\",\n",
    "                \"retry_limit\": 3\n",
    "            },\n",
    "            \"chunk_text\": {\n",
    "                \"cpu_limit\": \"500m\", \n",
    "                \"memory_limit\": \"1Gi\",\n",
    "                \"retry_limit\": 3\n",
    "            },\n",
    "            \"generate_embeddings\": {\n",
    "                \"cpu_limit\": \"1000m\",\n",
    "                \"memory_limit\": \"4Gi\", \n",
    "                \"retry_limit\": 2,\n",
    "                \"gpu_limit\": \"0\"  # Set to \"1\" si hay GPU disponible\n",
    "            },\n",
    "            \"index_elasticsearch\": {\n",
    "                \"cpu_limit\": \"500m\",\n",
    "                \"memory_limit\": \"2Gi\",\n",
    "                \"retry_limit\": 3\n",
    "            }\n",
    "        },\n",
    "        \"default_parameters\": {\n",
    "            \"chunk_size\": 512,\n",
    "            \"chunk_overlap\": 50,\n",
    "            \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "            \"es_index\": \"rag-documents\"\n",
    "        },\n",
    "        \"storage\": {\n",
    "            \"minio\": {\n",
    "                \"endpoint\": \"minio:9000\",\n",
    "                \"access_key\": \"minio\",\n",
    "                \"secret_key\": \"minio123\",\n",
    "                \"bucket_raw\": \"raw-documents\",\n",
    "                \"bucket_processed\": \"processed-documents\",\n",
    "                \"bucket_failed\": \"failed-documents\",\n",
    "                \"bucket_pipeline\": \"pipeline\"\n",
    "            },\n",
    "            \"elasticsearch\": {\n",
    "                \"endpoint\": \"elasticsearch:9200\",\n",
    "                \"index_prefix\": \"rag-\",\n",
    "                \"replicas\": 0,\n",
    "                \"shards\": 1\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open('config/pipeline_config.yaml', 'w') as f:\n",
    "        yaml.dump(pipeline_config, f, indent=2, default_flow_style=False)\n",
    "    \n",
    "    print(\"✅ Creado: config/pipeline_config.yaml\")\n",
    "    \n",
    "    # 2. Secrets Template\n",
    "    secrets_template = {\n",
    "        \"apiVersion\": \"v1\",\n",
    "        \"kind\": \"Secret\",\n",
    "        \"metadata\": {\n",
    "            \"name\": \"rag-pipeline-secrets\",\n",
    "            \"namespace\": \"rag-openshift-ai\",\n",
    "            \"labels\": {\n",
    "                \"app\": \"rag-pipeline\",\n",
    "                \"author\": \"carlos-estay\",\n",
    "                \"github\": \"pkstaz\"\n",
    "            }\n",
    "        },\n",
    "        \"type\": \"Opaque\",\n",
    "        \"stringData\": {\n",
    "            \"minio-access-key\": \"minio\",\n",
    "            \"minio-secret-key\": \"minio123\",\n",
    "            \"elasticsearch-username\": \"\",\n",
    "            \"elasticsearch-password\": \"\",\n",
    "            \"pipeline-webhook-token\": \"rag-pipeline-token-change-me\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open('config/secrets.yaml', 'w') as f:\n",
    "        yaml.dump(secrets_template, f, indent=2, default_flow_style=False)\n",
    "    \n",
    "    print(\"✅ Creado: config/secrets.yaml\")\n",
    "    \n",
    "    # 3. Requirements file\n",
    "    requirements_content = '''# RAG Pipeline Requirements\n",
    "# Author: Carlos Estay (github: pkstaz)\n",
    "# Core KFP\n",
    "kfp>=2.0.0\n",
    "\n",
    "# Document Processing\n",
    "PyPDF2==3.0.1\n",
    "python-docx==0.8.11\n",
    "chardet==5.2.0\n",
    "\n",
    "# Text Processing\n",
    "tiktoken==0.5.1\n",
    "langchain==0.0.350\n",
    "\n",
    "# ML/Embeddings\n",
    "sentence-transformers==2.2.2\n",
    "torch>=2.0.1\n",
    "numpy==1.24.3\n",
    "\n",
    "# Storage/DB\n",
    "minio==7.1.17\n",
    "elasticsearch==8.11.0\n",
    "\n",
    "# Web/API (for webhook)\n",
    "flask==2.3.3\n",
    "requests==2.31.0\n",
    "\n",
    "# Utilities\n",
    "pyyaml==6.0.1\n",
    "python-dateutil==2.8.2\n",
    "'''\n",
    "    \n",
    "    with open('requirements.txt', 'w') as f:\n",
    "        f.write(requirements_content)\n",
    "    \n",
    "    print(\"✅ Creado: requirements.txt\")\n",
    "    \n",
    "    # 4. Environment variables template\n",
    "    env_template = '''# RAG Pipeline Environment Variables\n",
    "# Author: Carlos Estay (github: pkstaz)\n",
    "# Copy to .env and modify as needed\n",
    "\n",
    "# MinIO Configuration\n",
    "MINIO_ENDPOINT=minio:9000\n",
    "MINIO_ACCESS_KEY=minio\n",
    "MINIO_SECRET_KEY=minio123\n",
    "MINIO_SECURE=false\n",
    "\n",
    "# ElasticSearch Configuration  \n",
    "ES_ENDPOINT=elasticsearch:9200\n",
    "ES_INDEX=rag-documents\n",
    "ES_USERNAME=\n",
    "ES_PASSWORD=\n",
    "\n",
    "# Pipeline Configuration\n",
    "CHUNK_SIZE=512\n",
    "CHUNK_OVERLAP=50\n",
    "EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2\n",
    "\n",
    "# OpenShift AI Pipeline\n",
    "KFP_HOST=http://ml-pipeline:8888\n",
    "PIPELINE_NAMESPACE=rag-openshift-ai\n",
    "\n",
    "# Webhook Configuration\n",
    "WEBHOOK_PORT=8080\n",
    "WEBHOOK_TOKEN=rag-pipeline-token-change-me\n",
    "'''\n",
    "    \n",
    "    with open('config/env.template', 'w') as f:\n",
    "        f.write(env_template)\n",
    "    \n",
    "    print(\"✅ Creado: config/env.template\")\n",
    "    \n",
    "    print(\"\\n📋 Archivos de configuración creados:\")\n",
    "    print(\"  config/pipeline_config.yaml - Configuración del pipeline\")\n",
    "    print(\"  config/secrets.yaml - Template de secrets K8s\")\n",
    "    print(\"  requirements.txt - Dependencias Python\")  \n",
    "    print(\"  config/env.template - Variables de ambiente\")\n",
    "\n",
    "# Crear archivos de configuración\n",
    "create_configuration_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432bfe28-97fd-4f7c-ba34-f2d3069338c0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 🪣 Configuración de MinIO - Crear Buckets\n",
    "\n",
    "Configuramos MinIO y creamos todos los buckets necesarios incluyendo el bucket \"pipeline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ced5bd2-2026-4962-a90b-060016d41b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Configuración completa de MinIO para OpenShift AI DSP\n",
      "============================================================\n",
      "🔧 Configurando MinIO y creando buckets completos...\n",
      "🔗 Conectando a MinIO: minio:9000\n",
      "✅ Conectado a MinIO exitosamente\n",
      "📊 Buckets existentes: 6\n",
      "  - failed-documents (creado: 2025-07-01 17:40:26.746000+00:00)\n",
      "  - pipeline (creado: 2025-07-01 17:46:35.035000+00:00)\n",
      "  - processed-documents (creado: 2025-07-01 17:40:26.739000+00:00)\n",
      "  - rag-documents (creado: 2025-07-01 16:45:51.180000+00:00)\n",
      "  - raw-documents (creado: 2025-07-01 17:40:26.732000+00:00)\n",
      "  - test-datasets (creado: 2025-07-01 17:40:26.753000+00:00)\n",
      "\n",
      "🪣 Creando buckets necesarios...\n",
      "  ℹ️ Bucket ya existe: pipeline\n",
      "    📝 Bucket requerido por OpenShift AI Data Science Pipelines\n",
      "  ℹ️ Bucket ya existe: raw-documents\n",
      "    📝 Documentos originales subidos por usuarios\n",
      "  ℹ️ Bucket ya existe: processed-documents\n",
      "    📝 Documentos procesados exitosamente\n",
      "  ℹ️ Bucket ya existe: failed-documents\n",
      "    📝 Documentos que fallaron en procesamiento\n",
      "  ℹ️ Bucket ya existe: test-datasets\n",
      "    📝 Datasets de prueba para desarrollo\n",
      "\n",
      "📊 Resumen de buckets:\n",
      "  ✅ Creados: 0\n",
      "  ℹ️ Ya existían: 5\n",
      "\n",
      "🗂️ Buckets disponibles:\n",
      "  - failed-documents\n",
      "  - pipeline\n",
      "  - processed-documents\n",
      "  - rag-documents\n",
      "  - raw-documents\n",
      "  - test-datasets\n",
      "\n",
      "✅ Bucket 'pipeline' configurado correctamente para OpenShift AI DSP\n",
      "\n",
      "📄 Subiendo documento de prueba...\n",
      "✅ Documento subido: raw-documents/test-document-openshift-ai.txt\n",
      "📏 Tamaño: 1809 caracteres\n",
      "📊 Palabras aproximadas: 241\n",
      "📋 Verificación:\n",
      "  Size: 1828 bytes\n",
      "  Content-Type: text/plain\n",
      "  Last-Modified: 2025-07-01 18:55:05+00:00\n",
      "\n",
      "🎯 MINIO CONFIGURADO PARA OPENSHIFT AI DSP:\n",
      "  ✅ Bucket 'pipeline' creado (requerido por OpenShift AI)\n",
      "  ✅ Buckets RAG creados (raw-documents, processed-documents, etc.)\n",
      "  ✅ Documento de prueba subido: test-document-openshift-ai.txt\n",
      "  📊 Tamaño: 241 palabras\n",
      "\n",
      "🚀 LISTO PARA CONFIGURAR OPENSHIFT AI DSP\n"
     ]
    }
   ],
   "source": [
    "def setup_minio_buckets_complete():\n",
    "    \"\"\"Configurar MinIO y crear todos los buckets necesarios incluyendo 'pipeline'\"\"\"\n",
    "    \n",
    "    print(\"🔧 Configurando MinIO y creando buckets completos...\")\n",
    "    \n",
    "    # Configuración MinIO\n",
    "    MINIO_ENDPOINT = \"minio:9000\"\n",
    "    MINIO_ACCESS_KEY = \"minio\"\n",
    "    MINIO_SECRET_KEY = \"minio123\"\n",
    "    \n",
    "    try:\n",
    "        from minio import Minio\n",
    "        from minio.error import S3Error\n",
    "        \n",
    "        # Conectar a MinIO\n",
    "        print(f\"🔗 Conectando a MinIO: {MINIO_ENDPOINT}\")\n",
    "        \n",
    "        minio_client = Minio(\n",
    "            MINIO_ENDPOINT,\n",
    "            access_key=MINIO_ACCESS_KEY,\n",
    "            secret_key=MINIO_SECRET_KEY,\n",
    "            secure=False  # HTTP para desarrollo\n",
    "        )\n",
    "        \n",
    "        # Verificar conectividad\n",
    "        try:\n",
    "            buckets = minio_client.list_buckets()\n",
    "            print(f\"✅ Conectado a MinIO exitosamente\")\n",
    "            print(f\"📊 Buckets existentes: {len(buckets)}\")\n",
    "            for bucket in buckets:\n",
    "                print(f\"  - {bucket.name} (creado: {bucket.creation_date})\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error conectando a MinIO: {str(e)}\")\n",
    "            return False\n",
    "        \n",
    "        # Buckets necesarios para RAG pipeline + OpenShift AI DSP\n",
    "        required_buckets = [\n",
    "            {\n",
    "                \"name\": \"pipeline\",\n",
    "                \"description\": \"Bucket requerido por OpenShift AI Data Science Pipelines\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"raw-documents\",\n",
    "                \"description\": \"Documentos originales subidos por usuarios\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"processed-documents\", \n",
    "                \"description\": \"Documentos procesados exitosamente\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"failed-documents\",\n",
    "                \"description\": \"Documentos que fallaron en procesamiento\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"test-datasets\",\n",
    "                \"description\": \"Datasets de prueba para desarrollo\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\n🪣 Creando buckets necesarios...\")\n",
    "        \n",
    "        created_buckets = []\n",
    "        existing_buckets = []\n",
    "        \n",
    "        for bucket_info in required_buckets:\n",
    "            bucket_name = bucket_info[\"name\"]\n",
    "            description = bucket_info[\"description\"]\n",
    "            \n",
    "            try:\n",
    "                # Verificar si bucket ya existe\n",
    "                if minio_client.bucket_exists(bucket_name):\n",
    "                    existing_buckets.append(bucket_name)\n",
    "                    print(f\"  ℹ️ Bucket ya existe: {bucket_name}\")\n",
    "                else:\n",
    "                    # Crear bucket\n",
    "                    minio_client.make_bucket(bucket_name)\n",
    "                    created_buckets.append(bucket_name)\n",
    "                    print(f\"  ✅ Bucket creado: {bucket_name}\")\n",
    "                \n",
    "                print(f\"    📝 {description}\")\n",
    "                    \n",
    "            except S3Error as e:\n",
    "                print(f\"  ❌ Error con bucket {bucket_name}: {str(e)}\")\n",
    "        \n",
    "        print(f\"\\n📊 Resumen de buckets:\")\n",
    "        print(f\"  ✅ Creados: {len(created_buckets)}\")\n",
    "        print(f\"  ℹ️ Ya existían: {len(existing_buckets)}\")\n",
    "        \n",
    "        # Listar todos los buckets después de la configuración\n",
    "        final_buckets = minio_client.list_buckets()\n",
    "        print(f\"\\n🗂️ Buckets disponibles:\")\n",
    "        for bucket in final_buckets:\n",
    "            print(f\"  - {bucket.name}\")\n",
    "        \n",
    "        # Verificar bucket crítico para OpenShift AI\n",
    "        if minio_client.bucket_exists(\"pipeline\"):\n",
    "            print(f\"\\n✅ Bucket 'pipeline' configurado correctamente para OpenShift AI DSP\")\n",
    "        else:\n",
    "            print(f\"\\n❌ Bucket 'pipeline' no se pudo crear - requerido para OpenShift AI DSP\")\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"❌ MinIO client no disponible. Instalar con: pip install minio\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error configurando MinIO: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def upload_test_document():\n",
    "    \"\"\"Subir documento de prueba al bucket raw-documents\"\"\"\n",
    "    \n",
    "    print(f\"\\n📄 Subiendo documento de prueba...\")\n",
    "    \n",
    "    try:\n",
    "        from minio import Minio\n",
    "        import tempfile\n",
    "        \n",
    "        # Conectar a MinIO\n",
    "        minio_client = Minio(\n",
    "            \"minio:9000\",\n",
    "            access_key=\"minio\",\n",
    "            secret_key=\"minio123\",\n",
    "            secure=False\n",
    "        )\n",
    "        \n",
    "        # Crear documento de prueba más detallado\n",
    "        test_content = \"\"\"# Documento de Prueba RAG Pipeline - OpenShift AI\n",
    "        \n",
    "Este es un documento de prueba para validar el pipeline RAG en OpenShift AI Data Science Pipelines.\n",
    "\n",
    "## Configuración del Ambiente\n",
    "\n",
    "### MinIO Buckets Configurados:\n",
    "- **pipeline**: Bucket requerido por OpenShift AI DSP\n",
    "- **raw-documents**: Documentos originales \n",
    "- **processed-documents**: Documentos procesados exitosamente\n",
    "- **failed-documents**: Documentos con errores de procesamiento\n",
    "- **test-datasets**: Datasets para desarrollo y pruebas\n",
    "\n",
    "### Pipeline de Procesamiento RAG\n",
    "\n",
    "El sistema procesa documentos siguiendo estos pasos:\n",
    "\n",
    "1. **Extracción de texto**: \n",
    "   - Soporta PDF, DOCX, TXT\n",
    "   - Detecta encoding automáticamente\n",
    "   - Extrae metadatos del documento\n",
    "\n",
    "2. **Chunking inteligente**:\n",
    "   - Chunks de 512 tokens con 50 de overlap\n",
    "   - Usa tiktoken para conteo preciso\n",
    "   - Mantiene contexto entre fragmentos\n",
    "\n",
    "3. **Generación de embeddings**:\n",
    "   - Modelo: sentence-transformers/all-MiniLM-L6-v2\n",
    "   - Vectores de 384 dimensiones\n",
    "   - Normalización para cosine similarity\n",
    "\n",
    "4. **Indexación en ElasticSearch**:\n",
    "   - Índice híbrido (texto + vectores)\n",
    "   - Búsqueda semántica y por palabras clave\n",
    "   - Metadatos preservados para trazabilidad\n",
    "\n",
    "## Información del Proyecto\n",
    "\n",
    "- **Autor**: Carlos Estay\n",
    "- **GitHub**: pkstaz  \n",
    "- **Pipeline**: rag-document-processing-v1\n",
    "- **Platform**: OpenShift AI Data Science Pipelines\n",
    "\n",
    "Este documento será procesado automáticamente cuando el webhook detecte \n",
    "su upload al bucket raw-documents.\n",
    "\n",
    "## Testing del Pipeline\n",
    "\n",
    "Para validar el funcionamiento:\n",
    "\n",
    "1. Este documento se procesa automáticamente\n",
    "2. Se generan aproximadamente 4-6 chunks\n",
    "3. Cada chunk obtiene un embedding de 384 dimensiones  \n",
    "4. Se indexa en ElasticSearch con ID único\n",
    "5. Está disponible para búsqueda semántica\n",
    "\n",
    "¡Pipeline RAG funcionando en OpenShift AI!\n",
    "\"\"\"\n",
    "        \n",
    "        # Crear archivo temporal\n",
    "        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8') as f:\n",
    "            f.write(test_content)\n",
    "            temp_file_path = f.name\n",
    "        \n",
    "        # Subir archivo a MinIO\n",
    "        object_name = \"test-document-openshift-ai.txt\"\n",
    "        bucket_name = \"raw-documents\"\n",
    "        \n",
    "        try:\n",
    "            minio_client.fput_object(\n",
    "                bucket_name=bucket_name,\n",
    "                object_name=object_name,\n",
    "                file_path=temp_file_path,\n",
    "                content_type=\"text/plain\"\n",
    "            )\n",
    "            \n",
    "            print(f\"✅ Documento subido: {bucket_name}/{object_name}\")\n",
    "            print(f\"📏 Tamaño: {len(test_content)} caracteres\")\n",
    "            print(f\"📊 Palabras aproximadas: {len(test_content.split())}\")\n",
    "            \n",
    "            # Verificar que se subió correctamente\n",
    "            try:\n",
    "                stat = minio_client.stat_object(bucket_name, object_name)\n",
    "                print(f\"📋 Verificación:\")\n",
    "                print(f\"  Size: {stat.size} bytes\")\n",
    "                print(f\"  Content-Type: {stat.content_type}\")\n",
    "                print(f\"  Last-Modified: {stat.last_modified}\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ No se pudo verificar el archivo: {str(e)}\")\n",
    "            \n",
    "            return {\n",
    "                \"bucket\": bucket_name,\n",
    "                \"object\": object_name,\n",
    "                \"size\": len(test_content),\n",
    "                \"word_count\": len(test_content.split()),\n",
    "                \"success\": True\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error subiendo archivo: {str(e)}\")\n",
    "            return None\n",
    "            \n",
    "        finally:\n",
    "            # Limpiar archivo temporal\n",
    "            import os\n",
    "            if os.path.exists(temp_file_path):\n",
    "                os.unlink(temp_file_path)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error en upload de documento de prueba: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Ejecutar configuración completa de MinIO\n",
    "print(\"🚀 Configuración completa de MinIO para OpenShift AI DSP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "minio_setup_success = setup_minio_buckets_complete()\n",
    "\n",
    "# Subir documento de prueba si MinIO está configurado\n",
    "if minio_setup_success:\n",
    "    test_upload_result = upload_test_document()\n",
    "    \n",
    "    if test_upload_result:\n",
    "        print(f\"\\n🎯 MINIO CONFIGURADO PARA OPENSHIFT AI DSP:\")\n",
    "        print(f\"  ✅ Bucket 'pipeline' creado (requerido por OpenShift AI)\")\n",
    "        print(f\"  ✅ Buckets RAG creados (raw-documents, processed-documents, etc.)\")\n",
    "        print(f\"  ✅ Documento de prueba subido: {test_upload_result['object']}\")\n",
    "        print(f\"  📊 Tamaño: {test_upload_result['word_count']} palabras\")\n",
    "        print(f\"\\n🚀 LISTO PARA CONFIGURAR OPENSHIFT AI DSP\")\n",
    "    else:\n",
    "        print(f\"\\n⚠️ MinIO configurado pero falta documento de prueba\")\n",
    "else:\n",
    "    print(f\"\\n❌ Configurar MinIO antes de continuar\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
