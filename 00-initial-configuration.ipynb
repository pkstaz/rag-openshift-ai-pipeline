{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93aec484-3cea-471f-967e-325519f7a133",
   "metadata": {},
   "source": [
    "# 00 - Initial Configuration\n",
    "\n",
    "## üéØ Objetivo\n",
    "Crear, compilar y deployar el pipeline RAG en OpenShift AI Data Science Pipelines.\n",
    "Este notebook toma los components desarrollados en el notebook anterior y los prepara para deployment real.\n",
    "\n",
    "## üìã Lo que Haremos\n",
    "1. **Configurar MinIO con buckets necesarios**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1800f3a",
   "metadata": {},
   "source": [
    "## üîß Setup Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee277a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ KubeFlow Pipelines disponible: 2.12.1\n",
      "‚úÖ MinIO client disponible\n",
      "üöÄ Notebook de deployment iniciado\n",
      "üìÅ Directorio actual: /opt/app-root/src\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Verificar KFP y dependencias\n",
    "try:\n",
    "    import kfp\n",
    "    from kfp import dsl, compiler\n",
    "    from kfp.client import Client\n",
    "    from kfp.dsl import component, pipeline, Input, Output, Dataset\n",
    "    print(f\"‚úÖ KubeFlow Pipelines disponible: {kfp.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå Instalando KubeFlow Pipelines...\")\n",
    "    !pip install kfp>=2.0.0\n",
    "\n",
    "# Verificar MinIO client\n",
    "try:\n",
    "    from minio import Minio\n",
    "    print(\"‚úÖ MinIO client disponible\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå Instalando MinIO client...\")\n",
    "    !pip install minio\n",
    "\n",
    "print(\"üöÄ Notebook de deployment iniciado\")\n",
    "print(f\"üìÅ Directorio actual: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a861af26-a347-462a-8327-9f2f8fa2ed8e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ‚öôÔ∏è Configuration Files\n",
    "\n",
    "Creamos los archivos de configuraci√≥n necesarios para el deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b60954e6-e468-4e11-a685-82ce1a11d350",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datetime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 177\u001b[39m\n\u001b[32m    174\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m  config/env.template - Variables de ambiente\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    176\u001b[39m \u001b[38;5;66;03m# Crear archivos de configuraci√≥n\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m \u001b[43mcreate_configuration_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mcreate_configuration_files\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Crear archivos de configuraci√≥n del proyecto\"\"\"\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 1. Pipeline Configuration\u001b[39;00m\n\u001b[32m      5\u001b[39m pipeline_config = {\n\u001b[32m      6\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpipeline\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m      7\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mrag-document-processing-v1\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      8\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mversion\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m1.0.0\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdescription\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mRAG Document Processing Pipeline para OpenShift AI\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     10\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mauthor\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mCarlos Estay\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     11\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mgithub\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mpkstaz\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcreated\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mdatetime\u001b[49m.now().isoformat()\n\u001b[32m     13\u001b[39m     },\n\u001b[32m     14\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcomponents\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m     15\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mbase_image\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mpytorch/pytorch:2.0.1-cuda11.7-cudnn8-devel\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     16\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mextract_text\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m     17\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mcpu_limit\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m500m\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     18\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmemory_limit\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m1Gi\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     19\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mretry_limit\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m3\u001b[39m\n\u001b[32m     20\u001b[39m         },\n\u001b[32m     21\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mchunk_text\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m     22\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mcpu_limit\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m500m\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m     23\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmemory_limit\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m1Gi\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     24\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mretry_limit\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m3\u001b[39m\n\u001b[32m     25\u001b[39m         },\n\u001b[32m     26\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mgenerate_embeddings\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m     27\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mcpu_limit\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m1000m\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     28\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmemory_limit\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m4Gi\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m     29\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mretry_limit\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m2\u001b[39m,\n\u001b[32m     30\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mgpu_limit\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m0\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# Set to \"1\" si hay GPU disponible\u001b[39;00m\n\u001b[32m     31\u001b[39m         },\n\u001b[32m     32\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mindex_elasticsearch\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m     33\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mcpu_limit\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m500m\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     34\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmemory_limit\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m2Gi\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     35\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mretry_limit\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m3\u001b[39m\n\u001b[32m     36\u001b[39m         }\n\u001b[32m     37\u001b[39m     },\n\u001b[32m     38\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdefault_parameters\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m     39\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mchunk_size\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m512\u001b[39m,\n\u001b[32m     40\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mchunk_overlap\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m50\u001b[39m,\n\u001b[32m     41\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33membedding_model\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msentence-transformers/all-MiniLM-L6-v2\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     42\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mes_index\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mrag-documents\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     43\u001b[39m     },\n\u001b[32m     44\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mstorage\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m     45\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mminio\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m     46\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mendpoint\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mminio:9000\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     47\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33maccess_key\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mminio\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     48\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33msecret_key\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mminio123\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     49\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mbucket_raw\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mraw-documents\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     50\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mbucket_processed\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mprocessed-documents\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     51\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mbucket_failed\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mfailed-documents\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     52\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mbucket_pipeline\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mpipeline\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     53\u001b[39m         },\n\u001b[32m     54\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33melasticsearch\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m     55\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mendpoint\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33melasticsearch:9200\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     56\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mindex_prefix\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mrag-\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     57\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mreplicas\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0\u001b[39m,\n\u001b[32m     58\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mshards\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m1\u001b[39m\n\u001b[32m     59\u001b[39m         }\n\u001b[32m     60\u001b[39m     }\n\u001b[32m     61\u001b[39m }\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mconfig/pipeline_config.yaml\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     64\u001b[39m     yaml.dump(pipeline_config, f, indent=\u001b[32m2\u001b[39m, default_flow_style=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'datetime' is not defined"
     ]
    }
   ],
   "source": [
    "def create_configuration_files():\n",
    "    \"\"\"Crear archivos de configuraci√≥n del proyecto\"\"\"\n",
    "    \n",
    "    # 1. Pipeline Configuration\n",
    "    pipeline_config = {\n",
    "        \"pipeline\": {\n",
    "            \"name\": \"rag-document-processing-v1\",\n",
    "            \"version\": \"1.0.0\",\n",
    "            \"description\": \"RAG Document Processing Pipeline para OpenShift AI\",\n",
    "            \"author\": \"Carlos Estay\",\n",
    "            \"github\": \"pkstaz\",\n",
    "            \"created\": datetime.now().isoformat()\n",
    "        },\n",
    "        \"components\": {\n",
    "            \"base_image\": \"pytorch/pytorch:2.0.1-cuda11.7-cudnn8-devel\",\n",
    "            \"extract_text\": {\n",
    "                \"cpu_limit\": \"500m\",\n",
    "                \"memory_limit\": \"1Gi\",\n",
    "                \"retry_limit\": 3\n",
    "            },\n",
    "            \"chunk_text\": {\n",
    "                \"cpu_limit\": \"500m\", \n",
    "                \"memory_limit\": \"1Gi\",\n",
    "                \"retry_limit\": 3\n",
    "            },\n",
    "            \"generate_embeddings\": {\n",
    "                \"cpu_limit\": \"1000m\",\n",
    "                \"memory_limit\": \"4Gi\", \n",
    "                \"retry_limit\": 2,\n",
    "                \"gpu_limit\": \"0\"  # Set to \"1\" si hay GPU disponible\n",
    "            },\n",
    "            \"index_elasticsearch\": {\n",
    "                \"cpu_limit\": \"500m\",\n",
    "                \"memory_limit\": \"2Gi\",\n",
    "                \"retry_limit\": 3\n",
    "            }\n",
    "        },\n",
    "        \"default_parameters\": {\n",
    "            \"chunk_size\": 512,\n",
    "            \"chunk_overlap\": 50,\n",
    "            \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "            \"es_index\": \"rag-documents\"\n",
    "        },\n",
    "        \"storage\": {\n",
    "            \"minio\": {\n",
    "                \"endpoint\": \"minio:9000\",\n",
    "                \"access_key\": \"minio\",\n",
    "                \"secret_key\": \"minio123\",\n",
    "                \"bucket_raw\": \"raw-documents\",\n",
    "                \"bucket_processed\": \"processed-documents\",\n",
    "                \"bucket_failed\": \"failed-documents\",\n",
    "                \"bucket_pipeline\": \"pipeline\"\n",
    "            },\n",
    "            \"elasticsearch\": {\n",
    "                \"endpoint\": \"elasticsearch:9200\",\n",
    "                \"index_prefix\": \"rag-\",\n",
    "                \"replicas\": 0,\n",
    "                \"shards\": 1\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open('config/pipeline_config.yaml', 'w') as f:\n",
    "        yaml.dump(pipeline_config, f, indent=2, default_flow_style=False)\n",
    "    \n",
    "    print(\"‚úÖ Creado: config/pipeline_config.yaml\")\n",
    "    \n",
    "    # 2. Secrets Template\n",
    "    secrets_template = {\n",
    "        \"apiVersion\": \"v1\",\n",
    "        \"kind\": \"Secret\",\n",
    "        \"metadata\": {\n",
    "            \"name\": \"rag-pipeline-secrets\",\n",
    "            \"namespace\": \"rag-openshift-ai\",\n",
    "            \"labels\": {\n",
    "                \"app\": \"rag-pipeline\",\n",
    "                \"author\": \"carlos-estay\",\n",
    "                \"github\": \"pkstaz\"\n",
    "            }\n",
    "        },\n",
    "        \"type\": \"Opaque\",\n",
    "        \"stringData\": {\n",
    "            \"minio-access-key\": \"minio\",\n",
    "            \"minio-secret-key\": \"minio123\",\n",
    "            \"elasticsearch-username\": \"\",\n",
    "            \"elasticsearch-password\": \"\",\n",
    "            \"pipeline-webhook-token\": \"rag-pipeline-token-change-me\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open('config/secrets.yaml', 'w') as f:\n",
    "        yaml.dump(secrets_template, f, indent=2, default_flow_style=False)\n",
    "    \n",
    "    print(\"‚úÖ Creado: config/secrets.yaml\")\n",
    "    \n",
    "    # 3. Requirements file\n",
    "    requirements_content = '''# RAG Pipeline Requirements\n",
    "# Author: Carlos Estay (github: pkstaz)\n",
    "# Core KFP\n",
    "kfp>=2.0.0\n",
    "\n",
    "# Document Processing\n",
    "PyPDF2==3.0.1\n",
    "python-docx==0.8.11\n",
    "chardet==5.2.0\n",
    "\n",
    "# Text Processing\n",
    "tiktoken==0.5.1\n",
    "langchain==0.0.350\n",
    "\n",
    "# ML/Embeddings\n",
    "sentence-transformers==2.2.2\n",
    "torch>=2.0.1\n",
    "numpy==1.24.3\n",
    "\n",
    "# Storage/DB\n",
    "minio==7.1.17\n",
    "elasticsearch==8.11.0\n",
    "\n",
    "# Web/API (for webhook)\n",
    "flask==2.3.3\n",
    "requests==2.31.0\n",
    "\n",
    "# Utilities\n",
    "pyyaml==6.0.1\n",
    "python-dateutil==2.8.2\n",
    "'''\n",
    "    \n",
    "    with open('requirements.txt', 'w') as f:\n",
    "        f.write(requirements_content)\n",
    "    \n",
    "    print(\"‚úÖ Creado: requirements.txt\")\n",
    "    \n",
    "    # 4. Environment variables template\n",
    "    env_template = '''# RAG Pipeline Environment Variables\n",
    "# Author: Carlos Estay (github: pkstaz)\n",
    "# Copy to .env and modify as needed\n",
    "\n",
    "# MinIO Configuration\n",
    "MINIO_ENDPOINT=minio:9000\n",
    "MINIO_ACCESS_KEY=minio\n",
    "MINIO_SECRET_KEY=minio123\n",
    "MINIO_SECURE=false\n",
    "\n",
    "# ElasticSearch Configuration  \n",
    "ES_ENDPOINT=elasticsearch:9200\n",
    "ES_INDEX=rag-documents\n",
    "ES_USERNAME=\n",
    "ES_PASSWORD=\n",
    "\n",
    "# Pipeline Configuration\n",
    "CHUNK_SIZE=512\n",
    "CHUNK_OVERLAP=50\n",
    "EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2\n",
    "\n",
    "# OpenShift AI Pipeline\n",
    "KFP_HOST=http://ml-pipeline:8888\n",
    "PIPELINE_NAMESPACE=rag-openshift-ai\n",
    "\n",
    "# Webhook Configuration\n",
    "WEBHOOK_PORT=8080\n",
    "WEBHOOK_TOKEN=rag-pipeline-token-change-me\n",
    "'''\n",
    "    \n",
    "    with open('config/env.template', 'w') as f:\n",
    "        f.write(env_template)\n",
    "    \n",
    "    print(\"‚úÖ Creado: config/env.template\")\n",
    "    \n",
    "    print(\"\\nüìã Archivos de configuraci√≥n creados:\")\n",
    "    print(\"  config/pipeline_config.yaml - Configuraci√≥n del pipeline\")\n",
    "    print(\"  config/secrets.yaml - Template de secrets K8s\")\n",
    "    print(\"  requirements.txt - Dependencias Python\")  \n",
    "    print(\"  config/env.template - Variables de ambiente\")\n",
    "\n",
    "# Crear archivos de configuraci√≥n\n",
    "create_configuration_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432bfe28-97fd-4f7c-ba34-f2d3069338c0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ü™£ Configuraci√≥n de MinIO - Crear Buckets\n",
    "\n",
    "Configuramos MinIO y creamos todos los buckets necesarios incluyendo el bucket \"pipeline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ced5bd2-2026-4962-a90b-060016d41b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Configuraci√≥n completa de MinIO para OpenShift AI DSP\n",
      "============================================================\n",
      "üîß Configurando MinIO y creando buckets completos...\n",
      "üîó Conectando a MinIO: minio:9000\n",
      "‚úÖ Conectado a MinIO exitosamente\n",
      "üìä Buckets existentes: 6\n",
      "  - failed-documents (creado: 2025-07-01 17:40:26.746000+00:00)\n",
      "  - pipeline (creado: 2025-07-01 17:46:35.035000+00:00)\n",
      "  - processed-documents (creado: 2025-07-01 17:40:26.739000+00:00)\n",
      "  - rag-documents (creado: 2025-07-01 16:45:51.180000+00:00)\n",
      "  - raw-documents (creado: 2025-07-01 17:40:26.732000+00:00)\n",
      "  - test-datasets (creado: 2025-07-01 17:40:26.753000+00:00)\n",
      "\n",
      "ü™£ Creando buckets necesarios...\n",
      "  ‚ÑπÔ∏è Bucket ya existe: pipeline\n",
      "    üìù Bucket requerido por OpenShift AI Data Science Pipelines\n",
      "  ‚ÑπÔ∏è Bucket ya existe: raw-documents\n",
      "    üìù Documentos originales subidos por usuarios\n",
      "  ‚ÑπÔ∏è Bucket ya existe: processed-documents\n",
      "    üìù Documentos procesados exitosamente\n",
      "  ‚ÑπÔ∏è Bucket ya existe: failed-documents\n",
      "    üìù Documentos que fallaron en procesamiento\n",
      "  ‚ÑπÔ∏è Bucket ya existe: test-datasets\n",
      "    üìù Datasets de prueba para desarrollo\n",
      "\n",
      "üìä Resumen de buckets:\n",
      "  ‚úÖ Creados: 0\n",
      "  ‚ÑπÔ∏è Ya exist√≠an: 5\n",
      "\n",
      "üóÇÔ∏è Buckets disponibles:\n",
      "  - failed-documents\n",
      "  - pipeline\n",
      "  - processed-documents\n",
      "  - rag-documents\n",
      "  - raw-documents\n",
      "  - test-datasets\n",
      "\n",
      "‚úÖ Bucket 'pipeline' configurado correctamente para OpenShift AI DSP\n",
      "\n",
      "üìÑ Subiendo documento de prueba...\n",
      "‚úÖ Documento subido: raw-documents/test-document-openshift-ai.txt\n",
      "üìè Tama√±o: 1809 caracteres\n",
      "üìä Palabras aproximadas: 241\n",
      "üìã Verificaci√≥n:\n",
      "  Size: 1828 bytes\n",
      "  Content-Type: text/plain\n",
      "  Last-Modified: 2025-07-01 18:55:05+00:00\n",
      "\n",
      "üéØ MINIO CONFIGURADO PARA OPENSHIFT AI DSP:\n",
      "  ‚úÖ Bucket 'pipeline' creado (requerido por OpenShift AI)\n",
      "  ‚úÖ Buckets RAG creados (raw-documents, processed-documents, etc.)\n",
      "  ‚úÖ Documento de prueba subido: test-document-openshift-ai.txt\n",
      "  üìä Tama√±o: 241 palabras\n",
      "\n",
      "üöÄ LISTO PARA CONFIGURAR OPENSHIFT AI DSP\n"
     ]
    }
   ],
   "source": [
    "def setup_minio_buckets_complete():\n",
    "    \"\"\"Configurar MinIO y crear todos los buckets necesarios incluyendo 'pipeline'\"\"\"\n",
    "    \n",
    "    print(\"üîß Configurando MinIO y creando buckets completos...\")\n",
    "    \n",
    "    # Configuraci√≥n MinIO\n",
    "    MINIO_ENDPOINT = \"minio:9000\"\n",
    "    MINIO_ACCESS_KEY = \"minio\"\n",
    "    MINIO_SECRET_KEY = \"minio123\"\n",
    "    \n",
    "    try:\n",
    "        from minio import Minio\n",
    "        from minio.error import S3Error\n",
    "        \n",
    "        # Conectar a MinIO\n",
    "        print(f\"üîó Conectando a MinIO: {MINIO_ENDPOINT}\")\n",
    "        \n",
    "        minio_client = Minio(\n",
    "            MINIO_ENDPOINT,\n",
    "            access_key=MINIO_ACCESS_KEY,\n",
    "            secret_key=MINIO_SECRET_KEY,\n",
    "            secure=False  # HTTP para desarrollo\n",
    "        )\n",
    "        \n",
    "        # Verificar conectividad\n",
    "        try:\n",
    "            buckets = minio_client.list_buckets()\n",
    "            print(f\"‚úÖ Conectado a MinIO exitosamente\")\n",
    "            print(f\"üìä Buckets existentes: {len(buckets)}\")\n",
    "            for bucket in buckets:\n",
    "                print(f\"  - {bucket.name} (creado: {bucket.creation_date})\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error conectando a MinIO: {str(e)}\")\n",
    "            return False\n",
    "        \n",
    "        # Buckets necesarios para RAG pipeline + OpenShift AI DSP\n",
    "        required_buckets = [\n",
    "            {\n",
    "                \"name\": \"pipeline\",\n",
    "                \"description\": \"Bucket requerido por OpenShift AI Data Science Pipelines\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"raw-documents\",\n",
    "                \"description\": \"Documentos originales subidos por usuarios\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"processed-documents\", \n",
    "                \"description\": \"Documentos procesados exitosamente\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"failed-documents\",\n",
    "                \"description\": \"Documentos que fallaron en procesamiento\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"test-datasets\",\n",
    "                \"description\": \"Datasets de prueba para desarrollo\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\nü™£ Creando buckets necesarios...\")\n",
    "        \n",
    "        created_buckets = []\n",
    "        existing_buckets = []\n",
    "        \n",
    "        for bucket_info in required_buckets:\n",
    "            bucket_name = bucket_info[\"name\"]\n",
    "            description = bucket_info[\"description\"]\n",
    "            \n",
    "            try:\n",
    "                # Verificar si bucket ya existe\n",
    "                if minio_client.bucket_exists(bucket_name):\n",
    "                    existing_buckets.append(bucket_name)\n",
    "                    print(f\"  ‚ÑπÔ∏è Bucket ya existe: {bucket_name}\")\n",
    "                else:\n",
    "                    # Crear bucket\n",
    "                    minio_client.make_bucket(bucket_name)\n",
    "                    created_buckets.append(bucket_name)\n",
    "                    print(f\"  ‚úÖ Bucket creado: {bucket_name}\")\n",
    "                \n",
    "                print(f\"    üìù {description}\")\n",
    "                    \n",
    "            except S3Error as e:\n",
    "                print(f\"  ‚ùå Error con bucket {bucket_name}: {str(e)}\")\n",
    "        \n",
    "        print(f\"\\nüìä Resumen de buckets:\")\n",
    "        print(f\"  ‚úÖ Creados: {len(created_buckets)}\")\n",
    "        print(f\"  ‚ÑπÔ∏è Ya exist√≠an: {len(existing_buckets)}\")\n",
    "        \n",
    "        # Listar todos los buckets despu√©s de la configuraci√≥n\n",
    "        final_buckets = minio_client.list_buckets()\n",
    "        print(f\"\\nüóÇÔ∏è Buckets disponibles:\")\n",
    "        for bucket in final_buckets:\n",
    "            print(f\"  - {bucket.name}\")\n",
    "        \n",
    "        # Verificar bucket cr√≠tico para OpenShift AI\n",
    "        if minio_client.bucket_exists(\"pipeline\"):\n",
    "            print(f\"\\n‚úÖ Bucket 'pipeline' configurado correctamente para OpenShift AI DSP\")\n",
    "        else:\n",
    "            print(f\"\\n‚ùå Bucket 'pipeline' no se pudo crear - requerido para OpenShift AI DSP\")\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"‚ùå MinIO client no disponible. Instalar con: pip install minio\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error configurando MinIO: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def upload_test_document():\n",
    "    \"\"\"Subir documento de prueba al bucket raw-documents\"\"\"\n",
    "    \n",
    "    print(f\"\\nüìÑ Subiendo documento de prueba...\")\n",
    "    \n",
    "    try:\n",
    "        from minio import Minio\n",
    "        import tempfile\n",
    "        \n",
    "        # Conectar a MinIO\n",
    "        minio_client = Minio(\n",
    "            \"minio:9000\",\n",
    "            access_key=\"minio\",\n",
    "            secret_key=\"minio123\",\n",
    "            secure=False\n",
    "        )\n",
    "        \n",
    "        # Crear documento de prueba m√°s detallado\n",
    "        test_content = \"\"\"# Documento de Prueba RAG Pipeline - OpenShift AI\n",
    "        \n",
    "Este es un documento de prueba para validar el pipeline RAG en OpenShift AI Data Science Pipelines.\n",
    "\n",
    "## Configuraci√≥n del Ambiente\n",
    "\n",
    "### MinIO Buckets Configurados:\n",
    "- **pipeline**: Bucket requerido por OpenShift AI DSP\n",
    "- **raw-documents**: Documentos originales \n",
    "- **processed-documents**: Documentos procesados exitosamente\n",
    "- **failed-documents**: Documentos con errores de procesamiento\n",
    "- **test-datasets**: Datasets para desarrollo y pruebas\n",
    "\n",
    "### Pipeline de Procesamiento RAG\n",
    "\n",
    "El sistema procesa documentos siguiendo estos pasos:\n",
    "\n",
    "1. **Extracci√≥n de texto**: \n",
    "   - Soporta PDF, DOCX, TXT\n",
    "   - Detecta encoding autom√°ticamente\n",
    "   - Extrae metadatos del documento\n",
    "\n",
    "2. **Chunking inteligente**:\n",
    "   - Chunks de 512 tokens con 50 de overlap\n",
    "   - Usa tiktoken para conteo preciso\n",
    "   - Mantiene contexto entre fragmentos\n",
    "\n",
    "3. **Generaci√≥n de embeddings**:\n",
    "   - Modelo: sentence-transformers/all-MiniLM-L6-v2\n",
    "   - Vectores de 384 dimensiones\n",
    "   - Normalizaci√≥n para cosine similarity\n",
    "\n",
    "4. **Indexaci√≥n en ElasticSearch**:\n",
    "   - √çndice h√≠brido (texto + vectores)\n",
    "   - B√∫squeda sem√°ntica y por palabras clave\n",
    "   - Metadatos preservados para trazabilidad\n",
    "\n",
    "## Informaci√≥n del Proyecto\n",
    "\n",
    "- **Autor**: Carlos Estay\n",
    "- **GitHub**: pkstaz  \n",
    "- **Pipeline**: rag-document-processing-v1\n",
    "- **Platform**: OpenShift AI Data Science Pipelines\n",
    "\n",
    "Este documento ser√° procesado autom√°ticamente cuando el webhook detecte \n",
    "su upload al bucket raw-documents.\n",
    "\n",
    "## Testing del Pipeline\n",
    "\n",
    "Para validar el funcionamiento:\n",
    "\n",
    "1. Este documento se procesa autom√°ticamente\n",
    "2. Se generan aproximadamente 4-6 chunks\n",
    "3. Cada chunk obtiene un embedding de 384 dimensiones  \n",
    "4. Se indexa en ElasticSearch con ID √∫nico\n",
    "5. Est√° disponible para b√∫squeda sem√°ntica\n",
    "\n",
    "¬°Pipeline RAG funcionando en OpenShift AI!\n",
    "\"\"\"\n",
    "        \n",
    "        # Crear archivo temporal\n",
    "        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8') as f:\n",
    "            f.write(test_content)\n",
    "            temp_file_path = f.name\n",
    "        \n",
    "        # Subir archivo a MinIO\n",
    "        object_name = \"test-document-openshift-ai.txt\"\n",
    "        bucket_name = \"raw-documents\"\n",
    "        \n",
    "        try:\n",
    "            minio_client.fput_object(\n",
    "                bucket_name=bucket_name,\n",
    "                object_name=object_name,\n",
    "                file_path=temp_file_path,\n",
    "                content_type=\"text/plain\"\n",
    "            )\n",
    "            \n",
    "            print(f\"‚úÖ Documento subido: {bucket_name}/{object_name}\")\n",
    "            print(f\"üìè Tama√±o: {len(test_content)} caracteres\")\n",
    "            print(f\"üìä Palabras aproximadas: {len(test_content.split())}\")\n",
    "            \n",
    "            # Verificar que se subi√≥ correctamente\n",
    "            try:\n",
    "                stat = minio_client.stat_object(bucket_name, object_name)\n",
    "                print(f\"üìã Verificaci√≥n:\")\n",
    "                print(f\"  Size: {stat.size} bytes\")\n",
    "                print(f\"  Content-Type: {stat.content_type}\")\n",
    "                print(f\"  Last-Modified: {stat.last_modified}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è No se pudo verificar el archivo: {str(e)}\")\n",
    "            \n",
    "            return {\n",
    "                \"bucket\": bucket_name,\n",
    "                \"object\": object_name,\n",
    "                \"size\": len(test_content),\n",
    "                \"word_count\": len(test_content.split()),\n",
    "                \"success\": True\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error subiendo archivo: {str(e)}\")\n",
    "            return None\n",
    "            \n",
    "        finally:\n",
    "            # Limpiar archivo temporal\n",
    "            import os\n",
    "            if os.path.exists(temp_file_path):\n",
    "                os.unlink(temp_file_path)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en upload de documento de prueba: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Ejecutar configuraci√≥n completa de MinIO\n",
    "print(\"üöÄ Configuraci√≥n completa de MinIO para OpenShift AI DSP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "minio_setup_success = setup_minio_buckets_complete()\n",
    "\n",
    "# Subir documento de prueba si MinIO est√° configurado\n",
    "if minio_setup_success:\n",
    "    test_upload_result = upload_test_document()\n",
    "    \n",
    "    if test_upload_result:\n",
    "        print(f\"\\nüéØ MINIO CONFIGURADO PARA OPENSHIFT AI DSP:\")\n",
    "        print(f\"  ‚úÖ Bucket 'pipeline' creado (requerido por OpenShift AI)\")\n",
    "        print(f\"  ‚úÖ Buckets RAG creados (raw-documents, processed-documents, etc.)\")\n",
    "        print(f\"  ‚úÖ Documento de prueba subido: {test_upload_result['object']}\")\n",
    "        print(f\"  üìä Tama√±o: {test_upload_result['word_count']} palabras\")\n",
    "        print(f\"\\nüöÄ LISTO PARA CONFIGURAR OPENSHIFT AI DSP\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è MinIO configurado pero falta documento de prueba\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Configurar MinIO antes de continuar\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
