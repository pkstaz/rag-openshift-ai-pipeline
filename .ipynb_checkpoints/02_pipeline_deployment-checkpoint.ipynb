{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93aec484-3cea-471f-967e-325519f7a133",
   "metadata": {},
   "source": [
    "# 02 - Pipeline Deployment - RAG OpenShift AI\n",
    "\n",
    "## üéØ Objetivo\n",
    "Crear, compilar y deployar el pipeline RAG en OpenShift AI Data Science Pipelines.\n",
    "Este notebook toma los components desarrollados en el notebook anterior y los prepara para deployment real.\n",
    "\n",
    "## üìã Lo que Haremos\n",
    "1. **Crear Components como archivos Python separados**\n",
    "2. **Definir Pipeline real con components importados**\n",
    "3. **Compilar Pipeline a YAML**\n",
    "4. **Configurar MinIO con buckets necesarios**\n",
    "5. **Deploy en OpenShift AI v2**\n",
    "6. **Testing del Pipeline deployado**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e089481-5396-4010-9c65-f18c251c2774",
   "metadata": {},
   "source": [
    "## üîß Setup Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dad78d83-5119-4ca2-9c7d-7c1ef165f21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ KubeFlow Pipelines disponible: 2.12.1\n",
      "‚úÖ MinIO client disponible\n",
      "üöÄ Notebook de deployment iniciado\n",
      "üìÅ Directorio actual: /opt/app-root/src\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import stat\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Verificar KFP y dependencias\n",
    "try:\n",
    "    import kfp\n",
    "    from kfp import dsl, compiler\n",
    "    from kfp.client import Client\n",
    "    from kfp.dsl import component, pipeline, Input, Output, Dataset\n",
    "    print(f\"‚úÖ KubeFlow Pipelines disponible: {kfp.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå Instalando KubeFlow Pipelines...\")\n",
    "    !pip install kfp>=2.0.0\n",
    "\n",
    "# Verificar MinIO client\n",
    "try:\n",
    "    from minio import Minio\n",
    "    print(\"‚úÖ MinIO client disponible\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå Instalando MinIO client...\")\n",
    "    !pip install minio\n",
    "\n",
    "print(\"üöÄ Notebook de deployment iniciado\")\n",
    "print(f\"üìÅ Directorio actual: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1485945c-18b7-447f-b027-be91bbff274a",
   "metadata": {},
   "source": [
    "## üìÅ Crear Estructura de Archivos para Components\n",
    "\n",
    "Creamos los archivos Python separados para cada component del pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a459e8b-6e00-4ae5-9280-01a8e92eaf2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Creando estructura de directorios...\n",
      "  ‚úÖ components/\n",
      "  ‚úÖ pipelines/\n",
      "  ‚úÖ webhook/\n",
      "  ‚úÖ config/\n",
      "  ‚úÖ deploy/minio/\n",
      "  ‚úÖ deploy/elasticsearch/\n",
      "  ‚úÖ deploy/openshift-ai/\n",
      "  ‚úÖ deploy/webhook/\n",
      "  ‚úÖ tests/\n",
      "  ‚úÖ components/__init__.py\n",
      "  ‚úÖ pipelines/__init__.py\n",
      "  ‚úÖ webhook/__init__.py\n",
      "\n",
      "üìã Estructura creada:\n",
      "  components/          # Pipeline components\n",
      "  pipelines/           # Pipeline definitions\n",
      "  webhook/             # Webhook handler\n",
      "  config/              # Configuration files\n",
      "  deploy/              # Deployment manifests\n",
      "  tests/               # Integration tests\n"
     ]
    }
   ],
   "source": [
    "def create_project_structure():\n",
    "    \"\"\"Crear la estructura de directorios y archivos del proyecto\"\"\"\n",
    "    \n",
    "    # Definir estructura de directorios\n",
    "    directories = [\n",
    "        \"components\",\n",
    "        \"pipelines\", \n",
    "        \"webhook\",\n",
    "        \"config\",\n",
    "        \"deploy/minio\",\n",
    "        \"deploy/elasticsearch\", \n",
    "        \"deploy/openshift-ai\",\n",
    "        \"deploy/webhook\",\n",
    "        \"tests\"\n",
    "    ]\n",
    "    \n",
    "    print(\"üìÅ Creando estructura de directorios...\")\n",
    "    \n",
    "    # Crear directorios\n",
    "    for dir_path in directories:\n",
    "        Path(dir_path).mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"  ‚úÖ {dir_path}/\")\n",
    "    \n",
    "    # Crear archivos __init__.py para modules Python\n",
    "    init_files = [\n",
    "        \"components/__init__.py\",\n",
    "        \"pipelines/__init__.py\",\n",
    "        \"webhook/__init__.py\"\n",
    "    ]\n",
    "    \n",
    "    for init_file in init_files:\n",
    "        Path(init_file).touch(exist_ok=True)\n",
    "        print(f\"  ‚úÖ {init_file}\")\n",
    "    \n",
    "    print(\"\\nüìã Estructura creada:\")\n",
    "    print(\"  components/          # Pipeline components\")\n",
    "    print(\"  pipelines/           # Pipeline definitions\") \n",
    "    print(\"  webhook/             # Webhook handler\")\n",
    "    print(\"  config/              # Configuration files\")\n",
    "    print(\"  deploy/              # Deployment manifests\")\n",
    "    print(\"  tests/               # Integration tests\")\n",
    "    \n",
    "    return directories\n",
    "\n",
    "# Crear estructura\n",
    "created_dirs = create_project_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcc2758-7cda-477a-98ed-ada741208d2f",
   "metadata": {},
   "source": [
    "## üîß Component 1: Text Processing Components\n",
    "\n",
    "Creamos el archivo con los components de procesamiento de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f466e8b-d3ee-4f53-a4f4-c5c0d4477e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Archivo creado: components/text_processing.py\n",
      "üìã Components incluidos:\n",
      "  - extract_text_component\n",
      "  - chunk_text_component\n"
     ]
    }
   ],
   "source": [
    "def create_text_processing_components():\n",
    "    \"\"\"Crear archivo components/text_processing.py\"\"\"\n",
    "    \n",
    "    text_processing_content = '''\"\"\"\n",
    "Text Processing Components para RAG Pipeline\n",
    "Incluye: extract_text_component y chunk_text_component\n",
    "Author: Carlos Estay (github: pkstaz)\n",
    "\"\"\"\n",
    "\n",
    "from kfp.dsl import component, Input, Output, Dataset\n",
    "\n",
    "@component(\n",
    "    base_image=\"pytorch/pytorch:2.0.1-cuda11.7-cudnn8-devel\",\n",
    "    packages_to_install=[\n",
    "        \"PyPDF2==3.0.1\",\n",
    "        \"python-docx==0.8.11\", \n",
    "        \"minio==7.1.17\",\n",
    "        \"chardet==5.2.0\"\n",
    "    ]\n",
    ")\n",
    "def extract_text_component(\n",
    "    bucket_name: str,\n",
    "    object_key: str,\n",
    "    minio_endpoint: str,\n",
    "    minio_access_key: str,\n",
    "    minio_secret_key: str,\n",
    "    extracted_text: Output[Dataset],\n",
    "    metadata: Output[Dataset]\n",
    "):\n",
    "    \"\"\"\n",
    "    Extrae texto de documentos almacenados en MinIO.\n",
    "    \n",
    "    Args:\n",
    "        bucket_name: Nombre del bucket en MinIO\n",
    "        object_key: Path del archivo en el bucket\n",
    "        minio_endpoint: Endpoint de MinIO\n",
    "        minio_access_key: Access key de MinIO\n",
    "        minio_secret_key: Secret key de MinIO\n",
    "        extracted_text: Output dataset con el texto extra√≠do\n",
    "        metadata: Output dataset con metadata del documento\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import json\n",
    "    import tempfile\n",
    "    from pathlib import Path\n",
    "    from datetime import datetime\n",
    "    from minio import Minio\n",
    "    import PyPDF2\n",
    "    from docx import Document\n",
    "    import chardet\n",
    "    \n",
    "    # Conectar a MinIO\n",
    "    minio_client = Minio(\n",
    "        minio_endpoint,\n",
    "        access_key=minio_access_key,\n",
    "        secret_key=minio_secret_key,\n",
    "        secure=False\n",
    "    )\n",
    "    \n",
    "    # Crear directorio temporal\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        local_file_path = os.path.join(temp_dir, object_key.split('/')[-1])\n",
    "        \n",
    "        # Descargar archivo desde MinIO\n",
    "        try:\n",
    "            minio_client.fget_object(bucket_name, object_key, local_file_path)\n",
    "            print(f\"‚úÖ Archivo descargado: {local_file_path}\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error descargando archivo: {str(e)}\")\n",
    "        \n",
    "        # Detectar tipo de archivo\n",
    "        file_extension = Path(local_file_path).suffix.lower()\n",
    "        file_size = os.path.getsize(local_file_path)\n",
    "        \n",
    "        # Extraer texto seg√∫n el tipo de archivo\n",
    "        extracted_content = \"\"\n",
    "        \n",
    "        if file_extension == '.pdf':\n",
    "            try:\n",
    "                with open(local_file_path, 'rb') as file:\n",
    "                    pdf_reader = PyPDF2.PdfReader(file)\n",
    "                    for page_num in range(len(pdf_reader.pages)):\n",
    "                        page = pdf_reader.pages[page_num]\n",
    "                        extracted_content += page.extract_text() + \"\\\\n\"\n",
    "                print(f\"‚úÖ PDF procesado: {len(pdf_reader.pages)} p√°ginas\")\n",
    "            except Exception as e:\n",
    "                raise Exception(f\"Error procesando PDF: {str(e)}\")\n",
    "                \n",
    "        elif file_extension == '.docx':\n",
    "            try:\n",
    "                doc = Document(local_file_path)\n",
    "                for paragraph in doc.paragraphs:\n",
    "                    extracted_content += paragraph.text + \"\\\\n\"\n",
    "                print(f\"‚úÖ DOCX procesado: {len(doc.paragraphs)} p√°rrafos\")\n",
    "            except Exception as e:\n",
    "                raise Exception(f\"Error procesando DOCX: {str(e)}\")\n",
    "                \n",
    "        elif file_extension in ['.txt', '.md']:\n",
    "            try:\n",
    "                # Detectar encoding\n",
    "                with open(local_file_path, 'rb') as file:\n",
    "                    raw_data = file.read()\n",
    "                    encoding = chardet.detect(raw_data)['encoding']\n",
    "                \n",
    "                # Leer con encoding detectado\n",
    "                with open(local_file_path, 'r', encoding=encoding) as file:\n",
    "                    extracted_content = file.read()\n",
    "                print(f\"‚úÖ TXT procesado con encoding: {encoding}\")\n",
    "            except Exception as e:\n",
    "                raise Exception(f\"Error procesando TXT: {str(e)}\")\n",
    "                \n",
    "        else:\n",
    "            raise Exception(f\"Tipo de archivo no soportado: {file_extension}\")\n",
    "        \n",
    "        # Validar que se extrajo contenido\n",
    "        if not extracted_content.strip():\n",
    "            raise Exception(\"No se pudo extraer texto del documento\")\n",
    "        \n",
    "        # Preparar metadata\n",
    "        document_metadata = {\n",
    "            \"source_file\": object_key,\n",
    "            \"file_type\": file_extension,\n",
    "            \"file_size\": file_size,\n",
    "            \"processed_at\": datetime.now().isoformat(),\n",
    "            \"char_count\": len(extracted_content),\n",
    "            \"word_count\": len(extracted_content.split()),\n",
    "            \"bucket_name\": bucket_name\n",
    "        }\n",
    "        \n",
    "        # Guardar outputs\n",
    "        with open(extracted_text.path, 'w', encoding='utf-8') as f:\n",
    "            f.write(extracted_content)\n",
    "            \n",
    "        with open(metadata.path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(document_metadata, f, indent=2)\n",
    "        \n",
    "        print(f\"‚úÖ Texto extra√≠do: {len(extracted_content)} caracteres\")\n",
    "\n",
    "\n",
    "@component(\n",
    "    base_image=\"pytorch/pytorch:2.0.1-cuda11.7-cudnn8-devel\",\n",
    "    packages_to_install=[\n",
    "        \"tiktoken==0.5.1\",\n",
    "        \"langchain==0.0.350\"\n",
    "    ]\n",
    ")\n",
    "def chunk_text_component(\n",
    "    extracted_text: Input[Dataset],\n",
    "    metadata: Input[Dataset],\n",
    "    chunk_size: int,\n",
    "    chunk_overlap: int,\n",
    "    chunks: Output[Dataset]\n",
    "):\n",
    "    \"\"\"\n",
    "    Divide el texto en chunks con overlap para processing √≥ptimo.\n",
    "    \n",
    "    Args:\n",
    "        extracted_text: Input dataset con texto extra√≠do\n",
    "        metadata: Input dataset con metadata del documento\n",
    "        chunk_size: Tama√±o m√°ximo de cada chunk (en tokens)\n",
    "        chunk_overlap: Overlap entre chunks (en tokens)\n",
    "        chunks: Output dataset con chunks procesados\n",
    "    \"\"\"\n",
    "    import json\n",
    "    import tiktoken\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    \n",
    "    # Leer input data\n",
    "    with open(extracted_text.path, 'r', encoding='utf-8') as f:\n",
    "        text_content = f.read()\n",
    "    \n",
    "    with open(metadata.path, 'r', encoding='utf-8') as f:\n",
    "        doc_metadata = json.load(f)\n",
    "    \n",
    "    # Configurar tokenizer\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    def count_tokens(text: str) -> int:\n",
    "        return len(encoding.encode(text))\n",
    "    \n",
    "    # Configurar text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size * 4,  # Aproximaci√≥n: 1 token ‚âà 4 caracteres\n",
    "        chunk_overlap=chunk_overlap * 4,\n",
    "        length_function=len,\n",
    "        separators=[\"\\\\n\\\\n\", \"\\\\n\", \". \", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    # Dividir texto en chunks\n",
    "    text_chunks = text_splitter.split_text(text_content)\n",
    "    print(f\"‚úÖ Texto dividido en {len(text_chunks)} chunks\")\n",
    "    \n",
    "    # Procesar cada chunk\n",
    "    processed_chunks = []\n",
    "    \n",
    "    for i, chunk_text in enumerate(text_chunks):\n",
    "        token_count = count_tokens(chunk_text)\n",
    "        \n",
    "        chunk_metadata = {\n",
    "            \"chunk_id\": f\"{doc_metadata['source_file']}_chunk_{i:04d}\",\n",
    "            \"chunk_index\": i,\n",
    "            \"total_chunks\": len(text_chunks),\n",
    "            \"text\": chunk_text.strip(),\n",
    "            \"token_count\": token_count,\n",
    "            \"char_count\": len(chunk_text),\n",
    "            \"word_count\": len(chunk_text.split()),\n",
    "            \"source_document\": doc_metadata['source_file'],\n",
    "            \"file_type\": doc_metadata['file_type'],\n",
    "            \"processed_at\": doc_metadata['processed_at']\n",
    "        }\n",
    "        \n",
    "        processed_chunks.append(chunk_metadata)\n",
    "    \n",
    "    # Filtrar chunks muy peque√±os\n",
    "    processed_chunks = [chunk for chunk in processed_chunks if chunk['token_count'] >= 10]\n",
    "    \n",
    "    print(f\"‚úÖ Chunks procesados: {len(processed_chunks)}\")\n",
    "    \n",
    "    # Guardar chunks\n",
    "    with open(chunks.path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(processed_chunks, f, indent=2, ensure_ascii=False)\n",
    "'''\n",
    "    \n",
    "    # Escribir archivo\n",
    "    with open('components/text_processing.py', 'w', encoding='utf-8') as f:\n",
    "        f.write(text_processing_content)\n",
    "    \n",
    "    print(\"‚úÖ Archivo creado: components/text_processing.py\")\n",
    "    print(\"üìã Components incluidos:\")\n",
    "    print(\"  - extract_text_component\")\n",
    "    print(\"  - chunk_text_component\")\n",
    "\n",
    "# Crear archivo de text processing\n",
    "create_text_processing_components()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6450cc6b-4952-4409-b132-a621330ce9b4",
   "metadata": {},
   "source": [
    "## üéØ Component 2: Vector Processing Components\n",
    "\n",
    "Creamos el archivo con los components de embeddings e indexaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b4d7a4a-222c-4422-8326-82a9a518f9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Archivo creado: components/vector_processing.py\n",
      "üìã Components incluidos:\n",
      "  - generate_embeddings_component\n",
      "  - index_elasticsearch_component\n"
     ]
    }
   ],
   "source": [
    "def create_vector_processing_components():\n",
    "    \"\"\"Crear archivo components/vector_processing.py\"\"\"\n",
    "    \n",
    "    vector_processing_content = '''\"\"\"\n",
    "Vector Processing Components para RAG Pipeline\n",
    "Incluye: generate_embeddings_component y index_elasticsearch_component\n",
    "Author: Carlos Estay (github: pkstaz)\n",
    "\"\"\"\n",
    "\n",
    "from kfp.dsl import component, Input, Output, Dataset\n",
    "\n",
    "@component(\n",
    "    base_image=\"pytorch/pytorch:2.0.1-cuda11.7-cudnn8-devel\",\n",
    "    packages_to_install=[\n",
    "        \"sentence-transformers==2.2.2\",\n",
    "        \"numpy==1.24.3\"\n",
    "    ]\n",
    ")\n",
    "def generate_embeddings_component(\n",
    "    chunks: Input[Dataset],\n",
    "    model_name: str,\n",
    "    embeddings: Output[Dataset]\n",
    "):\n",
    "    \"\"\"\n",
    "    Genera embeddings vectoriales para los chunks de texto.\n",
    "    \n",
    "    Args:\n",
    "        chunks: Input dataset con chunks de texto\n",
    "        model_name: Nombre del modelo de embeddings\n",
    "        embeddings: Output dataset con embeddings generados\n",
    "    \"\"\"\n",
    "    import json\n",
    "    import numpy as np\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    import torch\n",
    "    \n",
    "    # Verificar si hay GPU disponible\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"üñ•Ô∏è Usando device: {device}\")\n",
    "    \n",
    "    # Cargar modelo de embeddings\n",
    "    print(f\"üì• Cargando modelo: {model_name}\")\n",
    "    model = SentenceTransformer(model_name, device=device)\n",
    "    \n",
    "    # Leer chunks\n",
    "    with open(chunks.path, 'r', encoding='utf-8') as f:\n",
    "        chunk_data = json.load(f)\n",
    "    \n",
    "    print(f\"üìù Procesando {len(chunk_data)} chunks\")\n",
    "    \n",
    "    # Extraer textos para embedding\n",
    "    texts = [chunk['text'] for chunk in chunk_data]\n",
    "    \n",
    "    # Generar embeddings en batches para eficiencia\n",
    "    batch_size = 32\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        batch_embeddings = model.encode(\n",
    "            batch_texts,\n",
    "            convert_to_numpy=True,\n",
    "            show_progress_bar=True if i == 0 else False,\n",
    "            normalize_embeddings=True\n",
    "        )\n",
    "        all_embeddings.extend(batch_embeddings)\n",
    "        \n",
    "        if i % (batch_size * 5) == 0:\n",
    "            print(f\"  Procesado: {min(i + batch_size, len(texts))}/{len(texts)} chunks\")\n",
    "    \n",
    "    print(f\"‚úÖ Embeddings generados: {len(all_embeddings)} vectores de {len(all_embeddings[0])} dimensiones\")\n",
    "    \n",
    "    # Combinar chunks con sus embeddings\n",
    "    enriched_chunks = []\n",
    "    for chunk, embedding in zip(chunk_data, all_embeddings):\n",
    "        enriched_chunk = chunk.copy()\n",
    "        enriched_chunk['embedding'] = embedding.tolist()\n",
    "        enriched_chunk['embedding_dim'] = len(embedding)\n",
    "        enriched_chunk['embedding_model'] = model_name\n",
    "        enriched_chunks.append(enriched_chunk)\n",
    "    \n",
    "    # Guardar chunks enriquecidos con embeddings\n",
    "    with open(embeddings.path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(enriched_chunks, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"‚úÖ Chunks enriquecidos guardados\")\n",
    "\n",
    "\n",
    "@component(\n",
    "    base_image=\"pytorch/pytorch:2.0.1-cuda11.7-cudnn8-devel\",\n",
    "    packages_to_install=[\n",
    "        \"elasticsearch==8.11.0\"\n",
    "    ]\n",
    ")\n",
    "def index_elasticsearch_component(\n",
    "    enriched_chunks: Input[Dataset],\n",
    "    es_endpoint: str,\n",
    "    es_index: str,\n",
    "    index_status: Output[Dataset]\n",
    "):\n",
    "    \"\"\"\n",
    "    Indexa chunks enriquecidos en ElasticSearch.\n",
    "    \n",
    "    Args:\n",
    "        enriched_chunks: Input dataset con chunks y embeddings\n",
    "        es_endpoint: Endpoint de ElasticSearch\n",
    "        es_index: Nombre del √≠ndice\n",
    "        index_status: Output dataset con status de indexaci√≥n\n",
    "    \"\"\"\n",
    "    import json\n",
    "    from datetime import datetime\n",
    "    from elasticsearch import Elasticsearch\n",
    "    from elasticsearch.helpers import bulk\n",
    "    \n",
    "    # Conectar a ElasticSearch\n",
    "    try:\n",
    "        es = Elasticsearch([es_endpoint], verify_certs=False)\n",
    "        \n",
    "        if not es.ping():\n",
    "            raise Exception(\"No se puede conectar a ElasticSearch\")\n",
    "        \n",
    "        print(f\"‚úÖ Conectado a ElasticSearch: {es_endpoint}\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error conectando a ElasticSearch: {str(e)}\")\n",
    "    \n",
    "    # Leer chunks enriquecidos\n",
    "    with open(enriched_chunks.path, 'r', encoding='utf-8') as f:\n",
    "        chunks_data = json.load(f)\n",
    "    \n",
    "    print(f\"üìù Indexando {len(chunks_data)} chunks en √≠ndice: {es_index}\")\n",
    "    \n",
    "    # Definir mapping del √≠ndice\n",
    "    index_mapping = {\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"chunk_id\": {\"type\": \"keyword\"},\n",
    "                \"text\": {\n",
    "                    \"type\": \"text\",\n",
    "                    \"analyzer\": \"standard\"\n",
    "                },\n",
    "                \"embedding\": {\n",
    "                    \"type\": \"dense_vector\",\n",
    "                    \"dims\": chunks_data[0]['embedding_dim'] if chunks_data else 384,\n",
    "                    \"index\": True,\n",
    "                    \"similarity\": \"cosine\"\n",
    "                },\n",
    "                \"source_document\": {\"type\": \"keyword\"},\n",
    "                \"file_type\": {\"type\": \"keyword\"},\n",
    "                \"chunk_index\": {\"type\": \"integer\"},\n",
    "                \"total_chunks\": {\"type\": \"integer\"},\n",
    "                \"token_count\": {\"type\": \"integer\"},\n",
    "                \"char_count\": {\"type\": \"integer\"},\n",
    "                \"word_count\": {\"type\": \"integer\"},\n",
    "                \"processed_at\": {\"type\": \"date\"},\n",
    "                \"indexed_at\": {\"type\": \"date\"}\n",
    "            }\n",
    "        },\n",
    "        \"settings\": {\n",
    "            \"number_of_shards\": 1,\n",
    "            \"number_of_replicas\": 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Crear √≠ndice si no existe\n",
    "    if not es.indices.exists(index=es_index):\n",
    "        es.indices.create(index=es_index, body=index_mapping)\n",
    "        print(f\"‚úÖ √çndice creado: {es_index}\")\n",
    "    else:\n",
    "        print(f\"‚ÑπÔ∏è √çndice ya existe: {es_index}\")\n",
    "    \n",
    "    # Preparar documentos para bulk indexing\n",
    "    documents = []\n",
    "    for chunk in chunks_data:\n",
    "        doc = {\n",
    "            \"_index\": es_index,\n",
    "            \"_id\": chunk['chunk_id'],\n",
    "            \"_source\": {\n",
    "                **chunk,\n",
    "                \"indexed_at\": datetime.now().isoformat()\n",
    "            }\n",
    "        }\n",
    "        documents.append(doc)\n",
    "    \n",
    "    # Indexar en batches\n",
    "    try:\n",
    "        success_count, failed_items = bulk(\n",
    "            es,\n",
    "            documents,\n",
    "            chunk_size=100,\n",
    "            request_timeout=300\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Indexaci√≥n completada:\")\n",
    "        print(f\"  Documentos exitosos: {success_count}\")\n",
    "        print(f\"  Documentos fallidos: {len(failed_items) if failed_items else 0}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error en bulk indexing: {str(e)}\")\n",
    "    \n",
    "    # Refresh del √≠ndice\n",
    "    es.indices.refresh(index=es_index)\n",
    "    \n",
    "    # Verificar indexaci√≥n\n",
    "    doc_count = es.count(index=es_index)['count']\n",
    "    print(f\"‚úÖ Total documentos en √≠ndice: {doc_count}\")\n",
    "    \n",
    "    # Preparar status de indexaci√≥n\n",
    "    indexing_status = {\n",
    "        \"index_name\": es_index,\n",
    "        \"total_chunks\": len(chunks_data),\n",
    "        \"indexed_chunks\": success_count,\n",
    "        \"failed_chunks\": len(failed_items) if failed_items else 0,\n",
    "        \"total_documents_in_index\": doc_count,\n",
    "        \"indexed_at\": datetime.now().isoformat(),\n",
    "        \"success\": len(failed_items) == 0 if failed_items else True\n",
    "    }\n",
    "    \n",
    "    # Guardar status\n",
    "    with open(index_status.path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(indexing_status, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Status de indexaci√≥n guardado\")\n",
    "'''\n",
    "    \n",
    "    # Escribir archivo\n",
    "    with open('components/vector_processing.py', 'w', encoding='utf-8') as f:\n",
    "        f.write(vector_processing_content)\n",
    "    \n",
    "    print(\"‚úÖ Archivo creado: components/vector_processing.py\")\n",
    "    print(\"üìã Components incluidos:\")\n",
    "    print(\"  - generate_embeddings_component\")\n",
    "    print(\"  - index_elasticsearch_component\")\n",
    "\n",
    "# Crear archivo de vector processing\n",
    "create_vector_processing_components()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd93c39-0827-4243-a2d6-dad949c97f5a",
   "metadata": {},
   "source": [
    "## üîó Pipeline Definition: RAG Pipeline Principal\n",
    "\n",
    "Creamos el pipeline principal que orquesta todos los components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6df058dc-d4fc-486c-a233-1fae0d571269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Archivo creado: pipelines/rag_pipeline.py\n",
      "üìã Pipeline function incluida:\n",
      "  - rag_document_pipeline: Pipeline principal\n"
     ]
    }
   ],
   "source": [
    "def create_rag_pipeline():\n",
    "    \"\"\"Crear archivo pipelines/rag_pipeline.py\"\"\"\n",
    "    \n",
    "    rag_pipeline_content = '''\"\"\"\n",
    "RAG Pipeline Principal - OpenShift AI Data Science Pipeline\n",
    "Orquesta todos los components para procesamiento completo de documentos\n",
    "Author: Carlos Estay (github: pkstaz)\n",
    "\"\"\"\n",
    "\n",
    "from kfp import dsl\n",
    "from kfp.dsl import pipeline\n",
    "\n",
    "# Importar components\n",
    "from components.text_processing import extract_text_component, chunk_text_component\n",
    "from components.vector_processing import generate_embeddings_component, index_elasticsearch_component\n",
    "\n",
    "@pipeline(\n",
    "    name=\"rag-document-processing-v1\",\n",
    "    description=\"Pipeline completo de procesamiento de documentos RAG para OpenShift AI\"\n",
    ")\n",
    "def rag_document_pipeline(\n",
    "    bucket_name: str = \"raw-documents\",\n",
    "    object_key: str = \"\",\n",
    "    minio_endpoint: str = \"minio:9000\",\n",
    "    minio_access_key: str = \"minio\",\n",
    "    minio_secret_key: str = \"minio123\",\n",
    "    es_endpoint: str = \"elasticsearch:9200\",\n",
    "    es_index: str = \"rag-documents\",\n",
    "    chunk_size: int = 512,\n",
    "    chunk_overlap: int = 50,\n",
    "    embedding_model: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Pipeline completo de procesamiento de documentos RAG.\n",
    "    \n",
    "    Args:\n",
    "        bucket_name: Nombre del bucket en MinIO\n",
    "        object_key: Path del archivo a procesar\n",
    "        minio_endpoint: Endpoint de MinIO\n",
    "        minio_access_key: Access key de MinIO\n",
    "        minio_secret_key: Secret key de MinIO\n",
    "        es_endpoint: Endpoint de ElasticSearch\n",
    "        es_index: Nombre del √≠ndice en ElasticSearch\n",
    "        chunk_size: Tama√±o de chunks en tokens\n",
    "        chunk_overlap: Overlap entre chunks en tokens\n",
    "        embedding_model: Modelo para generar embeddings\n",
    "    \n",
    "    Returns:\n",
    "        Status de indexaci√≥n final\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Extract text from document\n",
    "    extract_task = extract_text_component(\n",
    "        bucket_name=bucket_name,\n",
    "        object_key=object_key,\n",
    "        minio_endpoint=minio_endpoint,\n",
    "        minio_access_key=minio_access_key,\n",
    "        minio_secret_key=minio_secret_key\n",
    "    )\n",
    "    extract_task.set_display_name(\"üìÑ Extract Text\")\n",
    "    extract_task.set_cpu_limit(\"500m\")\n",
    "    extract_task.set_memory_limit(\"1Gi\")\n",
    "    extract_task.set_retry(3)\n",
    "    \n",
    "    # Step 2: Chunk the extracted text\n",
    "    chunk_task = chunk_text_component(\n",
    "        extracted_text=extract_task.outputs['extracted_text'],\n",
    "        metadata=extract_task.outputs['metadata'],\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    ).after(extract_task)\n",
    "    chunk_task.set_display_name(\"üß© Chunk Text\")\n",
    "    chunk_task.set_cpu_limit(\"500m\")\n",
    "    chunk_task.set_memory_limit(\"1Gi\")\n",
    "    chunk_task.set_retry(3)\n",
    "    \n",
    "    # Step 3: Generate embeddings\n",
    "    embedding_task = generate_embeddings_component(\n",
    "        chunks=chunk_task.outputs['chunks'],\n",
    "        model_name=embedding_model\n",
    "    ).after(chunk_task)\n",
    "    embedding_task.set_display_name(\"üéØ Generate Embeddings\")\n",
    "    embedding_task.set_cpu_limit(\"1000m\")\n",
    "    embedding_task.set_memory_limit(\"4Gi\")\n",
    "    embedding_task.set_retry(2)\n",
    "    # embedding_task.set_gpu_limit(\"1\")  # Uncomment si hay GPUs disponibles\n",
    "    \n",
    "    # Step 4: Index in ElasticSearch\n",
    "    index_task = index_elasticsearch_component(\n",
    "        enriched_chunks=embedding_task.outputs['embeddings'],\n",
    "        es_endpoint=es_endpoint,\n",
    "        es_index=es_index\n",
    "    ).after(embedding_task)\n",
    "    index_task.set_display_name(\"üîç Index ElasticSearch\")\n",
    "    index_task.set_cpu_limit(\"500m\")\n",
    "    index_task.set_memory_limit(\"2Gi\")\n",
    "    index_task.set_retry(3)\n",
    "    \n",
    "    # Return final status\n",
    "    return index_task.outputs['index_status']\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Para testing local del pipeline definition\n",
    "    print(\"‚úÖ RAG Pipeline definido correctamente\")\n",
    "    print(\"üìã Pipeline functions disponibles:\")\n",
    "    print(\"  - rag_document_pipeline: Procesamiento individual\")\n",
    "'''\n",
    "    \n",
    "    # Escribir archivo\n",
    "    with open('pipelines/rag_pipeline.py', 'w', encoding='utf-8') as f:\n",
    "        f.write(rag_pipeline_content)\n",
    "    \n",
    "    print(\"‚úÖ Archivo creado: pipelines/rag_pipeline.py\")\n",
    "    print(\"üìã Pipeline function incluida:\")\n",
    "    print(\"  - rag_document_pipeline: Pipeline principal\")\n",
    "\n",
    "# Crear archivo del pipeline\n",
    "create_rag_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a861af26-a347-462a-8327-9f2f8fa2ed8e",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration Files\n",
    "\n",
    "Creamos los archivos de configuraci√≥n necesarios para el deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b60954e6-e468-4e11-a685-82ce1a11d350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Creado: config/pipeline_config.yaml\n",
      "‚úÖ Creado: config/secrets.yaml\n",
      "‚úÖ Creado: requirements.txt\n",
      "‚úÖ Creado: config/env.template\n",
      "\n",
      "üìã Archivos de configuraci√≥n creados:\n",
      "  config/pipeline_config.yaml - Configuraci√≥n del pipeline\n",
      "  config/secrets.yaml - Template de secrets K8s\n",
      "  requirements.txt - Dependencias Python\n",
      "  config/env.template - Variables de ambiente\n"
     ]
    }
   ],
   "source": [
    "def create_configuration_files():\n",
    "    \"\"\"Crear archivos de configuraci√≥n del proyecto\"\"\"\n",
    "    \n",
    "    # 1. Pipeline Configuration\n",
    "    pipeline_config = {\n",
    "        \"pipeline\": {\n",
    "            \"name\": \"rag-document-processing-v1\",\n",
    "            \"version\": \"1.0.0\",\n",
    "            \"description\": \"RAG Document Processing Pipeline para OpenShift AI\",\n",
    "            \"author\": \"Carlos Estay\",\n",
    "            \"github\": \"pkstaz\",\n",
    "            \"created\": datetime.now().isoformat()\n",
    "        },\n",
    "        \"components\": {\n",
    "            \"base_image\": \"pytorch/pytorch:2.0.1-cuda11.7-cudnn8-devel\",\n",
    "            \"extract_text\": {\n",
    "                \"cpu_limit\": \"500m\",\n",
    "                \"memory_limit\": \"1Gi\",\n",
    "                \"retry_limit\": 3\n",
    "            },\n",
    "            \"chunk_text\": {\n",
    "                \"cpu_limit\": \"500m\", \n",
    "                \"memory_limit\": \"1Gi\",\n",
    "                \"retry_limit\": 3\n",
    "            },\n",
    "            \"generate_embeddings\": {\n",
    "                \"cpu_limit\": \"1000m\",\n",
    "                \"memory_limit\": \"4Gi\", \n",
    "                \"retry_limit\": 2,\n",
    "                \"gpu_limit\": \"0\"  # Set to \"1\" si hay GPU disponible\n",
    "            },\n",
    "            \"index_elasticsearch\": {\n",
    "                \"cpu_limit\": \"500m\",\n",
    "                \"memory_limit\": \"2Gi\",\n",
    "                \"retry_limit\": 3\n",
    "            }\n",
    "        },\n",
    "        \"default_parameters\": {\n",
    "            \"chunk_size\": 512,\n",
    "            \"chunk_overlap\": 50,\n",
    "            \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "            \"es_index\": \"rag-documents\"\n",
    "        },\n",
    "        \"storage\": {\n",
    "            \"minio\": {\n",
    "                \"endpoint\": \"minio:9000\",\n",
    "                \"access_key\": \"minio\",\n",
    "                \"secret_key\": \"minio123\",\n",
    "                \"bucket_raw\": \"raw-documents\",\n",
    "                \"bucket_processed\": \"processed-documents\",\n",
    "                \"bucket_failed\": \"failed-documents\",\n",
    "                \"bucket_pipeline\": \"pipeline\"\n",
    "            },\n",
    "            \"elasticsearch\": {\n",
    "                \"endpoint\": \"elasticsearch:9200\",\n",
    "                \"index_prefix\": \"rag-\",\n",
    "                \"replicas\": 0,\n",
    "                \"shards\": 1\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open('config/pipeline_config.yaml', 'w') as f:\n",
    "        yaml.dump(pipeline_config, f, indent=2, default_flow_style=False)\n",
    "    \n",
    "    print(\"‚úÖ Creado: config/pipeline_config.yaml\")\n",
    "    \n",
    "    # 2. Secrets Template\n",
    "    secrets_template = {\n",
    "        \"apiVersion\": \"v1\",\n",
    "        \"kind\": \"Secret\",\n",
    "        \"metadata\": {\n",
    "            \"name\": \"rag-pipeline-secrets\",\n",
    "            \"namespace\": \"rag-openshift-ai\",\n",
    "            \"labels\": {\n",
    "                \"app\": \"rag-pipeline\",\n",
    "                \"author\": \"carlos-estay\",\n",
    "                \"github\": \"pkstaz\"\n",
    "            }\n",
    "        },\n",
    "        \"type\": \"Opaque\",\n",
    "        \"stringData\": {\n",
    "            \"minio-access-key\": \"minio\",\n",
    "            \"minio-secret-key\": \"minio123\",\n",
    "            \"elasticsearch-username\": \"\",\n",
    "            \"elasticsearch-password\": \"\",\n",
    "            \"pipeline-webhook-token\": \"rag-pipeline-token-change-me\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open('config/secrets.yaml', 'w') as f:\n",
    "        yaml.dump(secrets_template, f, indent=2, default_flow_style=False)\n",
    "    \n",
    "    print(\"‚úÖ Creado: config/secrets.yaml\")\n",
    "    \n",
    "    # 3. Requirements file\n",
    "    requirements_content = '''# RAG Pipeline Requirements\n",
    "# Author: Carlos Estay (github: pkstaz)\n",
    "# Core KFP\n",
    "kfp>=2.0.0\n",
    "\n",
    "# Document Processing\n",
    "PyPDF2==3.0.1\n",
    "python-docx==0.8.11\n",
    "chardet==5.2.0\n",
    "\n",
    "# Text Processing\n",
    "tiktoken==0.5.1\n",
    "langchain==0.0.350\n",
    "\n",
    "# ML/Embeddings\n",
    "sentence-transformers==2.2.2\n",
    "torch>=2.0.1\n",
    "numpy==1.24.3\n",
    "\n",
    "# Storage/DB\n",
    "minio==7.1.17\n",
    "elasticsearch==8.11.0\n",
    "\n",
    "# Web/API (for webhook)\n",
    "flask==2.3.3\n",
    "requests==2.31.0\n",
    "\n",
    "# Utilities\n",
    "pyyaml==6.0.1\n",
    "python-dateutil==2.8.2\n",
    "'''\n",
    "    \n",
    "    with open('requirements.txt', 'w') as f:\n",
    "        f.write(requirements_content)\n",
    "    \n",
    "    print(\"‚úÖ Creado: requirements.txt\")\n",
    "    \n",
    "    # 4. Environment variables template\n",
    "    env_template = '''# RAG Pipeline Environment Variables\n",
    "# Author: Carlos Estay (github: pkstaz)\n",
    "# Copy to .env and modify as needed\n",
    "\n",
    "# MinIO Configuration\n",
    "MINIO_ENDPOINT=minio:9000\n",
    "MINIO_ACCESS_KEY=minio\n",
    "MINIO_SECRET_KEY=minio123\n",
    "MINIO_SECURE=false\n",
    "\n",
    "# ElasticSearch Configuration  \n",
    "ES_ENDPOINT=elasticsearch:9200\n",
    "ES_INDEX=rag-documents\n",
    "ES_USERNAME=\n",
    "ES_PASSWORD=\n",
    "\n",
    "# Pipeline Configuration\n",
    "CHUNK_SIZE=512\n",
    "CHUNK_OVERLAP=50\n",
    "EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2\n",
    "\n",
    "# OpenShift AI Pipeline\n",
    "KFP_HOST=http://ml-pipeline:8888\n",
    "PIPELINE_NAMESPACE=rag-openshift-ai\n",
    "\n",
    "# Webhook Configuration\n",
    "WEBHOOK_PORT=8080\n",
    "WEBHOOK_TOKEN=rag-pipeline-token-change-me\n",
    "'''\n",
    "    \n",
    "    with open('config/env.template', 'w') as f:\n",
    "        f.write(env_template)\n",
    "    \n",
    "    print(\"‚úÖ Creado: config/env.template\")\n",
    "    \n",
    "    print(\"\\nüìã Archivos de configuraci√≥n creados:\")\n",
    "    print(\"  config/pipeline_config.yaml - Configuraci√≥n del pipeline\")\n",
    "    print(\"  config/secrets.yaml - Template de secrets K8s\")\n",
    "    print(\"  requirements.txt - Dependencias Python\")  \n",
    "    print(\"  config/env.template - Variables de ambiente\")\n",
    "\n",
    "# Crear archivos de configuraci√≥n\n",
    "create_configuration_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432bfe28-97fd-4f7c-ba34-f2d3069338c0",
   "metadata": {},
   "source": [
    "## ü™£ Configuraci√≥n de MinIO - Crear Buckets\n",
    "\n",
    "Configuramos MinIO y creamos todos los buckets necesarios incluyendo el bucket \"pipeline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ced5bd2-2026-4962-a90b-060016d41b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Configuraci√≥n completa de MinIO para OpenShift AI DSP\n",
      "============================================================\n",
      "üîß Configurando MinIO y creando buckets completos...\n",
      "üîó Conectando a MinIO: minio:9000\n",
      "‚úÖ Conectado a MinIO exitosamente\n",
      "üìä Buckets existentes: 6\n",
      "  - failed-documents (creado: 2025-07-01 17:40:26.746000+00:00)\n",
      "  - pipeline (creado: 2025-07-01 17:46:35.035000+00:00)\n",
      "  - processed-documents (creado: 2025-07-01 17:40:26.739000+00:00)\n",
      "  - rag-documents (creado: 2025-07-01 16:45:51.180000+00:00)\n",
      "  - raw-documents (creado: 2025-07-01 17:40:26.732000+00:00)\n",
      "  - test-datasets (creado: 2025-07-01 17:40:26.753000+00:00)\n",
      "\n",
      "ü™£ Creando buckets necesarios...\n",
      "  ‚ÑπÔ∏è Bucket ya existe: pipeline\n",
      "    üìù Bucket requerido por OpenShift AI Data Science Pipelines\n",
      "  ‚ÑπÔ∏è Bucket ya existe: raw-documents\n",
      "    üìù Documentos originales subidos por usuarios\n",
      "  ‚ÑπÔ∏è Bucket ya existe: processed-documents\n",
      "    üìù Documentos procesados exitosamente\n",
      "  ‚ÑπÔ∏è Bucket ya existe: failed-documents\n",
      "    üìù Documentos que fallaron en procesamiento\n",
      "  ‚ÑπÔ∏è Bucket ya existe: test-datasets\n",
      "    üìù Datasets de prueba para desarrollo\n",
      "\n",
      "üìä Resumen de buckets:\n",
      "  ‚úÖ Creados: 0\n",
      "  ‚ÑπÔ∏è Ya exist√≠an: 5\n",
      "\n",
      "üóÇÔ∏è Buckets disponibles:\n",
      "  - failed-documents\n",
      "  - pipeline\n",
      "  - processed-documents\n",
      "  - rag-documents\n",
      "  - raw-documents\n",
      "  - test-datasets\n",
      "\n",
      "‚úÖ Bucket 'pipeline' configurado correctamente para OpenShift AI DSP\n",
      "\n",
      "üìÑ Subiendo documento de prueba...\n",
      "‚úÖ Documento subido: raw-documents/test-document-openshift-ai.txt\n",
      "üìè Tama√±o: 1809 caracteres\n",
      "üìä Palabras aproximadas: 241\n",
      "üìã Verificaci√≥n:\n",
      "  Size: 1828 bytes\n",
      "  Content-Type: text/plain\n",
      "  Last-Modified: 2025-07-01 18:55:05+00:00\n",
      "\n",
      "üéØ MINIO CONFIGURADO PARA OPENSHIFT AI DSP:\n",
      "  ‚úÖ Bucket 'pipeline' creado (requerido por OpenShift AI)\n",
      "  ‚úÖ Buckets RAG creados (raw-documents, processed-documents, etc.)\n",
      "  ‚úÖ Documento de prueba subido: test-document-openshift-ai.txt\n",
      "  üìä Tama√±o: 241 palabras\n",
      "\n",
      "üöÄ LISTO PARA CONFIGURAR OPENSHIFT AI DSP\n"
     ]
    }
   ],
   "source": [
    "def setup_minio_buckets_complete():\n",
    "    \"\"\"Configurar MinIO y crear todos los buckets necesarios incluyendo 'pipeline'\"\"\"\n",
    "    \n",
    "    print(\"üîß Configurando MinIO y creando buckets completos...\")\n",
    "    \n",
    "    # Configuraci√≥n MinIO\n",
    "    MINIO_ENDPOINT = \"minio:9000\"\n",
    "    MINIO_ACCESS_KEY = \"minio\"\n",
    "    MINIO_SECRET_KEY = \"minio123\"\n",
    "    \n",
    "    try:\n",
    "        from minio import Minio\n",
    "        from minio.error import S3Error\n",
    "        \n",
    "        # Conectar a MinIO\n",
    "        print(f\"üîó Conectando a MinIO: {MINIO_ENDPOINT}\")\n",
    "        \n",
    "        minio_client = Minio(\n",
    "            MINIO_ENDPOINT,\n",
    "            access_key=MINIO_ACCESS_KEY,\n",
    "            secret_key=MINIO_SECRET_KEY,\n",
    "            secure=False  # HTTP para desarrollo\n",
    "        )\n",
    "        \n",
    "        # Verificar conectividad\n",
    "        try:\n",
    "            buckets = minio_client.list_buckets()\n",
    "            print(f\"‚úÖ Conectado a MinIO exitosamente\")\n",
    "            print(f\"üìä Buckets existentes: {len(buckets)}\")\n",
    "            for bucket in buckets:\n",
    "                print(f\"  - {bucket.name} (creado: {bucket.creation_date})\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error conectando a MinIO: {str(e)}\")\n",
    "            return False\n",
    "        \n",
    "        # Buckets necesarios para RAG pipeline + OpenShift AI DSP\n",
    "        required_buckets = [\n",
    "            {\n",
    "                \"name\": \"pipeline\",\n",
    "                \"description\": \"Bucket requerido por OpenShift AI Data Science Pipelines\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"raw-documents\",\n",
    "                \"description\": \"Documentos originales subidos por usuarios\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"processed-documents\", \n",
    "                \"description\": \"Documentos procesados exitosamente\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"failed-documents\",\n",
    "                \"description\": \"Documentos que fallaron en procesamiento\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"test-datasets\",\n",
    "                \"description\": \"Datasets de prueba para desarrollo\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\nü™£ Creando buckets necesarios...\")\n",
    "        \n",
    "        created_buckets = []\n",
    "        existing_buckets = []\n",
    "        \n",
    "        for bucket_info in required_buckets:\n",
    "            bucket_name = bucket_info[\"name\"]\n",
    "            description = bucket_info[\"description\"]\n",
    "            \n",
    "            try:\n",
    "                # Verificar si bucket ya existe\n",
    "                if minio_client.bucket_exists(bucket_name):\n",
    "                    existing_buckets.append(bucket_name)\n",
    "                    print(f\"  ‚ÑπÔ∏è Bucket ya existe: {bucket_name}\")\n",
    "                else:\n",
    "                    # Crear bucket\n",
    "                    minio_client.make_bucket(bucket_name)\n",
    "                    created_buckets.append(bucket_name)\n",
    "                    print(f\"  ‚úÖ Bucket creado: {bucket_name}\")\n",
    "                \n",
    "                print(f\"    üìù {description}\")\n",
    "                    \n",
    "            except S3Error as e:\n",
    "                print(f\"  ‚ùå Error con bucket {bucket_name}: {str(e)}\")\n",
    "        \n",
    "        print(f\"\\nüìä Resumen de buckets:\")\n",
    "        print(f\"  ‚úÖ Creados: {len(created_buckets)}\")\n",
    "        print(f\"  ‚ÑπÔ∏è Ya exist√≠an: {len(existing_buckets)}\")\n",
    "        \n",
    "        # Listar todos los buckets despu√©s de la configuraci√≥n\n",
    "        final_buckets = minio_client.list_buckets()\n",
    "        print(f\"\\nüóÇÔ∏è Buckets disponibles:\")\n",
    "        for bucket in final_buckets:\n",
    "            print(f\"  - {bucket.name}\")\n",
    "        \n",
    "        # Verificar bucket cr√≠tico para OpenShift AI\n",
    "        if minio_client.bucket_exists(\"pipeline\"):\n",
    "            print(f\"\\n‚úÖ Bucket 'pipeline' configurado correctamente para OpenShift AI DSP\")\n",
    "        else:\n",
    "            print(f\"\\n‚ùå Bucket 'pipeline' no se pudo crear - requerido para OpenShift AI DSP\")\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"‚ùå MinIO client no disponible. Instalar con: pip install minio\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error configurando MinIO: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def upload_test_document():\n",
    "    \"\"\"Subir documento de prueba al bucket raw-documents\"\"\"\n",
    "    \n",
    "    print(f\"\\nüìÑ Subiendo documento de prueba...\")\n",
    "    \n",
    "    try:\n",
    "        from minio import Minio\n",
    "        import tempfile\n",
    "        \n",
    "        # Conectar a MinIO\n",
    "        minio_client = Minio(\n",
    "            \"minio:9000\",\n",
    "            access_key=\"minio\",\n",
    "            secret_key=\"minio123\",\n",
    "            secure=False\n",
    "        )\n",
    "        \n",
    "        # Crear documento de prueba m√°s detallado\n",
    "        test_content = \"\"\"# Documento de Prueba RAG Pipeline - OpenShift AI\n",
    "        \n",
    "Este es un documento de prueba para validar el pipeline RAG en OpenShift AI Data Science Pipelines.\n",
    "\n",
    "## Configuraci√≥n del Ambiente\n",
    "\n",
    "### MinIO Buckets Configurados:\n",
    "- **pipeline**: Bucket requerido por OpenShift AI DSP\n",
    "- **raw-documents**: Documentos originales \n",
    "- **processed-documents**: Documentos procesados exitosamente\n",
    "- **failed-documents**: Documentos con errores de procesamiento\n",
    "- **test-datasets**: Datasets para desarrollo y pruebas\n",
    "\n",
    "### Pipeline de Procesamiento RAG\n",
    "\n",
    "El sistema procesa documentos siguiendo estos pasos:\n",
    "\n",
    "1. **Extracci√≥n de texto**: \n",
    "   - Soporta PDF, DOCX, TXT\n",
    "   - Detecta encoding autom√°ticamente\n",
    "   - Extrae metadatos del documento\n",
    "\n",
    "2. **Chunking inteligente**:\n",
    "   - Chunks de 512 tokens con 50 de overlap\n",
    "   - Usa tiktoken para conteo preciso\n",
    "   - Mantiene contexto entre fragmentos\n",
    "\n",
    "3. **Generaci√≥n de embeddings**:\n",
    "   - Modelo: sentence-transformers/all-MiniLM-L6-v2\n",
    "   - Vectores de 384 dimensiones\n",
    "   - Normalizaci√≥n para cosine similarity\n",
    "\n",
    "4. **Indexaci√≥n en ElasticSearch**:\n",
    "   - √çndice h√≠brido (texto + vectores)\n",
    "   - B√∫squeda sem√°ntica y por palabras clave\n",
    "   - Metadatos preservados para trazabilidad\n",
    "\n",
    "## Informaci√≥n del Proyecto\n",
    "\n",
    "- **Autor**: Carlos Estay\n",
    "- **GitHub**: pkstaz  \n",
    "- **Pipeline**: rag-document-processing-v1\n",
    "- **Platform**: OpenShift AI Data Science Pipelines\n",
    "\n",
    "Este documento ser√° procesado autom√°ticamente cuando el webhook detecte \n",
    "su upload al bucket raw-documents.\n",
    "\n",
    "## Testing del Pipeline\n",
    "\n",
    "Para validar el funcionamiento:\n",
    "\n",
    "1. Este documento se procesa autom√°ticamente\n",
    "2. Se generan aproximadamente 4-6 chunks\n",
    "3. Cada chunk obtiene un embedding de 384 dimensiones  \n",
    "4. Se indexa en ElasticSearch con ID √∫nico\n",
    "5. Est√° disponible para b√∫squeda sem√°ntica\n",
    "\n",
    "¬°Pipeline RAG funcionando en OpenShift AI!\n",
    "\"\"\"\n",
    "        \n",
    "        # Crear archivo temporal\n",
    "        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8') as f:\n",
    "            f.write(test_content)\n",
    "            temp_file_path = f.name\n",
    "        \n",
    "        # Subir archivo a MinIO\n",
    "        object_name = \"test-document-openshift-ai.txt\"\n",
    "        bucket_name = \"raw-documents\"\n",
    "        \n",
    "        try:\n",
    "            minio_client.fput_object(\n",
    "                bucket_name=bucket_name,\n",
    "                object_name=object_name,\n",
    "                file_path=temp_file_path,\n",
    "                content_type=\"text/plain\"\n",
    "            )\n",
    "            \n",
    "            print(f\"‚úÖ Documento subido: {bucket_name}/{object_name}\")\n",
    "            print(f\"üìè Tama√±o: {len(test_content)} caracteres\")\n",
    "            print(f\"üìä Palabras aproximadas: {len(test_content.split())}\")\n",
    "            \n",
    "            # Verificar que se subi√≥ correctamente\n",
    "            try:\n",
    "                stat = minio_client.stat_object(bucket_name, object_name)\n",
    "                print(f\"üìã Verificaci√≥n:\")\n",
    "                print(f\"  Size: {stat.size} bytes\")\n",
    "                print(f\"  Content-Type: {stat.content_type}\")\n",
    "                print(f\"  Last-Modified: {stat.last_modified}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è No se pudo verificar el archivo: {str(e)}\")\n",
    "            \n",
    "            return {\n",
    "                \"bucket\": bucket_name,\n",
    "                \"object\": object_name,\n",
    "                \"size\": len(test_content),\n",
    "                \"word_count\": len(test_content.split()),\n",
    "                \"success\": True\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error subiendo archivo: {str(e)}\")\n",
    "            return None\n",
    "            \n",
    "        finally:\n",
    "            # Limpiar archivo temporal\n",
    "            import os\n",
    "            if os.path.exists(temp_file_path):\n",
    "                os.unlink(temp_file_path)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en upload de documento de prueba: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Ejecutar configuraci√≥n completa de MinIO\n",
    "print(\"üöÄ Configuraci√≥n completa de MinIO para OpenShift AI DSP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "minio_setup_success = setup_minio_buckets_complete()\n",
    "\n",
    "# Subir documento de prueba si MinIO est√° configurado\n",
    "if minio_setup_success:\n",
    "    test_upload_result = upload_test_document()\n",
    "    \n",
    "    if test_upload_result:\n",
    "        print(f\"\\nüéØ MINIO CONFIGURADO PARA OPENSHIFT AI DSP:\")\n",
    "        print(f\"  ‚úÖ Bucket 'pipeline' creado (requerido por OpenShift AI)\")\n",
    "        print(f\"  ‚úÖ Buckets RAG creados (raw-documents, processed-documents, etc.)\")\n",
    "        print(f\"  ‚úÖ Documento de prueba subido: {test_upload_result['object']}\")\n",
    "        print(f\"  üìä Tama√±o: {test_upload_result['word_count']} palabras\")\n",
    "        print(f\"\\nüöÄ LISTO PARA CONFIGURAR OPENSHIFT AI DSP\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è MinIO configurado pero falta documento de prueba\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Configurar MinIO antes de continuar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32d3679-1682-41c3-8d11-b85af8997978",
   "metadata": {},
   "source": [
    "## üî® Compilaci√≥n del Pipeline - KFP 2.12.1 Compatible\n",
    "\n",
    "Compilamos el pipeline usando la sintaxis correcta para KFP 2.12.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dd540c20-9d52-42a4-9dd4-2341cc7dab00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî® Compilando Pipeline Standalone...\n",
      "‚úÖ Imports para KFP 2.12.1 correctos\n",
      "‚úÖ Pipeline simplificado definido con sintaxis KFP 2.12.1\n",
      "‚úÖ Pipeline compilado exitosamente: rag_simple_pipeline_v1.yaml\n",
      "üìÑ Tama√±o del archivo: 6423 bytes\n",
      "\n",
      "üìù Primeras l√≠neas del YAML:\n",
      "  1: # PIPELINE DEFINITION\n",
      "  2: # Name: rag-simple-test-v1\n",
      "  3: # Description: Pipeline RAG simplificado para testing - Carlos Estay (pkstaz)\n",
      "  4: # Inputs:\n",
      "  5: #    bucket_name: str [Default: 'raw-documents']\n",
      "  6: #    es_endpoint: str [Default: 'elasticsearch:9200']\n",
      "  7: #    object_key: str [Default: 'test-document-openshift-ai.txt']\n",
      "  8: # Outputs:\n",
      "\n",
      "‚úÖ PIPELINE COMPILADO EXITOSAMENTE\n",
      "üìÑ Archivo: rag_simple_pipeline_v1.yaml\n",
      "üöÄ Listo para subir a OpenShift AI Dashboard\n"
     ]
    }
   ],
   "source": [
    "def compile_standalone_pipeline():\n",
    "    \"\"\"Compilar pipeline standalone compatible con KFP 2.12.1\"\"\"\n",
    "    \n",
    "    print(\"üî® Compilando Pipeline Standalone...\")\n",
    "    \n",
    "    try:\n",
    "        from kfp import dsl\n",
    "        from kfp.dsl import component, pipeline\n",
    "        from kfp import compiler\n",
    "        \n",
    "        print(\"‚úÖ Imports para KFP 2.12.1 correctos\")\n",
    "        \n",
    "        @component(\n",
    "            base_image=\"pytorch/pytorch:2.0.1-cuda11.7-cudnn8-devel\",\n",
    "            packages_to_install=[\"PyPDF2==3.0.1\", \"minio==7.1.17\"]\n",
    "        )\n",
    "        def simple_extract_component(\n",
    "            bucket_name: str,\n",
    "            object_key: str\n",
    "        ) -> str:\n",
    "            \"\"\"Component simplificado para testing - retorna string\"\"\"\n",
    "            \n",
    "            # Simular extracci√≥n\n",
    "            test_content = f\"Contenido extra√≠do de {object_key} en bucket {bucket_name}\"\n",
    "            \n",
    "            print(f\"‚úÖ Texto extra√≠do: {len(test_content)} caracteres\")\n",
    "            \n",
    "            return test_content\n",
    "\n",
    "        @component(\n",
    "            base_image=\"pytorch/pytorch:2.0.1-cuda11.7-cudnn8-devel\",\n",
    "            packages_to_install=[\"elasticsearch==8.11.0\"]\n",
    "        )\n",
    "        def simple_index_component(\n",
    "            extracted_text: str,\n",
    "            es_endpoint: str\n",
    "        ) -> dict:\n",
    "            \"\"\"Component simplificado de indexaci√≥n - retorna dict\"\"\"\n",
    "            from datetime import datetime\n",
    "            \n",
    "            # Simular indexaci√≥n\n",
    "            status = {\n",
    "                \"indexed_at\": datetime.now().isoformat(),\n",
    "                \"content_length\": len(extracted_text),\n",
    "                \"es_endpoint\": es_endpoint,\n",
    "                \"success\": True\n",
    "            }\n",
    "            \n",
    "            print(f\"‚úÖ Indexaci√≥n simulada completada\")\n",
    "            \n",
    "            return status\n",
    "\n",
    "        @pipeline(\n",
    "            name=\"rag-simple-test-v1\",\n",
    "            description=\"Pipeline RAG simplificado para testing - Carlos Estay (pkstaz)\"\n",
    "        )\n",
    "        def simple_rag_pipeline(\n",
    "            bucket_name: str = \"raw-documents\",\n",
    "            object_key: str = \"test-document-openshift-ai.txt\",\n",
    "            es_endpoint: str = \"elasticsearch:9200\"\n",
    "        ) -> dict:\n",
    "            \"\"\"Pipeline simplificado para verificar compilaci√≥n\"\"\"\n",
    "            \n",
    "            # Step 1: Extract\n",
    "            extract_task = simple_extract_component(\n",
    "                bucket_name=bucket_name,\n",
    "                object_key=object_key\n",
    "            )\n",
    "            extract_task.set_display_name(\"üìÑ Simple Extract\")\n",
    "            extract_task.set_cpu_limit(\"500m\")\n",
    "            extract_task.set_memory_limit(\"1Gi\")\n",
    "            \n",
    "            # Step 2: Index\n",
    "            index_task = simple_index_component(\n",
    "                extracted_text=extract_task.output,\n",
    "                es_endpoint=es_endpoint\n",
    "            )\n",
    "            index_task.set_display_name(\"üîç Simple Index\")\n",
    "            index_task.set_cpu_limit(\"500m\")\n",
    "            index_task.set_memory_limit(\"1Gi\")\n",
    "            \n",
    "            return index_task.output\n",
    "        \n",
    "        print(\"‚úÖ Pipeline simplificado definido con sintaxis KFP 2.12.1\")\n",
    "        \n",
    "        # Compilar pipeline\n",
    "        output_file = 'rag_simple_pipeline_v1.yaml'\n",
    "        \n",
    "        compiler.Compiler().compile(\n",
    "            pipeline_func=simple_rag_pipeline,\n",
    "            package_path=output_file\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Pipeline compilado exitosamente: {output_file}\")\n",
    "        \n",
    "        # Verificar archivo\n",
    "        if Path(output_file).exists():\n",
    "            file_size = Path(output_file).stat().st_size\n",
    "            print(f\"üìÑ Tama√±o del archivo: {file_size} bytes\")\n",
    "            \n",
    "            # Verificar contenido\n",
    "            with open(output_file, 'r') as f:\n",
    "                content = f.read()\n",
    "                \n",
    "            if 'apiVersion' in content and 'kind' in content:\n",
    "                print(\"‚úÖ YAML v√°lido generado\")\n",
    "                \n",
    "                # Contar components\n",
    "                component_count = content.count('componentRef')\n",
    "                print(f\"üìä Components encontrados en YAML: {component_count}\")\n",
    "                \n",
    "                # Mostrar metadata\n",
    "                try:\n",
    "                    yaml_data = yaml.safe_load(content)\n",
    "                    metadata = yaml_data.get('metadata', {})\n",
    "                    print(f\"üìã Pipeline name: {metadata.get('name', 'N/A')}\")\n",
    "                except:\n",
    "                    print(\"‚ö†Ô∏è No se pudo parsear metadata del YAML\")\n",
    "            \n",
    "            print(f\"\\nüìù Primeras l√≠neas del YAML:\")\n",
    "            lines = content.split('\\n')[:8]\n",
    "            for i, line in enumerate(lines, 1):\n",
    "                print(f\"  {i}: {line}\")\n",
    "        \n",
    "        return output_file\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en compilaci√≥n: {str(e)}\")\n",
    "        \n",
    "        # M√°s debugging espec√≠fico\n",
    "        import traceback\n",
    "        print(f\"\\nüîç Traceback completo:\")\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        return None\n",
    "\n",
    "# Ejecutar compilaci√≥n\n",
    "compiled_file = compile_standalone_pipeline()\n",
    "\n",
    "if compiled_file:\n",
    "    print(f\"\\n‚úÖ PIPELINE COMPILADO EXITOSAMENTE\")\n",
    "    print(f\"üìÑ Archivo: {compiled_file}\")\n",
    "    print(f\"üöÄ Listo para subir a OpenShift AI Dashboard\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Error en compilaci√≥n del pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab83d764-fde4-4b69-8487-27c0c7b09549",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Configuraci√≥n de OpenShift AI Data Science Pipeline v2\n",
    "\n",
    "Creamos la configuraci√≥n compatible con OpenShift AI v2 pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9273d3e9-698e-4ff1-b72b-d093d951f703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Creando configuraci√≥n v2 para OpenShift AI con MinIO externo\n",
      "==================================================\n",
      "üîÑ Creando configuraci√≥n v2 compatible con MinIO externo...\n",
      "‚úÖ Creado: deploy/openshift-ai/dsp-config-v2.yaml\n",
      "‚úÖ Creado: deploy/openshift-ai/minio-secret.yaml\n",
      "‚úÖ Creado: deploy/openshift-ai/namespace.yaml\n",
      "\n",
      "üìú Creando scripts de deployment...\n",
      "‚úÖ Creado: deploy/openshift-ai/cleanup-v1.sh (executable)\n",
      "‚úÖ Creado: deploy/openshift-ai/deploy-v2.sh (executable)\n",
      "\n",
      "‚úÖ CONFIGURACI√ìN V2 CON MINIO EXTERNO CREADA\n",
      "\n",
      "üìÅ Archivos de OpenShift AI DSP:\n",
      "  - deploy/openshift-ai/namespace.yaml\n",
      "  - deploy/openshift-ai/dsp-config-v2.yaml (MinIO externo)\n",
      "  - deploy/openshift-ai/minio-secret.yaml\n",
      "  - deploy/openshift-ai/cleanup-v1.sh\n",
      "  - deploy/openshift-ai/deploy-v2.sh\n",
      "\n",
      "üöÄ PASOS PARA DEPLOYMENT:\n",
      "  1. Estar loggeado en OpenShift: oc login\n",
      "  2. Limpiar v1: ./deploy/openshift-ai/cleanup-v1.sh\n",
      "  3. Deploy v2: ./deploy/openshift-ai/deploy-v2.sh\n",
      "  4. Verificar: oc get pods -n rag-openshift-ai\n",
      "  5. Subir pipeline YAML via OpenShift AI Dashboard\n",
      "\n",
      "‚úÖ CONFIGURADO PARA MINIO EXTERNO:\n",
      "  - No deploy MinIO interno\n",
      "  - Usa externalStorage\n",
      "  - Endpoint: minio:9000\n",
      "  - Bucket: pipeline\n",
      "  - Credenciales: minio/minio123\n"
     ]
    }
   ],
   "source": [
    "def create_v2_compatible_dsp_config():\n",
    "    \"\"\"Crear configuraci√≥n compatible con OpenShift AI v2 pipelines usando MinIO externo\"\"\"\n",
    "    \n",
    "    print(\"üîÑ Creando configuraci√≥n v2 compatible con MinIO externo...\")\n",
    "    \n",
    "    # Configuraci√≥n actualizada para v2 con MinIO externo\n",
    "    dsp_config_v2 = {\n",
    "        \"apiVersion\": \"datasciencepipelinesapplications.opendatahub.io/v1alpha1\",\n",
    "        \"kind\": \"DataSciencePipelinesApplication\", \n",
    "        \"metadata\": {\n",
    "            \"name\": \"rag-pipeline-dsp-v2\",\n",
    "            \"namespace\": \"rag-openshift-ai\",\n",
    "            \"labels\": {\n",
    "                \"app\": \"rag-pipeline\",\n",
    "                \"version\": \"v2\",\n",
    "                \"author\": \"carlos-estay\",\n",
    "                \"github\": \"pkstaz\"\n",
    "            }\n",
    "        },\n",
    "        \"spec\": {\n",
    "            \"apiServer\": {\n",
    "                \"deploy\": True,\n",
    "                \"enableSamplePipeline\": False,\n",
    "                \"autoUpdatePipelineDefaultVersion\": True,\n",
    "                \"dbConfigConMaxLifetimeSec\": 120,\n",
    "                \"applyTektonCustomResource\": True,\n",
    "                \"cacheImage\": \"registry.redhat.io/ubi8/ubi-minimal:8.8\",\n",
    "                \"moveResultsImage\": \"registry.access.redhat.com/ubi8/ubi-minimal:8.8\",\n",
    "                \"argoLauncherImage\": \"registry.redhat.io/ubi8/ubi-minimal:8.8\",\n",
    "                \"resources\": {\n",
    "                    \"requests\": {\n",
    "                        \"cpu\": \"250m\",\n",
    "                        \"memory\": \"500Mi\"\n",
    "                    },\n",
    "                    \"limits\": {\n",
    "                        \"cpu\": \"500m\", \n",
    "                        \"memory\": \"1Gi\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"persistenceAgent\": {\n",
    "                \"deploy\": True,\n",
    "                \"numWorkers\": 2\n",
    "            },\n",
    "            \"scheduledWorkflow\": {\n",
    "                \"deploy\": True\n",
    "            },\n",
    "            \"objectStorage\": {\n",
    "                \"externalStorage\": {  # ‚úÖ Usar MinIO externo\n",
    "                    \"bucket\": \"pipeline\",\n",
    "                    \"host\": \"minio:9000\",\n",
    "                    \"scheme\": \"http\",\n",
    "                    \"s3CredentialsSecret\": {\n",
    "                        \"accessKey\": \"accesskey\",\n",
    "                        \"secretKey\": \"secretkey\", \n",
    "                        \"secretName\": \"minio-secret\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"database\": {\n",
    "                \"mariaDB\": {\n",
    "                    \"deploy\": True,\n",
    "                    \"pipelineDBName\": \"mlpipeline\",\n",
    "                    \"pvcSize\": \"10Gi\",\n",
    "                    \"resources\": {\n",
    "                        \"requests\": {\n",
    "                            \"cpu\": \"300m\",\n",
    "                            \"memory\": \"800Mi\"\n",
    "                        },\n",
    "                        \"limits\": {\n",
    "                            \"cpu\": \"1\",\n",
    "                            \"memory\": \"1Gi\"\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Sobrescribir con configuraci√≥n v2\n",
    "    with open('deploy/openshift-ai/dsp-config-v2.yaml', 'w') as f:\n",
    "        yaml.dump(dsp_config_v2, f, indent=2, default_flow_style=False)\n",
    "    \n",
    "    print(\"‚úÖ Creado: deploy/openshift-ai/dsp-config-v2.yaml\")\n",
    "    \n",
    "    # MinIO Secret para DSP\n",
    "    minio_secret = {\n",
    "        \"apiVersion\": \"v1\",\n",
    "        \"kind\": \"Secret\",\n",
    "        \"metadata\": {\n",
    "            \"name\": \"minio-secret\",\n",
    "            \"namespace\": \"rag-openshift-ai\",\n",
    "            \"labels\": {\n",
    "                \"app\": \"rag-pipeline\"\n",
    "            }\n",
    "        },\n",
    "        \"type\": \"Opaque\",\n",
    "        \"stringData\": {\n",
    "            \"accesskey\": \"minio\",\n",
    "            \"secretkey\": \"minio123\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open('deploy/openshift-ai/minio-secret.yaml', 'w') as f:\n",
    "        yaml.dump(minio_secret, f, indent=2, default_flow_style=False)\n",
    "    \n",
    "    print(\"‚úÖ Creado: deploy/openshift-ai/minio-secret.yaml\")\n",
    "    \n",
    "    # Namespace configuration\n",
    "    namespace_config = {\n",
    "        \"apiVersion\": \"v1\",\n",
    "        \"kind\": \"Namespace\",\n",
    "        \"metadata\": {\n",
    "            \"name\": \"rag-openshift-ai\",\n",
    "            \"labels\": {\n",
    "                \"opendatahub.io/dashboard\": \"true\",\n",
    "                \"modelmesh-enabled\": \"true\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open('deploy/openshift-ai/namespace.yaml', 'w') as f:\n",
    "        yaml.dump(namespace_config, f, indent=2, default_flow_style=False)\n",
    "    \n",
    "    print(\"‚úÖ Creado: deploy/openshift-ai/namespace.yaml\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def create_deployment_scripts():\n",
    "    \"\"\"Crear scripts de deployment para v2 con MinIO externo\"\"\"\n",
    "    \n",
    "    print(\"\\nüìú Creando scripts de deployment...\")\n",
    "    \n",
    "    # Script de cleanup v1\n",
    "    cleanup_script = '''#!/bin/bash\n",
    "# Cleanup script para remover pipeline v1 \n",
    "# Author: Carlos Estay (pkstaz)\n",
    "\n",
    "echo \"üßπ Cleaning up v1 pipeline resources...\"\n",
    "\n",
    "# Colors\n",
    "RED='\\\\033[0;31m'\n",
    "GREEN='\\\\033[0;32m'\n",
    "YELLOW='\\\\033[1;33m'\n",
    "NC='\\\\033[0m'\n",
    "\n",
    "# Verificar que estamos loggeados\n",
    "if ! oc whoami &> /dev/null; then\n",
    "    echo -e \"${RED}‚ùå Not logged in to OpenShift${NC}\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Eliminar DSP v1 anterior si existe\n",
    "echo -e \"${YELLOW}üóëÔ∏è Removing old v1 pipeline server...${NC}\"\n",
    "oc delete datasciencepipelinesapplication --all -n rag-openshift-ai --ignore-not-found=true\n",
    "\n",
    "# Esperar a que se complete la eliminaci√≥n\n",
    "echo -e \"${YELLOW}‚è≥ Waiting for cleanup to complete...${NC}\"\n",
    "sleep 10\n",
    "\n",
    "echo -e \"${GREEN}‚úÖ Cleanup completed!${NC}\"\n",
    "'''\n",
    "    \n",
    "    with open('deploy/openshift-ai/cleanup-v1.sh', 'w') as f:\n",
    "        f.write(cleanup_script)\n",
    "    \n",
    "    # Script de deployment v2 con MinIO externo\n",
    "    deployment_script_v2 = '''#!/bin/bash\n",
    "# OpenShift AI DSP v2 Deployment Script - MinIO Externo\n",
    "# Author: Carlos Estay (pkstaz)\n",
    "\n",
    "set -e\n",
    "\n",
    "echo \"üöÄ Deploying RAG Pipeline v2 to OpenShift AI (External MinIO)...\"\n",
    "\n",
    "# Colors\n",
    "RED='\\\\033[0;31m'\n",
    "GREEN='\\\\033[0;32m'\n",
    "YELLOW='\\\\033[1;33m'\n",
    "NC='\\\\033[0m'\n",
    "\n",
    "# Verificaciones previas\n",
    "if ! command -v oc &> /dev/null; then\n",
    "    echo -e \"${RED}‚ùå oc command not found${NC}\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "if ! oc whoami &> /dev/null; then\n",
    "    echo -e \"${RED}‚ùå Not logged in to OpenShift${NC}\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "echo -e \"${GREEN}‚úÖ OpenShift CLI ready${NC}\"\n",
    "\n",
    "# Paso 1: Cleanup v1 si existe\n",
    "echo -e \"${YELLOW}üßπ Cleaning up any existing v1 resources...${NC}\"\n",
    "oc delete datasciencepipelinesapplication --all -n rag-openshift-ai --ignore-not-found=true\n",
    "sleep 5\n",
    "\n",
    "# Paso 2: Deploy namespace\n",
    "echo -e \"${YELLOW}üì¶ Creating namespace...${NC}\"\n",
    "oc apply -f deploy/openshift-ai/namespace.yaml\n",
    "\n",
    "# Paso 3: Deploy MinIO secret\n",
    "echo -e \"${YELLOW}üîë Creating MinIO secret for external MinIO...${NC}\"\n",
    "oc apply -f deploy/openshift-ai/minio-secret.yaml\n",
    "\n",
    "# Paso 4: Verificar MinIO externo\n",
    "echo -e \"${YELLOW}üîç External MinIO Configuration:${NC}\"\n",
    "echo \"  MinIO Endpoint: minio:9000\"\n",
    "echo \"  Required Bucket: pipeline\"\n",
    "echo \"  Credentials: minio/minio123\"\n",
    "\n",
    "# Paso 5: Deploy DSP v2\n",
    "echo -e \"${YELLOW}üî¨ Creating Data Science Pipeline v2 (External MinIO)...${NC}\"\n",
    "oc apply -f deploy/openshift-ai/dsp-config-v2.yaml\n",
    "\n",
    "# Paso 6: Wait for DSP to be ready\n",
    "echo -e \"${YELLOW}‚è≥ Waiting for DSP v2 to be ready...${NC}\"\n",
    "oc wait --for=condition=Ready datasciencepipelinesapplication/rag-pipeline-dsp-v2 -n rag-openshift-ai --timeout=600s\n",
    "\n",
    "# Paso 7: Check status\n",
    "echo -e \"${YELLOW}üìä Checking DSP v2 status...${NC}\"\n",
    "oc get datasciencepipelinesapplications -n rag-openshift-ai\n",
    "oc get pods -n rag-openshift-ai\n",
    "\n",
    "echo -e \"${GREEN}‚úÖ OpenShift AI DSP v2 deployed successfully with external MinIO!${NC}\"\n",
    "\n",
    "# Next steps\n",
    "echo -e \"${YELLOW}üìã Next Steps:${NC}\"\n",
    "echo \"1. Verify external MinIO connectivity from OpenShift\"\n",
    "echo \"2. Access OpenShift AI Dashboard\"\n",
    "echo \"3. Navigate to Data Science Projects > rag-openshift-ai\"\n",
    "echo \"4. Go to Pipelines section\"\n",
    "echo \"5. Upload pipeline: rag_simple_pipeline_v1.yaml\"\n",
    "echo \"6. Create experiment and run pipeline\"\n",
    "\n",
    "echo -e \"${GREEN}üéâ Deployment completed!${NC}\"\n",
    "'''\n",
    "    \n",
    "    with open('deploy/openshift-ai/deploy-v2.sh', 'w') as f:\n",
    "        f.write(deployment_script_v2)\n",
    "    \n",
    "    # Hacer scripts ejecutables\n",
    "    Path('deploy/openshift-ai/cleanup-v1.sh').chmod(0o755)\n",
    "    Path('deploy/openshift-ai/deploy-v2.sh').chmod(0o755)\n",
    "    \n",
    "    print(\"‚úÖ Creado: deploy/openshift-ai/cleanup-v1.sh (executable)\")\n",
    "    print(\"‚úÖ Creado: deploy/openshift-ai/deploy-v2.sh (executable)\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Ejecutar creaci√≥n de archivos v2\n",
    "print(\"üîÑ Creando configuraci√≥n v2 para OpenShift AI con MinIO externo\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "v2_config = create_v2_compatible_dsp_config()\n",
    "scripts_created = create_deployment_scripts()\n",
    "\n",
    "if all([v2_config, scripts_created]):\n",
    "    print(f\"\\n‚úÖ CONFIGURACI√ìN V2 CON MINIO EXTERNO CREADA\")\n",
    "    print(f\"\\nüìÅ Archivos de OpenShift AI DSP:\")\n",
    "    print(f\"  - deploy/openshift-ai/namespace.yaml\")\n",
    "    print(f\"  - deploy/openshift-ai/dsp-config-v2.yaml (MinIO externo)\")\n",
    "    print(f\"  - deploy/openshift-ai/minio-secret.yaml\")\n",
    "    print(f\"  - deploy/openshift-ai/cleanup-v1.sh\")\n",
    "    print(f\"  - deploy/openshift-ai/deploy-v2.sh\")\n",
    "    \n",
    "    print(f\"\\nüöÄ PASOS PARA DEPLOYMENT:\")\n",
    "    print(f\"  1. Estar loggeado en OpenShift: oc login\")\n",
    "    print(f\"  2. Limpiar v1: ./deploy/openshift-ai/cleanup-v1.sh\")\n",
    "    print(f\"  3. Deploy v2: ./deploy/openshift-ai/deploy-v2.sh\")\n",
    "    print(f\"  4. Verificar: oc get pods -n rag-openshift-ai\")\n",
    "    print(f\"  5. Subir pipeline YAML via OpenShift AI Dashboard\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ CONFIGURADO PARA MINIO EXTERNO:\")\n",
    "    print(f\"  - No deploy MinIO interno\")\n",
    "    print(f\"  - Usa externalStorage\")\n",
    "    print(f\"  - Endpoint: minio:9000\")\n",
    "    print(f\"  - Bucket: pipeline\")\n",
    "    print(f\"  - Credenciales: minio/minio123\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Error creando configuraci√≥n v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde772e2-8121-4377-a983-5eedff90899a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
