{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93aec484-3cea-471f-967e-325519f7a133",
   "metadata": {},
   "source": [
    "# 02 - Pipeline Deployment - RAG OpenShift AI\n",
    "\n",
    "## üéØ Objetivo\n",
    "Crear, compilar y deployar el pipeline RAG en OpenShift AI Data Science Pipelines.\n",
    "Este notebook toma los components desarrollados en el notebook anterior y los prepara para deployment real.\n",
    "\n",
    "## üìã Lo que Haremos\n",
    "1. **Crear Components como archivos Python separados**\n",
    "2. **Definir Pipeline real con components importados**\n",
    "3. **Compilar Pipeline a YAML**\n",
    "4. **Deploy en OpenShift AI**\n",
    "5. **Testing del Pipeline deployado**\n",
    "6. **Configurar Webhook Handler**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e089481-5396-4010-9c65-f18c251c2774",
   "metadata": {},
   "source": [
    "## üîß Setup Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dad78d83-5119-4ca2-9c7d-7c1ef165f21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ KubeFlow Pipelines disponible: 2.12.1\n",
      "üîó Pipeline Host configurado: http://ml-pipeline:8888\n",
      "üöÄ Notebook de deployment iniciado\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Verificar KFP y dependencias\n",
    "try:\n",
    "    import kfp\n",
    "    from kfp import dsl, compiler\n",
    "    from kfp.client import Client\n",
    "    from kfp.dsl import component, pipeline, Input, Output, Dataset\n",
    "    print(f\"‚úÖ KubeFlow Pipelines disponible: {kfp.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå Instalando KubeFlow Pipelines...\")\n",
    "    !pip install kfp>=2.0.0\n",
    "\n",
    "# Verificar conexi√≥n a OpenShift AI (si est√° disponible)\n",
    "try:\n",
    "    # Intentar conectar al pipeline service\n",
    "    PIPELINE_HOST = os.getenv('KFP_HOST', 'http://ml-pipeline:8888')\n",
    "    print(f\"üîó Pipeline Host configurado: {PIPELINE_HOST}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Conexi√≥n a pipeline host pendiente: {e}\")\n",
    "\n",
    "print(\"üöÄ Notebook de deployment iniciado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1485945c-18b7-447f-b027-be91bbff274a",
   "metadata": {},
   "source": [
    "## üìÅ Crear Estructura de Archivos para Components\n",
    "\n",
    "Creamos los archivos Python separados para cada component del pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a459e8b-6e00-4ae5-9280-01a8e92eaf2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Creando estructura de directorios...\n",
      "  ‚úÖ components/\n",
      "  ‚úÖ pipelines/\n",
      "  ‚úÖ webhook/\n",
      "  ‚úÖ config/\n",
      "  ‚úÖ deploy/minio/\n",
      "  ‚úÖ deploy/elasticsearch/\n",
      "  ‚úÖ deploy/webhook/\n",
      "  ‚úÖ tests/\n",
      "  ‚úÖ components/__init__.py\n",
      "  ‚úÖ pipelines/__init__.py\n",
      "  ‚úÖ webhook/__init__.py\n",
      "\n",
      "üìã Estructura creada:\n",
      "  components/          # Pipeline components\n",
      "  pipelines/           # Pipeline definitions\n",
      "  webhook/             # Webhook handler\n",
      "  config/              # Configuration files\n",
      "  deploy/              # Deployment manifests\n",
      "  tests/               # Integration tests\n"
     ]
    }
   ],
   "source": [
    "def create_project_structure():\n",
    "    \"\"\"Crear la estructura de directorios y archivos del proyecto\"\"\"\n",
    "    \n",
    "    # Definir estructura de directorios\n",
    "    directories = [\n",
    "        \"components\",\n",
    "        \"pipelines\", \n",
    "        \"webhook\",\n",
    "        \"config\",\n",
    "        \"deploy/minio\",\n",
    "        \"deploy/elasticsearch\", \n",
    "        \"deploy/webhook\",\n",
    "        \"tests\"\n",
    "    ]\n",
    "    \n",
    "    print(\"üìÅ Creando estructura de directorios...\")\n",
    "    \n",
    "    # Crear directorios\n",
    "    for dir_path in directories:\n",
    "        Path(dir_path).mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"  ‚úÖ {dir_path}/\")\n",
    "    \n",
    "    # Crear archivos __init__.py para modules Python\n",
    "    init_files = [\n",
    "        \"components/__init__.py\",\n",
    "        \"pipelines/__init__.py\",\n",
    "        \"webhook/__init__.py\"\n",
    "    ]\n",
    "    \n",
    "    for init_file in init_files:\n",
    "        Path(init_file).touch(exist_ok=True)\n",
    "        print(f\"  ‚úÖ {init_file}\")\n",
    "    \n",
    "    print(\"\\nüìã Estructura creada:\")\n",
    "    print(\"  components/          # Pipeline components\")\n",
    "    print(\"  pipelines/           # Pipeline definitions\") \n",
    "    print(\"  webhook/             # Webhook handler\")\n",
    "    print(\"  config/              # Configuration files\")\n",
    "    print(\"  deploy/              # Deployment manifests\")\n",
    "    print(\"  tests/               # Integration tests\")\n",
    "    \n",
    "    return directories\n",
    "\n",
    "# Crear estructura\n",
    "created_dirs = create_project_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcc2758-7cda-477a-98ed-ada741208d2f",
   "metadata": {},
   "source": [
    "## üîß Component 1: Text Processing Components\n",
    "\n",
    "Creamos el archivo con los components de procesamiento de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f466e8b-d3ee-4f53-a4f4-c5c0d4477e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Archivo creado: components/text_processing.py\n",
      "üìã Components incluidos:\n",
      "  - extract_text_component\n",
      "  - chunk_text_component\n"
     ]
    }
   ],
   "source": [
    "def create_text_processing_components():\n",
    "    \"\"\"Crear archivo components/text_processing.py\"\"\"\n",
    "    \n",
    "    text_processing_content = '''\"\"\"\n",
    "Text Processing Components para RAG Pipeline\n",
    "Incluye: extract_text_component y chunk_text_component\n",
    "\"\"\"\n",
    "\n",
    "from kfp.dsl import component, Input, Output, Dataset\n",
    "\n",
    "@component(\n",
    "    base_image=\"pytorch/pytorch:2.0.1-cuda11.7-cudnn8-devel\",\n",
    "    packages_to_install=[\n",
    "        \"PyPDF2==3.0.1\",\n",
    "        \"python-docx==0.8.11\", \n",
    "        \"minio==7.1.17\",\n",
    "        \"chardet==5.2.0\"\n",
    "    ]\n",
    ")\n",
    "def extract_text_component(\n",
    "    bucket_name: str,\n",
    "    object_key: str,\n",
    "    minio_endpoint: str,\n",
    "    minio_access_key: str,\n",
    "    minio_secret_key: str,\n",
    "    extracted_text: Output[Dataset],\n",
    "    metadata: Output[Dataset]\n",
    "):\n",
    "    \"\"\"\n",
    "    Extrae texto de documentos almacenados en MinIO.\n",
    "    \n",
    "    Args:\n",
    "        bucket_name: Nombre del bucket en MinIO\n",
    "        object_key: Path del archivo en el bucket\n",
    "        minio_endpoint: Endpoint de MinIO\n",
    "        minio_access_key: Access key de MinIO\n",
    "        minio_secret_key: Secret key de MinIO\n",
    "        extracted_text: Output dataset con el texto extra√≠do\n",
    "        metadata: Output dataset con metadata del documento\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import json\n",
    "    import tempfile\n",
    "    from pathlib import Path\n",
    "    from datetime import datetime\n",
    "    from minio import Minio\n",
    "    import PyPDF2\n",
    "    from docx import Document\n",
    "    import chardet\n",
    "    \n",
    "    # Conectar a MinIO\n",
    "    minio_client = Minio(\n",
    "        minio_endpoint,\n",
    "        access_key=minio_access_key,\n",
    "        secret_key=minio_secret_key,\n",
    "        secure=False\n",
    "    )\n",
    "    \n",
    "    # Crear directorio temporal\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        local_file_path = os.path.join(temp_dir, object_key.split('/')[-1])\n",
    "        \n",
    "        # Descargar archivo desde MinIO\n",
    "        try:\n",
    "            minio_client.fget_object(bucket_name, object_key, local_file_path)\n",
    "            print(f\"‚úÖ Archivo descargado: {local_file_path}\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error descargando archivo: {str(e)}\")\n",
    "        \n",
    "        # Detectar tipo de archivo\n",
    "        file_extension = Path(local_file_path).suffix.lower()\n",
    "        file_size = os.path.getsize(local_file_path)\n",
    "        \n",
    "        # Extraer texto seg√∫n el tipo de archivo\n",
    "        extracted_content = \"\"\n",
    "        \n",
    "        if file_extension == '.pdf':\n",
    "            try:\n",
    "                with open(local_file_path, 'rb') as file:\n",
    "                    pdf_reader = PyPDF2.PdfReader(file)\n",
    "                    for page_num in range(len(pdf_reader.pages)):\n",
    "                        page = pdf_reader.pages[page_num]\n",
    "                        extracted_content += page.extract_text() + \"\\\\n\"\n",
    "                print(f\"‚úÖ PDF procesado: {len(pdf_reader.pages)} p√°ginas\")\n",
    "            except Exception as e:\n",
    "                raise Exception(f\"Error procesando PDF: {str(e)}\")\n",
    "                \n",
    "        elif file_extension == '.docx':\n",
    "            try:\n",
    "                doc = Document(local_file_path)\n",
    "                for paragraph in doc.paragraphs:\n",
    "                    extracted_content += paragraph.text + \"\\\\n\"\n",
    "                print(f\"‚úÖ DOCX procesado: {len(doc.paragraphs)} p√°rrafos\")\n",
    "            except Exception as e:\n",
    "                raise Exception(f\"Error procesando DOCX: {str(e)}\")\n",
    "                \n",
    "        elif file_extension in ['.txt', '.md']:\n",
    "            try:\n",
    "                # Detectar encoding\n",
    "                with open(local_file_path, 'rb') as file:\n",
    "                    raw_data = file.read()\n",
    "                    encoding = chardet.detect(raw_data)['encoding']\n",
    "                \n",
    "                # Leer con encoding detectado\n",
    "                with open(local_file_path, 'r', encoding=encoding) as file:\n",
    "                    extracted_content = file.read()\n",
    "                print(f\"‚úÖ TXT procesado con encoding: {encoding}\")\n",
    "            except Exception as e:\n",
    "                raise Exception(f\"Error procesando TXT: {str(e)}\")\n",
    "                \n",
    "        else:\n",
    "            raise Exception(f\"Tipo de archivo no soportado: {file_extension}\")\n",
    "        \n",
    "        # Validar que se extrajo contenido\n",
    "        if not extracted_content.strip():\n",
    "            raise Exception(\"No se pudo extraer texto del documento\")\n",
    "        \n",
    "        # Preparar metadata\n",
    "        document_metadata = {\n",
    "            \"source_file\": object_key,\n",
    "            \"file_type\": file_extension,\n",
    "            \"file_size\": file_size,\n",
    "            \"processed_at\": datetime.now().isoformat(),\n",
    "            \"char_count\": len(extracted_content),\n",
    "            \"word_count\": len(extracted_content.split()),\n",
    "            \"bucket_name\": bucket_name\n",
    "        }\n",
    "        \n",
    "        # Guardar outputs\n",
    "        with open(extracted_text.path, 'w', encoding='utf-8') as f:\n",
    "            f.write(extracted_content)\n",
    "            \n",
    "        with open(metadata.path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(document_metadata, f, indent=2)\n",
    "        \n",
    "        print(f\"‚úÖ Texto extra√≠do: {len(extracted_content)} caracteres\")\n",
    "\n",
    "\n",
    "@component(\n",
    "    base_image=\"pytorch/pytorch:2.0.1-cuda11.7-cudnn8-devel\",\n",
    "    packages_to_install=[\n",
    "        \"tiktoken==0.5.1\",\n",
    "        \"langchain==0.0.350\"\n",
    "    ]\n",
    ")\n",
    "def chunk_text_component(\n",
    "    extracted_text: Input[Dataset],\n",
    "    metadata: Input[Dataset],\n",
    "    chunk_size: int,\n",
    "    chunk_overlap: int,\n",
    "    chunks: Output[Dataset]\n",
    "):\n",
    "    \"\"\"\n",
    "    Divide el texto en chunks con overlap para processing √≥ptimo.\n",
    "    \n",
    "    Args:\n",
    "        extracted_text: Input dataset con texto extra√≠do\n",
    "        metadata: Input dataset con metadata del documento\n",
    "        chunk_size: Tama√±o m√°ximo de cada chunk (en tokens)\n",
    "        chunk_overlap: Overlap entre chunks (en tokens)\n",
    "        chunks: Output dataset con chunks procesados\n",
    "    \"\"\"\n",
    "    import json\n",
    "    import tiktoken\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    \n",
    "    # Leer input data\n",
    "    with open(extracted_text.path, 'r', encoding='utf-8') as f:\n",
    "        text_content = f.read()\n",
    "    \n",
    "    with open(metadata.path, 'r', encoding='utf-8') as f:\n",
    "        doc_metadata = json.load(f)\n",
    "    \n",
    "    # Configurar tokenizer\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    def count_tokens(text: str) -> int:\n",
    "        return len(encoding.encode(text))\n",
    "    \n",
    "    # Configurar text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size * 4,  # Aproximaci√≥n: 1 token ‚âà 4 caracteres\n",
    "        chunk_overlap=chunk_overlap * 4,\n",
    "        length_function=len,\n",
    "        separators=[\"\\\\n\\\\n\", \"\\\\n\", \". \", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    # Dividir texto en chunks\n",
    "    text_chunks = text_splitter.split_text(text_content)\n",
    "    print(f\"‚úÖ Texto dividido en {len(text_chunks)} chunks\")\n",
    "    \n",
    "    # Procesar cada chunk\n",
    "    processed_chunks = []\n",
    "    \n",
    "    for i, chunk_text in enumerate(text_chunks):\n",
    "        token_count = count_tokens(chunk_text)\n",
    "        \n",
    "        chunk_metadata = {\n",
    "            \"chunk_id\": f\"{doc_metadata['source_file']}_chunk_{i:04d}\",\n",
    "            \"chunk_index\": i,\n",
    "            \"total_chunks\": len(text_chunks),\n",
    "            \"text\": chunk_text.strip(),\n",
    "            \"token_count\": token_count,\n",
    "            \"char_count\": len(chunk_text),\n",
    "            \"word_count\": len(chunk_text.split()),\n",
    "            \"source_document\": doc_metadata['source_file'],\n",
    "            \"file_type\": doc_metadata['file_type'],\n",
    "            \"processed_at\": doc_metadata['processed_at']\n",
    "        }\n",
    "        \n",
    "        processed_chunks.append(chunk_metadata)\n",
    "    \n",
    "    # Filtrar chunks muy peque√±os\n",
    "    processed_chunks = [chunk for chunk in processed_chunks if chunk['token_count'] >= 10]\n",
    "    \n",
    "    print(f\"‚úÖ Chunks procesados: {len(processed_chunks)}\")\n",
    "    \n",
    "    # Guardar chunks\n",
    "    with open(chunks.path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(processed_chunks, f, indent=2, ensure_ascii=False)\n",
    "'''\n",
    "    \n",
    "    # Escribir archivo\n",
    "    with open('components/text_processing.py', 'w', encoding='utf-8') as f:\n",
    "        f.write(text_processing_content)\n",
    "    \n",
    "    print(\"‚úÖ Archivo creado: components/text_processing.py\")\n",
    "    print(\"üìã Components incluidos:\")\n",
    "    print(\"  - extract_text_component\")\n",
    "    print(\"  - chunk_text_component\")\n",
    "\n",
    "# Crear archivo de text processing\n",
    "create_text_processing_components()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6450cc6b-4952-4409-b132-a621330ce9b4",
   "metadata": {},
   "source": [
    "## üéØ Component 2: Vector Processing Components\n",
    "\n",
    "Creamos el archivo con los components de embeddings e indexaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b4d7a4a-222c-4422-8326-82a9a518f9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Archivo creado: components/vector_processing.py\n",
      "üìã Components incluidos:\n",
      "  - generate_embeddings_component\n",
      "  - index_elasticsearch_component\n"
     ]
    }
   ],
   "source": [
    "def create_vector_processing_components():\n",
    "    \"\"\"Crear archivo components/vector_processing.py\"\"\"\n",
    "    \n",
    "    vector_processing_content = '''\"\"\"\n",
    "Vector Processing Components para RAG Pipeline\n",
    "Incluye: generate_embeddings_component y index_elasticsearch_component\n",
    "\"\"\"\n",
    "\n",
    "from kfp.dsl import component, Input, Output, Dataset\n",
    "\n",
    "@component(\n",
    "    base_image=\"pytorch/pytorch:2.0.1-cuda11.7-cudnn8-devel\",\n",
    "    packages_to_install=[\n",
    "        \"sentence-transformers==2.2.2\",\n",
    "        \"numpy==1.24.3\"\n",
    "    ]\n",
    ")\n",
    "def generate_embeddings_component(\n",
    "    chunks: Input[Dataset],\n",
    "    model_name: str,\n",
    "    embeddings: Output[Dataset]\n",
    "):\n",
    "    \"\"\"\n",
    "    Genera embeddings vectoriales para los chunks de texto.\n",
    "    \n",
    "    Args:\n",
    "        chunks: Input dataset con chunks de texto\n",
    "        model_name: Nombre del modelo de embeddings\n",
    "        embeddings: Output dataset con embeddings generados\n",
    "    \"\"\"\n",
    "    import json\n",
    "    import numpy as np\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    import torch\n",
    "    \n",
    "    # Verificar si hay GPU disponible\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"üñ•Ô∏è Usando device: {device}\")\n",
    "    \n",
    "    # Cargar modelo de embeddings\n",
    "    print(f\"üì• Cargando modelo: {model_name}\")\n",
    "    model = SentenceTransformer(model_name, device=device)\n",
    "    \n",
    "    # Leer chunks\n",
    "    with open(chunks.path, 'r', encoding='utf-8') as f:\n",
    "        chunk_data = json.load(f)\n",
    "    \n",
    "    print(f\"üìù Procesando {len(chunk_data)} chunks\")\n",
    "    \n",
    "    # Extraer textos para embedding\n",
    "    texts = [chunk['text'] for chunk in chunk_data]\n",
    "    \n",
    "    # Generar embeddings en batches para eficiencia\n",
    "    batch_size = 32\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        batch_embeddings = model.encode(\n",
    "            batch_texts,\n",
    "            convert_to_numpy=True,\n",
    "            show_progress_bar=True if i == 0 else False,\n",
    "            normalize_embeddings=True\n",
    "        )\n",
    "        all_embeddings.extend(batch_embeddings)\n",
    "        \n",
    "        if i % (batch_size * 5) == 0:\n",
    "            print(f\"  Procesado: {min(i + batch_size, len(texts))}/{len(texts)} chunks\")\n",
    "    \n",
    "    print(f\"‚úÖ Embeddings generados: {len(all_embeddings)} vectores de {len(all_embeddings[0])} dimensiones\")\n",
    "    \n",
    "    # Combinar chunks con sus embeddings\n",
    "    enriched_chunks = []\n",
    "    for chunk, embedding in zip(chunk_data, all_embeddings):\n",
    "        enriched_chunk = chunk.copy()\n",
    "        enriched_chunk['embedding'] = embedding.tolist()\n",
    "        enriched_chunk['embedding_dim'] = len(embedding)\n",
    "        enriched_chunk['embedding_model'] = model_name\n",
    "        enriched_chunks.append(enriched_chunk)\n",
    "    \n",
    "    # Guardar chunks enriquecidos con embeddings\n",
    "    with open(embeddings.path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(enriched_chunks, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"‚úÖ Chunks enriquecidos guardados\")\n",
    "\n",
    "\n",
    "@component(\n",
    "    base_image=\"pytorch/pytorch:2.0.1-cuda11.7-cudnn8-devel\",\n",
    "    packages_to_install=[\n",
    "        \"elasticsearch==8.11.0\"\n",
    "    ]\n",
    ")\n",
    "def index_elasticsearch_component(\n",
    "    enriched_chunks: Input[Dataset],\n",
    "    es_endpoint: str,\n",
    "    es_index: str,\n",
    "    index_status: Output[Dataset]\n",
    "):\n",
    "    \"\"\"\n",
    "    Indexa chunks enriquecidos en ElasticSearch.\n",
    "    \n",
    "    Args:\n",
    "        enriched_chunks: Input dataset con chunks y embeddings\n",
    "        es_endpoint: Endpoint de ElasticSearch\n",
    "        es_index: Nombre del √≠ndice\n",
    "        index_status: Output dataset con status de indexaci√≥n\n",
    "    \"\"\"\n",
    "    import json\n",
    "    from datetime import datetime\n",
    "    from elasticsearch import Elasticsearch\n",
    "    from elasticsearch.helpers import bulk\n",
    "    \n",
    "    # Conectar a ElasticSearch\n",
    "    try:\n",
    "        es = Elasticsearch([es_endpoint], verify_certs=False)\n",
    "        \n",
    "        if not es.ping():\n",
    "            raise Exception(\"No se puede conectar a ElasticSearch\")\n",
    "        \n",
    "        print(f\"‚úÖ Conectado a ElasticSearch: {es_endpoint}\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error conectando a ElasticSearch: {str(e)}\")\n",
    "    \n",
    "    # Leer chunks enriquecidos\n",
    "    with open(enriched_chunks.path, 'r', encoding='utf-8') as f:\n",
    "        chunks_data = json.load(f)\n",
    "    \n",
    "    print(f\"üìù Indexando {len(chunks_data)} chunks en √≠ndice: {es_index}\")\n",
    "    \n",
    "    # Definir mapping del √≠ndice\n",
    "    index_mapping = {\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"chunk_id\": {\"type\": \"keyword\"},\n",
    "                \"text\": {\n",
    "                    \"type\": \"text\",\n",
    "                    \"analyzer\": \"standard\"\n",
    "                },\n",
    "                \"embedding\": {\n",
    "                    \"type\": \"dense_vector\",\n",
    "                    \"dims\": chunks_data[0]['embedding_dim'] if chunks_data else 384,\n",
    "                    \"index\": True,\n",
    "                    \"similarity\": \"cosine\"\n",
    "                },\n",
    "                \"source_document\": {\"type\": \"keyword\"},\n",
    "                \"file_type\": {\"type\": \"keyword\"},\n",
    "                \"chunk_index\": {\"type\": \"integer\"},\n",
    "                \"total_chunks\": {\"type\": \"integer\"},\n",
    "                \"token_count\": {\"type\": \"integer\"},\n",
    "                \"char_count\": {\"type\": \"integer\"},\n",
    "                \"word_count\": {\"type\": \"integer\"},\n",
    "                \"processed_at\": {\"type\": \"date\"},\n",
    "                \"indexed_at\": {\"type\": \"date\"}\n",
    "            }\n",
    "        },\n",
    "        \"settings\": {\n",
    "            \"number_of_shards\": 1,\n",
    "            \"number_of_replicas\": 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Crear √≠ndice si no existe\n",
    "    if not es.indices.exists(index=es_index):\n",
    "        es.indices.create(index=es_index, body=index_mapping)\n",
    "        print(f\"‚úÖ √çndice creado: {es_index}\")\n",
    "    else:\n",
    "        print(f\"‚ÑπÔ∏è √çndice ya existe: {es_index}\")\n",
    "    \n",
    "    # Preparar documentos para bulk indexing\n",
    "    documents = []\n",
    "    for chunk in chunks_data:\n",
    "        doc = {\n",
    "            \"_index\": es_index,\n",
    "            \"_id\": chunk['chunk_id'],\n",
    "            \"_source\": {\n",
    "                **chunk,\n",
    "                \"indexed_at\": datetime.now().isoformat()\n",
    "            }\n",
    "        }\n",
    "        documents.append(doc)\n",
    "    \n",
    "    # Indexar en batches\n",
    "    try:\n",
    "        success_count, failed_items = bulk(\n",
    "            es,\n",
    "            documents,\n",
    "            chunk_size=100,\n",
    "            request_timeout=300\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Indexaci√≥n completada:\")\n",
    "        print(f\"  Documentos exitosos: {success_count}\")\n",
    "        print(f\"  Documentos fallidos: {len(failed_items) if failed_items else 0}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error en bulk indexing: {str(e)}\")\n",
    "    \n",
    "    # Refresh del √≠ndice\n",
    "    es.indices.refresh(index=es_index)\n",
    "    \n",
    "    # Verificar indexaci√≥n\n",
    "    doc_count = es.count(index=es_index)['count']\n",
    "    print(f\"‚úÖ Total documentos en √≠ndice: {doc_count}\")\n",
    "    \n",
    "    # Preparar status de indexaci√≥n\n",
    "    indexing_status = {\n",
    "        \"index_name\": es_index,\n",
    "        \"total_chunks\": len(chunks_data),\n",
    "        \"indexed_chunks\": success_count,\n",
    "        \"failed_chunks\": len(failed_items) if failed_items else 0,\n",
    "        \"total_documents_in_index\": doc_count,\n",
    "        \"indexed_at\": datetime.now().isoformat(),\n",
    "        \"success\": len(failed_items) == 0 if failed_items else True\n",
    "    }\n",
    "    \n",
    "    # Guardar status\n",
    "    with open(index_status.path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(indexing_status, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Status de indexaci√≥n guardado\")\n",
    "'''\n",
    "    \n",
    "    # Escribir archivo\n",
    "    with open('components/vector_processing.py', 'w', encoding='utf-8') as f:\n",
    "        f.write(vector_processing_content)\n",
    "    \n",
    "    print(\"‚úÖ Archivo creado: components/vector_processing.py\")\n",
    "    print(\"üìã Components incluidos:\")\n",
    "    print(\"  - generate_embeddings_component\")\n",
    "    print(\"  - index_elasticsearch_component\")\n",
    "\n",
    "# Crear archivo de vector processing\n",
    "create_vector_processing_components()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f06750d-1104-45bb-a116-e0be73fa168a",
   "metadata": {},
   "source": [
    "## üîó Pipeline Definition: RAG Pipeline Principal\n",
    "\n",
    "Creamos el pipeline principal que orquesta todos los components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69860c84-8bca-4719-9cee-1520f0ae65c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Archivo creado: pipelines/rag_pipeline.py\n",
      "üìã Pipeline functions incluidas:\n",
      "  - rag_document_pipeline: Pipeline principal\n",
      "  - rag_batch_pipeline: Pipeline batch (placeholder)\n"
     ]
    }
   ],
   "source": [
    "def create_rag_pipeline():\n",
    "    \"\"\"Crear archivo pipelines/rag_pipeline.py\"\"\"\n",
    "    \n",
    "    rag_pipeline_content = '''\"\"\"\n",
    "RAG Pipeline Principal - OpenShift AI Data Science Pipeline\n",
    "Orquesta todos los components para procesamiento completo de documentos\n",
    "\"\"\"\n",
    "\n",
    "from kfp import dsl\n",
    "from kfp.dsl import pipeline\n",
    "\n",
    "# Importar components\n",
    "from components.text_processing import extract_text_component, chunk_text_component\n",
    "from components.vector_processing import generate_embeddings_component, index_elasticsearch_component\n",
    "\n",
    "@pipeline(\n",
    "    name=\"rag-document-processing-v1\",\n",
    "    description=\"Pipeline completo de procesamiento de documentos RAG para OpenShift AI\"\n",
    ")\n",
    "def rag_document_pipeline(\n",
    "    bucket_name: str = \"raw-documents\",\n",
    "    object_key: str = \"\",\n",
    "    minio_endpoint: str = \"minio-rag:9000\",\n",
    "    minio_access_key: str = \"ragadmin\",\n",
    "    minio_secret_key: str = \"RAGSecurePass123!\",\n",
    "    es_endpoint: str = \"elasticsearch:9200\",\n",
    "    es_index: str = \"rag-documents\",\n",
    "    chunk_size: int = 512,\n",
    "    chunk_overlap: int = 50,\n",
    "    embedding_model: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Pipeline completo de procesamiento de documentos RAG.\n",
    "    \n",
    "    Args:\n",
    "        bucket_name: Nombre del bucket en MinIO\n",
    "        object_key: Path del archivo a procesar\n",
    "        minio_endpoint: Endpoint de MinIO\n",
    "        minio_access_key: Access key de MinIO\n",
    "        minio_secret_key: Secret key de MinIO\n",
    "        es_endpoint: Endpoint de ElasticSearch\n",
    "        es_index: Nombre del √≠ndice en ElasticSearch\n",
    "        chunk_size: Tama√±o de chunks en tokens\n",
    "        chunk_overlap: Overlap entre chunks en tokens\n",
    "        embedding_model: Modelo para generar embeddings\n",
    "    \n",
    "    Returns:\n",
    "        Status de indexaci√≥n final\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Extract text from document\n",
    "    extract_task = extract_text_component(\n",
    "        bucket_name=bucket_name,\n",
    "        object_key=object_key,\n",
    "        minio_endpoint=minio_endpoint,\n",
    "        minio_access_key=minio_access_key,\n",
    "        minio_secret_key=minio_secret_key\n",
    "    )\n",
    "    extract_task.set_display_name(\"üìÑ Extract Text\")\n",
    "    extract_task.set_cpu_limit(\"500m\")\n",
    "    extract_task.set_memory_limit(\"1Gi\")\n",
    "    extract_task.set_retry(3)\n",
    "    \n",
    "    # Step 2: Chunk the extracted text\n",
    "    chunk_task = chunk_text_component(\n",
    "        extracted_text=extract_task.outputs['extracted_text'],\n",
    "        metadata=extract_task.outputs['metadata'],\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    ).after(extract_task)\n",
    "    chunk_task.set_display_name(\"üß© Chunk Text\")\n",
    "    chunk_task.set_cpu_limit(\"500m\")\n",
    "    chunk_task.set_memory_limit(\"1Gi\")\n",
    "    chunk_task.set_retry(3)\n",
    "    \n",
    "    # Step 3: Generate embeddings\n",
    "    embedding_task = generate_embeddings_component(\n",
    "        chunks=chunk_task.outputs['chunks'],\n",
    "        model_name=embedding_model\n",
    "    ).after(chunk_task)\n",
    "    embedding_task.set_display_name(\"üéØ Generate Embeddings\")\n",
    "    embedding_task.set_cpu_limit(\"1000m\")\n",
    "    embedding_task.set_memory_limit(\"4Gi\")\n",
    "    embedding_task.set_retry(2)\n",
    "    # embedding_task.set_gpu_limit(\"1\")  # Uncomment si hay GPUs disponibles\n",
    "    \n",
    "    # Step 4: Index in ElasticSearch\n",
    "    index_task = index_elasticsearch_component(\n",
    "        enriched_chunks=embedding_task.outputs['embeddings'],\n",
    "        es_endpoint=es_endpoint,\n",
    "        es_index=es_index\n",
    "    ).after(embedding_task)\n",
    "    index_task.set_display_name(\"üîç Index ElasticSearch\")\n",
    "    index_task.set_cpu_limit(\"500m\")\n",
    "    index_task.set_memory_limit(\"2Gi\")\n",
    "    index_task.set_retry(3)\n",
    "    \n",
    "    # Return final status\n",
    "    return index_task.outputs['index_status']\n",
    "\n",
    "\n",
    "# Pipeline alternativo para batch processing\n",
    "@pipeline(\n",
    "    name=\"rag-batch-processing-v1\", \n",
    "    description=\"Pipeline para procesamiento batch de m√∫ltiples documentos\"\n",
    ")\n",
    "def rag_batch_pipeline(\n",
    "    bucket_name: str = \"raw-documents\",\n",
    "    file_pattern: str = \"*.pdf\",\n",
    "    es_endpoint: str = \"elasticsearch:9200\",\n",
    "    es_index: str = \"rag-documents\",\n",
    "    chunk_size: int = 512,\n",
    "    chunk_overlap: int = 50,\n",
    "    embedding_model: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    batch_size: int = 10\n",
    "):\n",
    "    \"\"\"\n",
    "    Pipeline para procesamiento batch de m√∫ltiples documentos.\n",
    "    √ötil para procesamiento inicial de grandes vol√∫menes.\n",
    "    \"\"\"\n",
    "    # TODO: Implementar en futuras versiones\n",
    "    # - Listar archivos en bucket por patr√≥n\n",
    "    # - Procesar en batches paralelos\n",
    "    # - Consolidar resultados\n",
    "    pass\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Para testing local del pipeline definition\n",
    "    print(\"‚úÖ RAG Pipeline definido correctamente\")\n",
    "    print(\"üìã Pipeline functions disponibles:\")\n",
    "    print(\"  - rag_document_pipeline: Procesamiento individual\")\n",
    "    print(\"  - rag_batch_pipeline: Procesamiento batch (TODO)\")\n",
    "'''\n",
    "    \n",
    "    # Escribir archivo\n",
    "    with open('pipelines/rag_pipeline.py', 'w', encoding='utf-8') as f:\n",
    "        f.write(rag_pipeline_content)\n",
    "    \n",
    "    print(\"‚úÖ Archivo creado: pipelines/rag_pipeline.py\")\n",
    "    print(\"üìã Pipeline functions incluidas:\")\n",
    "    print(\"  - rag_document_pipeline: Pipeline principal\")\n",
    "    print(\"  - rag_batch_pipeline: Pipeline batch (placeholder)\")\n",
    "\n",
    "# Crear archivo del pipeline\n",
    "create_rag_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a05024-f639-44fc-8a00-4e836ffcc590",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration Files\n",
    "\n",
    "Creamos los archivos de configuraci√≥n necesarios para el deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a15743a-eb4d-47ac-8c05-b496dc181cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Creado: config/pipeline_config.yaml\n",
      "‚úÖ Creado: config/secrets.yaml\n",
      "‚úÖ Creado: requirements.txt\n",
      "‚úÖ Creado: config/env.template\n",
      "\n",
      "üìã Archivos de configuraci√≥n creados:\n",
      "  config/pipeline_config.yaml - Configuraci√≥n del pipeline\n",
      "  config/secrets.yaml - Template de secrets K8s\n",
      "  requirements.txt - Dependencias Python\n",
      "  config/env.template - Variables de ambiente\n"
     ]
    }
   ],
   "source": [
    "def create_configuration_files():\n",
    "    \"\"\"Crear archivos de configuraci√≥n del proyecto\"\"\"\n",
    "    \n",
    "    # 1. Pipeline Configuration\n",
    "    pipeline_config = {\n",
    "        \"pipeline\": {\n",
    "            \"name\": \"rag-document-processing-v1\",\n",
    "            \"version\": \"1.0.0\",\n",
    "            \"description\": \"RAG Document Processing Pipeline para OpenShift AI\",\n",
    "            \"author\": \"RAG Team\",\n",
    "            \"created\": datetime.now().isoformat()\n",
    "        },\n",
    "        \"components\": {\n",
    "            \"base_image\": \"pytorch/pytorch:2.0.1-cuda11.7-cudnn8-devel\",\n",
    "            \"extract_text\": {\n",
    "                \"cpu_limit\": \"500m\",\n",
    "                \"memory_limit\": \"1Gi\",\n",
    "                \"retry_limit\": 3\n",
    "            },\n",
    "            \"chunk_text\": {\n",
    "                \"cpu_limit\": \"500m\", \n",
    "                \"memory_limit\": \"1Gi\",\n",
    "                \"retry_limit\": 3\n",
    "            },\n",
    "            \"generate_embeddings\": {\n",
    "                \"cpu_limit\": \"1000m\",\n",
    "                \"memory_limit\": \"4Gi\", \n",
    "                \"retry_limit\": 2,\n",
    "                \"gpu_limit\": \"0\"  # Set to \"1\" si hay GPU disponible\n",
    "            },\n",
    "            \"index_elasticsearch\": {\n",
    "                \"cpu_limit\": \"500m\",\n",
    "                \"memory_limit\": \"2Gi\",\n",
    "                \"retry_limit\": 3\n",
    "            }\n",
    "        },\n",
    "        \"default_parameters\": {\n",
    "            \"chunk_size\": 512,\n",
    "            \"chunk_overlap\": 50,\n",
    "            \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "            \"es_index\": \"rag-documents\"\n",
    "        },\n",
    "        \"storage\": {\n",
    "            \"minio\": {\n",
    "                \"endpoint\": \"minio-rag:9000\",\n",
    "                \"bucket_raw\": \"raw-documents\",\n",
    "                \"bucket_processed\": \"processed-documents\",\n",
    "                \"bucket_failed\": \"failed-documents\"\n",
    "            },\n",
    "            \"elasticsearch\": {\n",
    "                \"endpoint\": \"elasticsearch:9200\",\n",
    "                \"index_prefix\": \"rag-\",\n",
    "                \"replicas\": 0,\n",
    "                \"shards\": 1\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open('config/pipeline_config.yaml', 'w') as f:\n",
    "        yaml.dump(pipeline_config, f, indent=2, default_flow_style=False)\n",
    "    \n",
    "    print(\"‚úÖ Creado: config/pipeline_config.yaml\")\n",
    "    \n",
    "    # 2. Secrets Template\n",
    "    secrets_template = {\n",
    "        \"apiVersion\": \"v1\",\n",
    "        \"kind\": \"Secret\",\n",
    "        \"metadata\": {\n",
    "            \"name\": \"rag-pipeline-secrets\",\n",
    "            \"namespace\": \"rag-openshift-ai\"\n",
    "        },\n",
    "        \"type\": \"Opaque\",\n",
    "        \"stringData\": {\n",
    "            \"minio-access-key\": \"minio\",\n",
    "            \"minio-secret-key\": \"minio123\",\n",
    "            \"elasticsearch-username\": \"\",\n",
    "            \"elasticsearch-password\": \"\",\n",
    "            \"pipeline-webhook-token\": \"rag-pipeline-token-change-me\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open('config/secrets.yaml', 'w') as f:\n",
    "        yaml.dump(secrets_template, f, indent=2, default_flow_style=False)\n",
    "    \n",
    "    print(\"‚úÖ Creado: config/secrets.yaml\")\n",
    "    \n",
    "    # 3. Requirements file\n",
    "    requirements_content = '''# RAG Pipeline Requirements\n",
    "# Core KFP\n",
    "kfp>=2.0.0\n",
    "\n",
    "# Document Processing\n",
    "PyPDF2==3.0.1\n",
    "python-docx==0.8.11\n",
    "chardet==5.2.0\n",
    "\n",
    "# Text Processing\n",
    "tiktoken==0.5.1\n",
    "langchain==0.0.350\n",
    "\n",
    "# ML/Embeddings\n",
    "sentence-transformers==2.2.2\n",
    "torch>=2.0.1\n",
    "numpy==1.24.3\n",
    "\n",
    "# Storage/DB\n",
    "minio==7.1.17\n",
    "elasticsearch==8.11.0\n",
    "\n",
    "# Web/API (for webhook)\n",
    "flask==2.3.3\n",
    "requests==2.31.0\n",
    "\n",
    "# Utilities\n",
    "pyyaml==6.0.1\n",
    "python-dateutil==2.8.2\n",
    "'''\n",
    "    \n",
    "    with open('requirements.txt', 'w') as f:\n",
    "        f.write(requirements_content)\n",
    "    \n",
    "    print(\"‚úÖ Creado: requirements.txt\")\n",
    "    \n",
    "    # 4. Environment variables template\n",
    "    env_template = '''# RAG Pipeline Environment Variables\n",
    "# Copy to .env and modify as needed\n",
    "\n",
    "# MinIO Configuration\n",
    "MINIO_ENDPOINT=minio:9000\n",
    "MINIO_ACCESS_KEY=minio\n",
    "MINIO_SECRET_KEY=minio123\n",
    "MINIO_SECURE=false\n",
    "\n",
    "# ElasticSearch Configuration  \n",
    "ES_ENDPOINT=elasticsearch:9200\n",
    "ES_INDEX=rag-documents\n",
    "ES_USERNAME=\n",
    "ES_PASSWORD=\n",
    "\n",
    "# Pipeline Configuration\n",
    "CHUNK_SIZE=512\n",
    "CHUNK_OVERLAP=50\n",
    "EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2\n",
    "\n",
    "# OpenShift AI Pipeline\n",
    "KFP_HOST=http://ml-pipeline:8888\n",
    "PIPELINE_NAMESPACE=rag-openshift-ai\n",
    "\n",
    "# Webhook Configuration\n",
    "WEBHOOK_PORT=8080\n",
    "WEBHOOK_TOKEN=rag-pipeline-token-change-me\n",
    "'''\n",
    "    \n",
    "    with open('config/env.template', 'w') as f:\n",
    "        f.write(env_template)\n",
    "    \n",
    "    print(\"‚úÖ Creado: config/env.template\")\n",
    "    \n",
    "    print(\"\\nüìã Archivos de configuraci√≥n creados:\")\n",
    "    print(\"  config/pipeline_config.yaml - Configuraci√≥n del pipeline\")\n",
    "    print(\"  config/secrets.yaml - Template de secrets K8s\")\n",
    "    print(\"  requirements.txt - Dependencias Python\")  \n",
    "    print(\"  config/env.template - Variables de ambiente\")\n",
    "\n",
    "# Crear archivos de configuraci√≥n\n",
    "create_configuration_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e61ca4c-acf0-4703-a62d-e6114d35f7c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
