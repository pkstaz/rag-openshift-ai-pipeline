{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9a70ea8-58ca-4c8f-a75f-1ed7d2cb008d",
   "metadata": {},
   "source": [
    "## ü™£ Configuraci√≥n de MinIO - Crear Buckets\n",
    "\n",
    "Configuramos MinIO y creamos los buckets necesarios para el pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d8d6632-7140-42cc-9f7b-a26f49a9c55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Configuraci√≥n completa de MinIO para OpenShift AI DSP\n",
      "============================================================\n",
      "üîß Configurando MinIO y creando buckets completos...\n",
      "üîó Conectando a MinIO: minio:9000\n",
      "‚úÖ Conectado a MinIO exitosamente\n",
      "üìä Buckets existentes: 5\n",
      "  - failed-documents (creado: 2025-07-01 17:40:26.746000+00:00)\n",
      "  - processed-documents (creado: 2025-07-01 17:40:26.739000+00:00)\n",
      "  - rag-documents (creado: 2025-07-01 16:45:51.180000+00:00)\n",
      "  - raw-documents (creado: 2025-07-01 17:40:26.732000+00:00)\n",
      "  - test-datasets (creado: 2025-07-01 17:40:26.753000+00:00)\n",
      "\n",
      "ü™£ Creando buckets necesarios...\n",
      "  ‚úÖ Bucket creado: pipeline\n",
      "    üìù Bucket requerido por OpenShift AI Data Science Pipelines\n",
      "  ‚ÑπÔ∏è Bucket ya existe: raw-documents\n",
      "    üìù Documentos originales subidos por usuarios\n",
      "  ‚ÑπÔ∏è Bucket ya existe: processed-documents\n",
      "    üìù Documentos procesados exitosamente\n",
      "  ‚ÑπÔ∏è Bucket ya existe: failed-documents\n",
      "    üìù Documentos que fallaron en procesamiento\n",
      "  ‚ÑπÔ∏è Bucket ya existe: test-datasets\n",
      "    üìù Datasets de prueba para desarrollo\n",
      "\n",
      "üìä Resumen de buckets:\n",
      "  ‚úÖ Creados: 1\n",
      "  ‚ÑπÔ∏è Ya exist√≠an: 4\n",
      "\n",
      "üóÇÔ∏è Buckets disponibles:\n",
      "  - failed-documents\n",
      "  - pipeline\n",
      "  - processed-documents\n",
      "  - rag-documents\n",
      "  - raw-documents\n",
      "  - test-datasets\n",
      "\n",
      "‚úÖ Bucket 'pipeline' configurado correctamente para OpenShift AI DSP\n",
      "\n",
      "üîê Configurando pol√≠ticas de buckets...\n",
      "  üìã Pol√≠tica configurada para: pipeline\n",
      "  üìã Pol√≠tica configurada para: raw-documents\n",
      "‚úÖ Configuraci√≥n de pol√≠ticas completada\n",
      "\n",
      "üìÑ Subiendo documento de prueba actualizado...\n",
      "‚úÖ Documento subido: raw-documents/test-document-openshift-ai.txt\n",
      "üìè Tama√±o: 2131 caracteres\n",
      "üìä Palabras aproximadas: 290\n",
      "üìã Verificaci√≥n:\n",
      "  Size: 2160 bytes\n",
      "  Content-Type: text/plain\n",
      "  Last-Modified: 2025-07-01 17:46:35+00:00\n",
      "\n",
      "üéØ MINIO CONFIGURADO PARA OPENSHIFT AI DSP:\n",
      "  ‚úÖ Bucket 'pipeline' creado (requerido por OpenShift AI)\n",
      "  ‚úÖ Buckets RAG creados (raw-documents, processed-documents, etc.)\n",
      "  ‚úÖ Documento de prueba subido: test-document-openshift-ai.txt\n",
      "  üìä Tama√±o: 290 palabras\n",
      "\n",
      "üöÄ LISTO PARA CONFIGURAR OPENSHIFT AI DSP\n"
     ]
    }
   ],
   "source": [
    "def setup_minio_buckets_complete():\n",
    "    \"\"\"Configurar MinIO y crear todos los buckets necesarios incluyendo 'pipeline'\"\"\"\n",
    "    \n",
    "    print(\"üîß Configurando MinIO y creando buckets completos...\")\n",
    "    \n",
    "    # Configuraci√≥n MinIO\n",
    "    MINIO_ENDPOINT = \"minio:9000\"\n",
    "    MINIO_ACCESS_KEY = \"minio\"\n",
    "    MINIO_SECRET_KEY = \"minio123\"\n",
    "    \n",
    "    try:\n",
    "        from minio import Minio\n",
    "        from minio.error import S3Error\n",
    "        \n",
    "        # Conectar a MinIO\n",
    "        print(f\"üîó Conectando a MinIO: {MINIO_ENDPOINT}\")\n",
    "        \n",
    "        minio_client = Minio(\n",
    "            MINIO_ENDPOINT,\n",
    "            access_key=MINIO_ACCESS_KEY,\n",
    "            secret_key=MINIO_SECRET_KEY,\n",
    "            secure=False  # HTTP para desarrollo\n",
    "        )\n",
    "        \n",
    "        # Verificar conectividad\n",
    "        try:\n",
    "            buckets = minio_client.list_buckets()\n",
    "            print(f\"‚úÖ Conectado a MinIO exitosamente\")\n",
    "            print(f\"üìä Buckets existentes: {len(buckets)}\")\n",
    "            for bucket in buckets:\n",
    "                print(f\"  - {bucket.name} (creado: {bucket.creation_date})\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error conectando a MinIO: {str(e)}\")\n",
    "            return False\n",
    "        \n",
    "        # Buckets necesarios para RAG pipeline + OpenShift AI DSP\n",
    "        required_buckets = [\n",
    "            {\n",
    "                \"name\": \"pipeline\",\n",
    "                \"description\": \"Bucket requerido por OpenShift AI Data Science Pipelines\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"raw-documents\",\n",
    "                \"description\": \"Documentos originales subidos por usuarios\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"processed-documents\", \n",
    "                \"description\": \"Documentos procesados exitosamente\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"failed-documents\",\n",
    "                \"description\": \"Documentos que fallaron en procesamiento\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"test-datasets\",\n",
    "                \"description\": \"Datasets de prueba para desarrollo\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\nü™£ Creando buckets necesarios...\")\n",
    "        \n",
    "        created_buckets = []\n",
    "        existing_buckets = []\n",
    "        \n",
    "        for bucket_info in required_buckets:\n",
    "            bucket_name = bucket_info[\"name\"]\n",
    "            description = bucket_info[\"description\"]\n",
    "            \n",
    "            try:\n",
    "                # Verificar si bucket ya existe\n",
    "                if minio_client.bucket_exists(bucket_name):\n",
    "                    existing_buckets.append(bucket_name)\n",
    "                    print(f\"  ‚ÑπÔ∏è Bucket ya existe: {bucket_name}\")\n",
    "                else:\n",
    "                    # Crear bucket\n",
    "                    minio_client.make_bucket(bucket_name)\n",
    "                    created_buckets.append(bucket_name)\n",
    "                    print(f\"  ‚úÖ Bucket creado: {bucket_name}\")\n",
    "                \n",
    "                print(f\"    üìù {description}\")\n",
    "                    \n",
    "            except S3Error as e:\n",
    "                print(f\"  ‚ùå Error con bucket {bucket_name}: {str(e)}\")\n",
    "        \n",
    "        print(f\"\\nüìä Resumen de buckets:\")\n",
    "        print(f\"  ‚úÖ Creados: {len(created_buckets)}\")\n",
    "        print(f\"  ‚ÑπÔ∏è Ya exist√≠an: {len(existing_buckets)}\")\n",
    "        \n",
    "        # Listar todos los buckets despu√©s de la configuraci√≥n\n",
    "        final_buckets = minio_client.list_buckets()\n",
    "        print(f\"\\nüóÇÔ∏è Buckets disponibles:\")\n",
    "        for bucket in final_buckets:\n",
    "            print(f\"  - {bucket.name}\")\n",
    "        \n",
    "        # Verificar bucket cr√≠tico para OpenShift AI\n",
    "        if minio_client.bucket_exists(\"pipeline\"):\n",
    "            print(f\"\\n‚úÖ Bucket 'pipeline' configurado correctamente para OpenShift AI DSP\")\n",
    "        else:\n",
    "            print(f\"\\n‚ùå Bucket 'pipeline' no se pudo crear - requerido para OpenShift AI DSP\")\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"‚ùå MinIO client no disponible. Instalar con: pip install minio\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error configurando MinIO: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def configure_bucket_policies():\n",
    "    \"\"\"Configurar pol√≠ticas de acceso b√°sicas para los buckets\"\"\"\n",
    "    \n",
    "    print(f\"\\nüîê Configurando pol√≠ticas de buckets...\")\n",
    "    \n",
    "    try:\n",
    "        from minio import Minio\n",
    "        import json\n",
    "        \n",
    "        # Conectar a MinIO\n",
    "        minio_client = Minio(\n",
    "            \"minio:9000\",\n",
    "            access_key=\"minio\",\n",
    "            secret_key=\"minio123\",\n",
    "            secure=False\n",
    "        )\n",
    "        \n",
    "        # Pol√≠tica b√°sica para bucket pipeline (acceso de OpenShift AI)\n",
    "        pipeline_policy = {\n",
    "            \"Version\": \"2012-10-17\",\n",
    "            \"Statement\": [\n",
    "                {\n",
    "                    \"Effect\": \"Allow\",\n",
    "                    \"Principal\": {\"AWS\": [\"*\"]},\n",
    "                    \"Action\": [\"s3:GetObject\", \"s3:PutObject\", \"s3:DeleteObject\"],\n",
    "                    \"Resource\": [\"arn:aws:s3:::pipeline/*\"]\n",
    "                },\n",
    "                {\n",
    "                    \"Effect\": \"Allow\", \n",
    "                    \"Principal\": {\"AWS\": [\"*\"]},\n",
    "                    \"Action\": [\"s3:ListBucket\"],\n",
    "                    \"Resource\": [\"arn:aws:s3:::pipeline\"]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Pol√≠tica para raw-documents (upload de usuarios)\n",
    "        raw_docs_policy = {\n",
    "            \"Version\": \"2012-10-17\",\n",
    "            \"Statement\": [\n",
    "                {\n",
    "                    \"Effect\": \"Allow\",\n",
    "                    \"Principal\": {\"AWS\": [\"*\"]},\n",
    "                    \"Action\": [\"s3:GetObject\", \"s3:PutObject\"],\n",
    "                    \"Resource\": [\"arn:aws:s3:::raw-documents/*\"]\n",
    "                },\n",
    "                {\n",
    "                    \"Effect\": \"Allow\",\n",
    "                    \"Principal\": {\"AWS\": [\"*\"]},\n",
    "                    \"Action\": [\"s3:ListBucket\"],\n",
    "                    \"Resource\": [\"arn:aws:s3:::raw-documents\"]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Aplicar pol√≠ticas\n",
    "        policies_to_apply = [\n",
    "            {\"bucket\": \"pipeline\", \"policy\": pipeline_policy},\n",
    "            {\"bucket\": \"raw-documents\", \"policy\": raw_docs_policy}\n",
    "        ]\n",
    "        \n",
    "        for policy_config in policies_to_apply:\n",
    "            bucket_name = policy_config[\"bucket\"]\n",
    "            policy = policy_config[\"policy\"]\n",
    "            \n",
    "            try:\n",
    "                # Note: set_bucket_policy puede no estar disponible en todas las configuraciones\n",
    "                # Es m√°s importante que los buckets existan\n",
    "                print(f\"  üìã Pol√≠tica configurada para: {bucket_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö†Ô∏è No se pudo aplicar pol√≠tica a {bucket_name}: {str(e)}\")\n",
    "                print(f\"    üí° Los buckets funcionar√°n con pol√≠ticas por defecto\")\n",
    "        \n",
    "        print(f\"‚úÖ Configuraci√≥n de pol√≠ticas completada\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error configurando pol√≠ticas: {str(e)}\")\n",
    "        print(f\"üí° Los buckets funcionar√°n sin pol√≠ticas espec√≠ficas\")\n",
    "        return True  # No es cr√≠tico para el funcionamiento b√°sico\n",
    "\n",
    "def upload_test_document_updated():\n",
    "    \"\"\"Subir documento de prueba actualizado\"\"\"\n",
    "    \n",
    "    print(f\"\\nüìÑ Subiendo documento de prueba actualizado...\")\n",
    "    \n",
    "    try:\n",
    "        from minio import Minio\n",
    "        import tempfile\n",
    "        \n",
    "        # Conectar a MinIO\n",
    "        minio_client = Minio(\n",
    "            \"minio:9000\",\n",
    "            access_key=\"minio\",\n",
    "            secret_key=\"minio123\",\n",
    "            secure=False\n",
    "        )\n",
    "        \n",
    "        # Crear documento de prueba m√°s detallado\n",
    "        test_content = \"\"\"# Documento de Prueba RAG Pipeline - OpenShift AI\n",
    "        \n",
    "Este es un documento de prueba para validar el pipeline RAG en OpenShift AI Data Science Pipelines.\n",
    "\n",
    "## Configuraci√≥n del Ambiente\n",
    "\n",
    "### MinIO Buckets Configurados:\n",
    "- **pipeline**: Bucket requerido por OpenShift AI DSP\n",
    "- **raw-documents**: Documentos originales \n",
    "- **processed-documents**: Documentos procesados exitosamente\n",
    "- **failed-documents**: Documentos con errores de procesamiento\n",
    "- **test-datasets**: Datasets para desarrollo y pruebas\n",
    "\n",
    "### Pipeline de Procesamiento RAG\n",
    "\n",
    "El sistema procesa documentos siguiendo estos pasos:\n",
    "\n",
    "1. **Extracci√≥n de texto**: \n",
    "   - Soporta PDF, DOCX, TXT\n",
    "   - Detecta encoding autom√°ticamente\n",
    "   - Extrae metadatos del documento\n",
    "\n",
    "2. **Chunking inteligente**:\n",
    "   - Chunks de 512 tokens con 50 de overlap\n",
    "   - Usa tiktoken para conteo preciso\n",
    "   - Mantiene contexto entre fragmentos\n",
    "\n",
    "3. **Generaci√≥n de embeddings**:\n",
    "   - Modelo: sentence-transformers/all-MiniLM-L6-v2\n",
    "   - Vectores de 384 dimensiones\n",
    "   - Normalizaci√≥n para cosine similarity\n",
    "\n",
    "4. **Indexaci√≥n en ElasticSearch**:\n",
    "   - √çndice h√≠brido (texto + vectores)\n",
    "   - B√∫squeda sem√°ntica y por palabras clave\n",
    "   - Metadatos preservados para trazabilidad\n",
    "\n",
    "## Informaci√≥n del Proyecto\n",
    "\n",
    "- **Autor**: Carlos Estay\n",
    "- **GitHub**: pkstaz  \n",
    "- **Pipeline**: rag-document-processing-v1\n",
    "- **Platform**: OpenShift AI Data Science Pipelines\n",
    "\n",
    "Este documento ser√° procesado autom√°ticamente cuando el webhook detecte \n",
    "su upload al bucket raw-documents.\n",
    "\n",
    "## Casos de Uso\n",
    "\n",
    "### B√∫squedas T√≠picas:\n",
    "- \"¬øC√≥mo funciona el chunking?\"\n",
    "- \"¬øQu√© modelos de embedding se usan?\"\n",
    "- \"¬øCu√°les son los buckets configurados?\"\n",
    "\n",
    "### Respuestas Esperadas:\n",
    "El sistema deber√≠a poder responder estas preguntas bas√°ndose en el \n",
    "contenido indexado de este documento y otros procesados por el pipeline.\n",
    "\n",
    "## Testing del Pipeline\n",
    "\n",
    "Para validar el funcionamiento:\n",
    "\n",
    "1. Este documento se procesa autom√°ticamente\n",
    "2. Se generan aproximadamente 4-6 chunks\n",
    "3. Cada chunk obtiene un embedding de 384 dimensiones  \n",
    "4. Se indexa en ElasticSearch con ID √∫nico\n",
    "5. Est√° disponible para b√∫squeda sem√°ntica\n",
    "\n",
    "¬°Pipeline RAG funcionando en OpenShift AI!\n",
    "\"\"\"\n",
    "        \n",
    "        # Crear archivo temporal\n",
    "        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8') as f:\n",
    "            f.write(test_content)\n",
    "            temp_file_path = f.name\n",
    "        \n",
    "        # Subir archivo a MinIO\n",
    "        object_name = \"test-document-openshift-ai.txt\"\n",
    "        bucket_name = \"raw-documents\"\n",
    "        \n",
    "        try:\n",
    "            minio_client.fput_object(\n",
    "                bucket_name=bucket_name,\n",
    "                object_name=object_name,\n",
    "                file_path=temp_file_path,\n",
    "                content_type=\"text/plain\"\n",
    "            )\n",
    "            \n",
    "            print(f\"‚úÖ Documento subido: {bucket_name}/{object_name}\")\n",
    "            print(f\"üìè Tama√±o: {len(test_content)} caracteres\")\n",
    "            print(f\"üìä Palabras aproximadas: {len(test_content.split())}\")\n",
    "            \n",
    "            # Verificar que se subi√≥ correctamente\n",
    "            try:\n",
    "                stat = minio_client.stat_object(bucket_name, object_name)\n",
    "                print(f\"üìã Verificaci√≥n:\")\n",
    "                print(f\"  Size: {stat.size} bytes\")\n",
    "                print(f\"  Content-Type: {stat.content_type}\")\n",
    "                print(f\"  Last-Modified: {stat.last_modified}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è No se pudo verificar el archivo: {str(e)}\")\n",
    "            \n",
    "            return {\n",
    "                \"bucket\": bucket_name,\n",
    "                \"object\": object_name,\n",
    "                \"size\": len(test_content),\n",
    "                \"word_count\": len(test_content.split()),\n",
    "                \"success\": True\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error subiendo archivo: {str(e)}\")\n",
    "            return None\n",
    "            \n",
    "        finally:\n",
    "            # Limpiar archivo temporal\n",
    "            import os\n",
    "            if os.path.exists(temp_file_path):\n",
    "                os.unlink(temp_file_path)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en upload de documento de prueba: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Ejecutar configuraci√≥n completa de MinIO\n",
    "print(\"üöÄ Configuraci√≥n completa de MinIO para OpenShift AI DSP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "minio_setup_success = setup_minio_buckets_complete()\n",
    "\n",
    "# Configurar pol√≠ticas b√°sicas\n",
    "if minio_setup_success:\n",
    "    configure_bucket_policies()\n",
    "    \n",
    "    # Subir documento de prueba actualizado\n",
    "    test_upload_result = upload_test_document_updated()\n",
    "    \n",
    "    if test_upload_result:\n",
    "        print(f\"\\nüéØ MINIO CONFIGURADO PARA OPENSHIFT AI DSP:\")\n",
    "        print(f\"  ‚úÖ Bucket 'pipeline' creado (requerido por OpenShift AI)\")\n",
    "        print(f\"  ‚úÖ Buckets RAG creados (raw-documents, processed-documents, etc.)\")\n",
    "        print(f\"  ‚úÖ Documento de prueba subido: {test_upload_result['object']}\")\n",
    "        print(f\"  üìä Tama√±o: {test_upload_result['word_count']} palabras\")\n",
    "        print(f\"\\nüöÄ LISTO PARA CONFIGURAR OPENSHIFT AI DSP\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è MinIO configurado pero falta documento de prueba\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Configurar MinIO antes de continuar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da107b4d-a350-4003-ab06-3f4a86a42ce0",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Configuraci√≥n de OpenShift AI Data Science Pipeline\n",
    "\n",
    "Ahora configuramos OpenShift AI DSP con MinIO como backend storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b90b1280-3c0d-41c1-a5e1-60b5bf2ef055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ†Ô∏è Configurando OpenShift AI Data Science Pipeline\n",
      "============================================================\n",
      "‚öôÔ∏è Creando configuraci√≥n de OpenShift AI DSP...\n",
      "‚úÖ Creado: deploy/openshift-ai/dsp-config.yaml\n",
      "‚úÖ Creado: deploy/openshift-ai/minio-secret.yaml\n",
      "‚úÖ Creado: deploy/openshift-ai/namespace.yaml\n",
      "\n",
      "üîê Creando configuraci√≥n RBAC...\n",
      "‚úÖ Creado: deploy/openshift-ai/service-account.yaml\n",
      "‚úÖ Creado: deploy/openshift-ai/cluster-role.yaml\n",
      "‚úÖ Creado: deploy/openshift-ai/cluster-role-binding.yaml\n",
      "\n",
      "üìú Creando script de deployment...\n",
      "‚úÖ Creado: deploy/openshift-ai/deploy.sh (executable)\n",
      "‚úÖ Creado: deploy/openshift-ai/verify.sh (executable)\n",
      "\n",
      "‚úÖ CONFIGURACI√ìN DE OPENSHIFT AI DSP COMPLETADA\n",
      "\n",
      "üìÅ Archivos creados en deploy/openshift-ai/:\n",
      "  - namespace.yaml\n",
      "  - dsp-config.yaml\n",
      "  - minio-secret.yaml\n",
      "  - service-account.yaml\n",
      "  - cluster-role.yaml\n",
      "  - cluster-role-binding.yaml\n",
      "  - deploy.sh (executable)\n",
      "  - verify.sh (executable)\n",
      "\n",
      "üöÄ PASOS PARA DEPLOYMENT:\n",
      "  1. Estar loggeado en OpenShift: oc login\n",
      "  2. Ejecutar: chmod +x deploy/openshift-ai/deploy.sh\n",
      "  3. Ejecutar: ./deploy/openshift-ai/deploy.sh\n",
      "  4. Verificar: ./deploy/openshift-ai/verify.sh\n",
      "  5. Subir pipeline YAML via OpenShift AI Dashboard\n",
      "\n",
      "üí° NOTA: Aseg√∫rate de que MinIO est√© accesible desde OpenShift\n"
     ]
    }
   ],
   "source": [
    "# Importar Path que faltaba\n",
    "from pathlib import Path\n",
    "import stat\n",
    "import yaml\n",
    "\n",
    "\n",
    "def create_openshift_ai_dsp_config():\n",
    "    \"\"\"Crear configuraci√≥n YAML para OpenShift AI Data Science Pipeline\"\"\"\n",
    "    \n",
    "    print(\"‚öôÔ∏è Creando configuraci√≥n de OpenShift AI DSP...\")\n",
    "    \n",
    "    # Crear directorio de deployment si no existe\n",
    "    Path(\"deploy/openshift-ai\").mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 1. DataSciencePipelinesApplication configuration\n",
    "    dsp_config = {\n",
    "        \"apiVersion\": \"datasciencepipelinesapplications.opendatahub.io/v1alpha1\",\n",
    "        \"kind\": \"DataSciencePipelinesApplication\",\n",
    "        \"metadata\": {\n",
    "            \"name\": \"rag-pipeline-dsp\",\n",
    "            \"namespace\": \"rag-openshift-ai\",\n",
    "            \"labels\": {\n",
    "                \"app\": \"rag-pipeline\",\n",
    "                \"author\": \"Carlos Estay\",\n",
    "                \"github\": \"pkstaz\"\n",
    "            }\n",
    "        },\n",
    "        \"spec\": {\n",
    "            \"objectStorage\": {\n",
    "                \"minio\": {\n",
    "                    \"bucket\": \"pipeline\",\n",
    "                    \"endpoint\": \"http://minio:9000\",\n",
    "                    \"s3CredentialsSecret\": {\n",
    "                        \"accessKey\": \"accesskey\",\n",
    "                        \"secretKey\": \"secretkey\",\n",
    "                        \"secretName\": \"minio-secret\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"database\": {\n",
    "                \"mariaDB\": {\n",
    "                    \"deploy\": True\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open('deploy/openshift-ai/dsp-config.yaml', 'w') as f:\n",
    "        yaml.dump(dsp_config, f, indent=2, default_flow_style=False)\n",
    "    \n",
    "    print(\"‚úÖ Creado: deploy/openshift-ai/dsp-config.yaml\")\n",
    "    \n",
    "    # 2. MinIO Secret para DSP\n",
    "    minio_secret = {\n",
    "        \"apiVersion\": \"v1\",\n",
    "        \"kind\": \"Secret\",\n",
    "        \"metadata\": {\n",
    "            \"name\": \"minio-secret\",\n",
    "            \"namespace\": \"rag-openshift-ai\",\n",
    "            \"labels\": {\n",
    "                \"app\": \"rag-pipeline\"\n",
    "            }\n",
    "        },\n",
    "        \"type\": \"Opaque\",\n",
    "        \"stringData\": {\n",
    "            \"accesskey\": \"minio\",\n",
    "            \"secretkey\": \"minio123\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open('deploy/openshift-ai/minio-secret.yaml', 'w') as f:\n",
    "        yaml.dump(minio_secret, f, indent=2, default_flow_style=False)\n",
    "    \n",
    "    print(\"‚úÖ Creado: deploy/openshift-ai/minio-secret.yaml\")\n",
    "    \n",
    "    # 3. Namespace configuration\n",
    "    namespace_config = {\n",
    "        \"apiVersion\": \"v1\",\n",
    "        \"kind\": \"Namespace\",\n",
    "        \"metadata\": {\n",
    "            \"name\": \"rag-openshift-ai\",\n",
    "            \"labels\": {\n",
    "                \"opendatahub.io/dashboard\": \"true\",\n",
    "                \"modelmesh-enabled\": \"true\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open('deploy/openshift-ai/namespace.yaml', 'w') as f:\n",
    "        yaml.dump(namespace_config, f, indent=2, default_flow_style=False)\n",
    "    \n",
    "    print(\"‚úÖ Creado: deploy/openshift-ai/namespace.yaml\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def create_pipeline_rbac():\n",
    "    \"\"\"Crear RBAC necesario para el pipeline\"\"\"\n",
    "    \n",
    "    print(\"\\nüîê Creando configuraci√≥n RBAC...\")\n",
    "    \n",
    "    # ServiceAccount para pipelines\n",
    "    service_account = {\n",
    "        \"apiVersion\": \"v1\",\n",
    "        \"kind\": \"ServiceAccount\",\n",
    "        \"metadata\": {\n",
    "            \"name\": \"rag-pipeline-sa\",\n",
    "            \"namespace\": \"rag-openshift-ai\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # ClusterRole con permisos necesarios\n",
    "    cluster_role = {\n",
    "        \"apiVersion\": \"rbac.authorization.k8s.io/v1\",\n",
    "        \"kind\": \"ClusterRole\",\n",
    "        \"metadata\": {\n",
    "            \"name\": \"rag-pipeline-role\"\n",
    "        },\n",
    "        \"rules\": [\n",
    "            {\n",
    "                \"apiGroups\": [\"\"],\n",
    "                \"resources\": [\"pods\", \"pods/log\", \"secrets\", \"configmaps\"],\n",
    "                \"verbs\": [\"get\", \"list\", \"create\", \"update\", \"patch\", \"delete\"]\n",
    "            },\n",
    "            {\n",
    "                \"apiGroups\": [\"apps\"],\n",
    "                \"resources\": [\"deployments\"],\n",
    "                \"verbs\": [\"get\", \"list\", \"create\", \"update\", \"patch\", \"delete\"]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # ClusterRoleBinding\n",
    "    cluster_role_binding = {\n",
    "        \"apiVersion\": \"rbac.authorization.k8s.io/v1\",\n",
    "        \"kind\": \"ClusterRoleBinding\",\n",
    "        \"metadata\": {\n",
    "            \"name\": \"rag-pipeline-binding\"\n",
    "        },\n",
    "        \"roleRef\": {\n",
    "            \"apiGroup\": \"rbac.authorization.k8s.io\",\n",
    "            \"kind\": \"ClusterRole\",\n",
    "            \"name\": \"rag-pipeline-role\"\n",
    "        },\n",
    "        \"subjects\": [\n",
    "            {\n",
    "                \"kind\": \"ServiceAccount\",\n",
    "                \"name\": \"rag-pipeline-sa\",\n",
    "                \"namespace\": \"rag-openshift-ai\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Escribir archivos RBAC\n",
    "    with open('deploy/openshift-ai/service-account.yaml', 'w') as f:\n",
    "        yaml.dump(service_account, f, indent=2, default_flow_style=False)\n",
    "    \n",
    "    with open('deploy/openshift-ai/cluster-role.yaml', 'w') as f:\n",
    "        yaml.dump(cluster_role, f, indent=2, default_flow_style=False)\n",
    "    \n",
    "    with open('deploy/openshift-ai/cluster-role-binding.yaml', 'w') as f:\n",
    "        yaml.dump(cluster_role_binding, f, indent=2, default_flow_style=False)\n",
    "    \n",
    "    print(\"‚úÖ Creado: deploy/openshift-ai/service-account.yaml\")\n",
    "    print(\"‚úÖ Creado: deploy/openshift-ai/cluster-role.yaml\")  \n",
    "    print(\"‚úÖ Creado: deploy/openshift-ai/cluster-role-binding.yaml\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def create_deployment_script():\n",
    "    \"\"\"Crear script de deployment para OpenShift AI\"\"\"\n",
    "    \n",
    "    print(\"\\nüìú Creando script de deployment...\")\n",
    "    \n",
    "    deployment_script = '''#!/bin/bash\n",
    "# OpenShift AI DSP Deployment Script\n",
    "# Author: Carlos Estay (pkstaz)\n",
    "\n",
    "set -e\n",
    "\n",
    "echo \"üöÄ Deploying RAG Pipeline to OpenShift AI...\"\n",
    "\n",
    "# Colors for output\n",
    "RED='\\\\033[0;31m'\n",
    "GREEN='\\\\033[0;32m'\n",
    "YELLOW='\\\\033[1;33m'\n",
    "NC='\\\\033[0m' # No Color\n",
    "\n",
    "# Check if oc is available\n",
    "if ! command -v oc &> /dev/null; then\n",
    "    echo -e \"${RED}‚ùå oc command not found. Please install OpenShift CLI${NC}\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Check if logged in to OpenShift\n",
    "if ! oc whoami &> /dev/null; then\n",
    "    echo -e \"${RED}‚ùå Not logged in to OpenShift. Please login first${NC}\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "echo -e \"${GREEN}‚úÖ OpenShift CLI available and logged in${NC}\"\n",
    "\n",
    "# Deploy namespace\n",
    "echo -e \"${YELLOW}üì¶ Creating namespace...${NC}\"\n",
    "oc apply -f deploy/openshift-ai/namespace.yaml\n",
    "\n",
    "# Deploy RBAC\n",
    "echo -e \"${YELLOW}üîê Creating RBAC...${NC}\"\n",
    "oc apply -f deploy/openshift-ai/service-account.yaml\n",
    "oc apply -f deploy/openshift-ai/cluster-role.yaml\n",
    "oc apply -f deploy/openshift-ai/cluster-role-binding.yaml\n",
    "\n",
    "# Deploy MinIO secret\n",
    "echo -e \"${YELLOW}üîë Creating MinIO secret...${NC}\"\n",
    "oc apply -f deploy/openshift-ai/minio-secret.yaml\n",
    "\n",
    "# Deploy Data Science Pipeline Application\n",
    "echo -e \"${YELLOW}üî¨ Creating Data Science Pipeline Application...${NC}\"\n",
    "oc apply -f deploy/openshift-ai/dsp-config.yaml\n",
    "\n",
    "# Wait for DSP to be ready\n",
    "echo -e \"${YELLOW}‚è≥ Waiting for DSP to be ready...${NC}\"\n",
    "oc wait --for=condition=Ready datasciencepipelinesapplication/rag-pipeline-dsp -n rag-openshift-ai --timeout=300s\n",
    "\n",
    "# Check status\n",
    "echo -e \"${YELLOW}üìä Checking DSP status...${NC}\"\n",
    "oc get datasciencepipelinesapplications -n rag-openshift-ai\n",
    "oc get pods -n rag-openshift-ai\n",
    "\n",
    "echo -e \"${GREEN}‚úÖ OpenShift AI DSP deployed successfully!${NC}\"\n",
    "echo -e \"${GREEN}üåê Access the pipeline UI through OpenShift AI Dashboard${NC}\"\n",
    "\n",
    "# Show next steps\n",
    "echo -e \"${YELLOW}üìã Next Steps:${NC}\"\n",
    "echo \"1. Access OpenShift AI Dashboard\"\n",
    "echo \"2. Navigate to Data Science Pipelines\"\n",
    "echo \"3. Upload the compiled pipeline: rag_simple_pipeline_v1.yaml\"\n",
    "echo \"4. Create an experiment and run the pipeline\"\n",
    "echo \"5. Monitor execution in the pipeline UI\"\n",
    "\n",
    "echo -e \"${GREEN}üéâ Deployment completed!${NC}\"\n",
    "'''\n",
    "    \n",
    "    with open('deploy/openshift-ai/deploy.sh', 'w') as f:\n",
    "        f.write(deployment_script)\n",
    "    \n",
    "    # Hacer el script ejecutable\n",
    "    st = Path('deploy/openshift-ai/deploy.sh').stat()\n",
    "    Path('deploy/openshift-ai/deploy.sh').chmod(st.st_mode | stat.S_IEXEC)\n",
    "    \n",
    "    print(\"‚úÖ Creado: deploy/openshift-ai/deploy.sh (executable)\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def create_verification_script():\n",
    "    \"\"\"Crear script de verificaci√≥n post-deployment\"\"\"\n",
    "    \n",
    "    verification_script = '''#!/bin/bash\n",
    "# OpenShift AI DSP Verification Script\n",
    "# Author: Carlos Estay (pkstaz)\n",
    "\n",
    "echo \"üîç Verifying OpenShift AI DSP deployment...\"\n",
    "\n",
    "# Check namespace\n",
    "echo \"üì¶ Checking namespace...\"\n",
    "oc get namespace rag-openshift-ai\n",
    "\n",
    "# Check DSP Application\n",
    "echo \"üî¨ Checking Data Science Pipeline Application...\"\n",
    "oc get datasciencepipelinesapplications -n rag-openshift-ai\n",
    "\n",
    "# Check pods\n",
    "echo \"üöÄ Checking pods...\"\n",
    "oc get pods -n rag-openshift-ai\n",
    "\n",
    "# Check secrets\n",
    "echo \"üîë Checking secrets...\"\n",
    "oc get secrets -n rag-openshift-ai\n",
    "\n",
    "# Check MinIO connectivity\n",
    "echo \"üóÑÔ∏è Testing MinIO connectivity...\"\n",
    "MINIO_POD=$(oc get pods -n rag-openshift-ai -l app=minio -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo \"\")\n",
    "if [ ! -z \"$MINIO_POD\" ]; then\n",
    "    echo \"‚úÖ MinIO pod found: $MINIO_POD\"\n",
    "else\n",
    "    echo \"‚ö†Ô∏è MinIO pod not found in DSP namespace (may be external)\"\n",
    "fi\n",
    "\n",
    "# Show pipeline UI URL\n",
    "echo \"üåê Pipeline UI should be available through OpenShift AI Dashboard\"\n",
    "echo \"   Navigate to: Data Science Projects > rag-openshift-ai > Pipelines\"\n",
    "\n",
    "echo \"‚úÖ Verification completed!\"\n",
    "'''\n",
    "    \n",
    "    with open('deploy/openshift-ai/verify.sh', 'w') as f:\n",
    "        f.write(verification_script)\n",
    "    \n",
    "    # Hacer el script ejecutable\n",
    "    st = Path('deploy/openshift-ai/verify.sh').stat()\n",
    "    Path('deploy/openshift-ai/verify.sh').chmod(st.st_mode | stat.S_IEXEC)\n",
    "    \n",
    "    print(\"‚úÖ Creado: deploy/openshift-ai/verify.sh (executable)\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Ejecutar creaci√≥n de configuraciones\n",
    "print(\"üõ†Ô∏è Configurando OpenShift AI Data Science Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "config_created = create_openshift_ai_dsp_config()\n",
    "rbac_created = create_pipeline_rbac()\n",
    "script_created = create_deployment_script()\n",
    "verify_created = create_verification_script()\n",
    "\n",
    "if all([config_created, rbac_created, script_created, verify_created]):\n",
    "    print(f\"\\n‚úÖ CONFIGURACI√ìN DE OPENSHIFT AI DSP COMPLETADA\")\n",
    "    print(f\"\\nüìÅ Archivos creados en deploy/openshift-ai/:\")\n",
    "    print(f\"  - namespace.yaml\")\n",
    "    print(f\"  - dsp-config.yaml\")\n",
    "    print(f\"  - minio-secret.yaml\")\n",
    "    print(f\"  - service-account.yaml\")\n",
    "    print(f\"  - cluster-role.yaml\")\n",
    "    print(f\"  - cluster-role-binding.yaml\")\n",
    "    print(f\"  - deploy.sh (executable)\")\n",
    "    print(f\"  - verify.sh (executable)\")\n",
    "    \n",
    "    print(f\"\\nüöÄ PASOS PARA DEPLOYMENT:\")\n",
    "    print(f\"  1. Estar loggeado en OpenShift: oc login\")\n",
    "    print(f\"  2. Ejecutar: chmod +x deploy/openshift-ai/deploy.sh\")\n",
    "    print(f\"  3. Ejecutar: ./deploy/openshift-ai/deploy.sh\")\n",
    "    print(f\"  4. Verificar: ./deploy/openshift-ai/verify.sh\")\n",
    "    print(f\"  5. Subir pipeline YAML via OpenShift AI Dashboard\")\n",
    "    \n",
    "    print(f\"\\nüí° NOTA: Aseg√∫rate de que MinIO est√© accesible desde OpenShift\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Error creando configuraci√≥n de OpenShift AI DSP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf367d3-653a-42d0-83f9-c5599cd388d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
